{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "24527571",
   "metadata": {},
   "source": [
    "# 爬取模型列表"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "855c0a93",
   "metadata": {},
   "outputs": [],
   "source": [
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "from selenium.webdriver.chrome.options import Options\n",
    "from selenium.common.exceptions import TimeoutException, NoSuchElementException, ElementClickInterceptedException\n",
    "import time\n",
    "import csv\n",
    "import json\n",
    "import logging\n",
    "import re\n",
    "from urllib.parse import urljoin, urlparse, parse_qs\n",
    "\n",
    "# 设置日志\n",
    "logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "class FixedHuggingFaceModelsCrawler:\n",
    "    def __init__(self, headless=True, delay=3):\n",
    "        \"\"\"\n",
    "        修复版模型爬虫\n",
    "        :param headless: 是否使用无头模式\n",
    "        :param delay: 页面加载等待时间\n",
    "        \"\"\"\n",
    "        self.delay = delay\n",
    "        self.models = []\n",
    "        self.current_page = 1\n",
    "        self.setup_driver(headless)\n",
    "        \n",
    "    def setup_driver(self, headless=True):\n",
    "        \"\"\"\n",
    "        设置Chrome浏览器驱动\n",
    "        \"\"\"\n",
    "        chrome_options = Options()\n",
    "        if headless:\n",
    "            chrome_options.add_argument(\"--headless\")\n",
    "        chrome_options.add_argument(\"--no-sandbox\")\n",
    "        chrome_options.add_argument(\"--disable-dev-shm-usage\")\n",
    "        chrome_options.add_argument(\"--disable-gpu\")\n",
    "        chrome_options.add_argument(\"--window-size=1920,1080\")\n",
    "        chrome_options.add_argument(\"--user-agent=Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36\")\n",
    "        \n",
    "        # 禁用图片和CSS加载以提高速度\n",
    "        prefs = {\n",
    "            \"profile.managed_default_content_settings.images\": 2,\n",
    "            \"profile.default_content_setting_values.notifications\": 2\n",
    "        }\n",
    "        chrome_options.add_experimental_option(\"prefs\", prefs)\n",
    "        \n",
    "        try:\n",
    "            self.driver = webdriver.Chrome(options=chrome_options)\n",
    "            self.wait = WebDriverWait(self.driver, 15)\n",
    "            logger.info(\"Chrome浏览器驱动初始化成功\")\n",
    "        except Exception as e:\n",
    "            logger.error(f\"浏览器驱动初始化失败: {e}\")\n",
    "            raise\n",
    "    \n",
    "    def load_page(self, url):\n",
    "        \"\"\"\n",
    "        加载页面并等待内容完全加载\n",
    "        :param url: 页面URL\n",
    "        :return: 是否成功加载\n",
    "        \"\"\"\n",
    "        try:\n",
    "            logger.info(f\"正在加载页面: {url}\")\n",
    "            self.driver.get(url)\n",
    "            \n",
    "            # 等待页面基本结构加载\n",
    "            self.wait.until(EC.presence_of_element_located((By.TAG_NAME, \"main\")))\n",
    "            \n",
    "            # 等待模型列表加载完成\n",
    "            time.sleep(self.delay)\n",
    "            \n",
    "            # 尝试等待具体的模型容器加载\n",
    "            try:\n",
    "                self.wait.until(EC.presence_of_element_located((By.CSS_SELECTOR, \"article, .overview-card, h4 a\")))\n",
    "            except TimeoutException:\n",
    "                logger.warning(\"未找到明确的模型容器，继续尝试提取\")\n",
    "            \n",
    "            # 滚动页面确保所有内容加载\n",
    "            self.driver.execute_script(\"window.scrollTo(0, document.body.scrollHeight/2);\")\n",
    "            time.sleep(1)\n",
    "            self.driver.execute_script(\"window.scrollTo(0, 0);\")\n",
    "            time.sleep(1)\n",
    "            \n",
    "            return True\n",
    "        except TimeoutException:\n",
    "            logger.error(f\"页面加载超时: {url}\")\n",
    "            return False\n",
    "        except Exception as e:\n",
    "            logger.error(f\"页面加载失败: {url}, 错误: {e}\")\n",
    "            return False\n",
    "    \n",
    "    def extract_models_from_current_page(self):\n",
    "        \"\"\"\n",
    "        从当前页面提取模型信息 - 改进版\n",
    "        :return: 模型列表\n",
    "        \"\"\"\n",
    "        models = []\n",
    "        \n",
    "        try:\n",
    "            # 等待页面完全加载\n",
    "            time.sleep(2)\n",
    "            \n",
    "            # 基于实际观察到的HTML结构，使用更精确的选择器\n",
    "            selectors_and_strategies = [\n",
    "                # 策略1: 查找包含模型路径的链接\n",
    "                (\"a[href^='/']\", self.extract_from_links),\n",
    "                # 策略2: 查找文章标题中的链接\n",
    "                (\"article h4 a, article h3 a\", self.extract_from_article_links),\n",
    "                # 策略3: 从页面文本中提取\n",
    "                (\"body\", self.extract_from_page_text),\n",
    "            ]\n",
    "            \n",
    "            for selector, extraction_func in selectors_and_strategies:\n",
    "                try:\n",
    "                    elements = self.driver.find_elements(By.CSS_SELECTOR, selector)\n",
    "                    if elements:\n",
    "                        extracted_models = extraction_func(elements)\n",
    "                        if extracted_models:\n",
    "                            models.extend(extracted_models)\n",
    "                            logger.info(f\"使用策略 '{selector}' 找到 {len(extracted_models)} 个模型\")\n",
    "                            break  # 如果成功提取到模型，就不再尝试其他策略\n",
    "                except Exception as e:\n",
    "                    logger.debug(f\"策略 '{selector}' 失败: {e}\")\n",
    "                    continue\n",
    "            \n",
    "            # 如果上述策略都失败，使用正则表达式从页面源代码提取\n",
    "            if not models:\n",
    "                models = self.extract_from_page_source()\n",
    "            \n",
    "            return models\n",
    "            \n",
    "        except Exception as e:\n",
    "            logger.error(f\"提取模型时出错: {e}\")\n",
    "            return []\n",
    "    \n",
    "    def extract_from_links(self, elements):\n",
    "        \"\"\"\n",
    "        从链接元素中提取模型信息\n",
    "        \"\"\"\n",
    "        models = []\n",
    "        processed_names = set()\n",
    "        \n",
    "        for element in elements:\n",
    "            try:\n",
    "                href = element.get_attribute(\"href\")\n",
    "                if not href:\n",
    "                    continue\n",
    "                \n",
    "                model_name = self.extract_model_name_from_url(href)\n",
    "                if model_name and model_name not in processed_names:\n",
    "                    display_text = element.text.strip()\n",
    "                    \n",
    "                    model_info = {\n",
    "                        'name': model_name,\n",
    "                        'url': f\"https://huggingface.co/{model_name}\",\n",
    "                        'display_text': display_text[:100] if display_text else \"\",\n",
    "                        'href': href\n",
    "                    }\n",
    "                    \n",
    "                    models.append(model_info)\n",
    "                    processed_names.add(model_name)\n",
    "                    logger.debug(f\"从链接找到模型: {model_name}\")\n",
    "                    \n",
    "            except Exception as e:\n",
    "                logger.debug(f\"处理链接元素时出错: {e}\")\n",
    "                continue\n",
    "        \n",
    "        return models\n",
    "    \n",
    "    def extract_from_article_links(self, elements):\n",
    "        \"\"\"\n",
    "        从文章标题链接中提取模型信息\n",
    "        \"\"\"\n",
    "        models = []\n",
    "        processed_names = set()\n",
    "        \n",
    "        for element in elements:\n",
    "            try:\n",
    "                href = element.get_attribute(\"href\")\n",
    "                if not href:\n",
    "                    continue\n",
    "                \n",
    "                model_name = self.extract_model_name_from_url(href)\n",
    "                if model_name and model_name not in processed_names:\n",
    "                    display_text = element.text.strip()\n",
    "                    \n",
    "                    # 尝试获取父元素的更多信息\n",
    "                    parent_text = \"\"\n",
    "                    try:\n",
    "                        parent = element.find_element(By.XPATH, \"./ancestor::article[1]\")\n",
    "                        parent_text = parent.text.strip()[:200]\n",
    "                    except:\n",
    "                        pass\n",
    "                    \n",
    "                    model_info = {\n",
    "                        'name': model_name,\n",
    "                        'url': f\"https://huggingface.co/{model_name}\",\n",
    "                        'display_text': display_text[:100] if display_text else \"\",\n",
    "                        'parent_text': parent_text,\n",
    "                        'href': href\n",
    "                    }\n",
    "                    \n",
    "                    models.append(model_info)\n",
    "                    processed_names.add(model_name)\n",
    "                    logger.debug(f\"从文章标题找到模型: {model_name}\")\n",
    "                    \n",
    "            except Exception as e:\n",
    "                logger.debug(f\"处理文章链接时出错: {e}\")\n",
    "                continue\n",
    "        \n",
    "        return models\n",
    "    \n",
    "    def extract_from_page_text(self, elements):\n",
    "        \"\"\"\n",
    "        从页面文本中提取模型信息\n",
    "        \"\"\"\n",
    "        models = []\n",
    "        try:\n",
    "            page_text = self.driver.page_source\n",
    "            \n",
    "            # 使用正则表达式查找模型模式\n",
    "            patterns = [\n",
    "                # 匹配 用户名/模型名 的模式\n",
    "                r'([a-zA-Z0-9_.-]+\\/[a-zA-Z0-9_.-]+)(?=\\s|\"|\\<|$)',\n",
    "                # 匹配href中的模型路径\n",
    "                r'href=\"\\/([a-zA-Z0-9_.-]+\\/[a-zA-Z0-9_.-]+)\"',\n",
    "            ]\n",
    "            \n",
    "            processed_names = set()\n",
    "            for pattern in patterns:\n",
    "                matches = re.findall(pattern, page_text)\n",
    "                for match in matches:\n",
    "                    if isinstance(match, tuple):\n",
    "                        match = match[0] if match[0] else match[1]\n",
    "                    \n",
    "                    if self.is_valid_model_name(match) and match not in processed_names:\n",
    "                        model_info = {\n",
    "                            'name': match,\n",
    "                            'url': f\"https://huggingface.co/{match}\",\n",
    "                            'display_text': \"\",\n",
    "                            'parent_text': \"\",\n",
    "                            'href': f\"/{match}\"\n",
    "                        }\n",
    "                        models.append(model_info)\n",
    "                        processed_names.add(match)\n",
    "                        logger.debug(f\"从页面文本找到模型: {match}\")\n",
    "            \n",
    "            return models\n",
    "            \n",
    "        except Exception as e:\n",
    "            logger.error(f\"从页面文本提取模型时出错: {e}\")\n",
    "            return []\n",
    "    \n",
    "    def extract_model_name_from_url(self, url):\n",
    "        \"\"\"\n",
    "        从URL中提取模型名称\n",
    "        \"\"\"\n",
    "        try:\n",
    "            if not url:\n",
    "                return None\n",
    "            \n",
    "            # 处理完整URL和相对路径\n",
    "            if url.startswith('http'):\n",
    "                parsed = urlparse(url)\n",
    "                path = parsed.path\n",
    "            else:\n",
    "                path = url\n",
    "            \n",
    "            # 移除开头的斜杠并分割路径\n",
    "            path = path.strip('/')\n",
    "            path_parts = path.split('/')\n",
    "            \n",
    "            # 模型路径应该是 用户名/模型名 的格式\n",
    "            if len(path_parts) >= 2:\n",
    "                username = path_parts[0]\n",
    "                modelname = path_parts[1]\n",
    "                \n",
    "                # 验证格式\n",
    "                if (username and modelname and \n",
    "                    re.match(r'^[a-zA-Z0-9_.-]+$', username) and \n",
    "                    re.match(r'^[a-zA-Z0-9_.-]+$', modelname) and\n",
    "                    not self.is_excluded_path(f\"{username}/{modelname}\")):\n",
    "                    return f\"{username}/{modelname}\"\n",
    "            \n",
    "            return None\n",
    "        except Exception as e:\n",
    "            logger.debug(f\"从URL提取模型名称时出错: {e}\")\n",
    "            return None\n",
    "    \n",
    "    def is_excluded_path(self, path):\n",
    "        \"\"\"\n",
    "        检查路径是否应该被排除\n",
    "        \"\"\"\n",
    "        exclude_patterns = [\n",
    "            'models/tasks', 'models/languages', 'models/libraries',\n",
    "            'models/licenses', 'docs/', 'datasets/', 'spaces/',\n",
    "            'settings/', 'login', 'join', 'pricing', 'enterprise'\n",
    "        ]\n",
    "        \n",
    "        return any(pattern in path.lower() for pattern in exclude_patterns)\n",
    "    \n",
    "    def extract_from_page_source(self):\n",
    "        \"\"\"\n",
    "        从页面源代码中使用正则表达式提取模型信息\n",
    "        \"\"\"\n",
    "        models = []\n",
    "        try:\n",
    "            page_source = self.driver.page_source\n",
    "            \n",
    "            # 更精确的正则表达式模式\n",
    "            patterns = [\n",
    "                r'href=\"\\/([a-zA-Z0-9_.-]+\\/[a-zA-Z0-9_.-]+)\"',\n",
    "                r'\"\\/([a-zA-Z0-9_.-]+\\/[a-zA-Z0-9_.-]+)\"',\n",
    "                r'>([a-zA-Z0-9_.-]+\\/[a-zA-Z0-9_.-]+)<',\n",
    "            ]\n",
    "            \n",
    "            processed_names = set()\n",
    "            for pattern in patterns:\n",
    "                matches = re.findall(pattern, page_source)\n",
    "                for match in matches:\n",
    "                    if self.is_valid_model_name(match) and match not in processed_names:\n",
    "                        model_info = {\n",
    "                            'name': match,\n",
    "                            'url': f\"https://huggingface.co/{match}\",\n",
    "                            'display_text': \"\",\n",
    "                            'parent_text': \"\",\n",
    "                            'href': f\"/{match}\"\n",
    "                        }\n",
    "                        models.append(model_info)\n",
    "                        processed_names.add(match)\n",
    "                        logger.debug(f\"从源代码找到模型: {match}\")\n",
    "            \n",
    "            return models\n",
    "            \n",
    "        except Exception as e:\n",
    "            logger.error(f\"从页面源代码提取模型时出错: {e}\")\n",
    "            return []\n",
    "    \n",
    "    def is_valid_model_name(self, name):\n",
    "        \"\"\"\n",
    "        验证是否是有效的模型名称\n",
    "        \"\"\"\n",
    "        if not name or '/' not in name:\n",
    "            return False\n",
    "        \n",
    "        parts = name.split('/')\n",
    "        if len(parts) != 2:\n",
    "            return False\n",
    "        \n",
    "        username, modelname = parts\n",
    "        \n",
    "        # 检查基本格式\n",
    "        if (len(username) < 1 or len(modelname) < 1 or\n",
    "            not re.match(r'^[a-zA-Z0-9_.-]+$', username) or \n",
    "            not re.match(r'^[a-zA-Z0-9_.-]+$', modelname)):\n",
    "            return False\n",
    "        \n",
    "        # 排除明显不是模型的路径\n",
    "        if self.is_excluded_path(name):\n",
    "            return False\n",
    "        \n",
    "        return True\n",
    "    \n",
    "    def find_next_page_element(self):\n",
    "        \"\"\"\n",
    "        查找下一页元素 - 改进版\n",
    "        \"\"\"\n",
    "        try:\n",
    "            # 先滚动到页面底部确保分页元素可见\n",
    "            self.driver.execute_script(\"window.scrollTo(0, document.body.scrollHeight);\")\n",
    "            time.sleep(1)\n",
    "            \n",
    "            # 多种策略查找下一页元素\n",
    "            next_strategies = [\n",
    "                # 策略1: 查找包含\"Next\"文本的链接\n",
    "                (\"//a[contains(text(), 'Next')]\", By.XPATH),\n",
    "                # 策略2: 查找aria-label为Next的元素\n",
    "                (\"a[aria-label*='Next']\", By.CSS_SELECTOR),\n",
    "                # 策略3: 查找包含下一页页码的链接\n",
    "                (f\"//a[contains(@href, 'p={self.current_page + 1}')]\", By.XPATH),\n",
    "                # 策略4: 查找分页容器中的最后一个链接\n",
    "                (\".pagination a:last-child\", By.CSS_SELECTOR),\n",
    "                # 策略5: 通过分页逻辑查找\n",
    "                (\"//a[position()=last() and contains(@href, 'p=')]\", By.XPATH),\n",
    "            ]\n",
    "            \n",
    "            for selector, by_type in next_strategies:\n",
    "                try:\n",
    "                    if by_type == By.XPATH:\n",
    "                        elements = self.driver.find_elements(By.XPATH, selector)\n",
    "                    else:\n",
    "                        elements = self.driver.find_elements(By.CSS_SELECTOR, selector)\n",
    "                    \n",
    "                    for element in elements:\n",
    "                        if (element.is_displayed() and element.is_enabled() and\n",
    "                            self.is_next_page_element(element)):\n",
    "                            logger.info(f\"找到下一页元素，策略: {selector}\")\n",
    "                            return element\n",
    "                            \n",
    "                except Exception as e:\n",
    "                    logger.debug(f\"策略 {selector} 失败: {e}\")\n",
    "                    continue\n",
    "            \n",
    "            logger.info(\"未找到下一页元素\")\n",
    "            return None\n",
    "            \n",
    "        except Exception as e:\n",
    "            logger.error(f\"查找下一页元素时出错: {e}\")\n",
    "            return None\n",
    "    \n",
    "    def is_next_page_element(self, element):\n",
    "        \"\"\"\n",
    "        验证元素是否是下一页链接\n",
    "        \"\"\"\n",
    "        try:\n",
    "            # 检查文本内容\n",
    "            text = element.text.lower().strip()\n",
    "            if 'next' in text or '>' in text:\n",
    "                return True\n",
    "            \n",
    "            # 检查href属性\n",
    "            href = element.get_attribute('href')\n",
    "            if href and f'p={self.current_page + 1}' in href:\n",
    "                return True\n",
    "            \n",
    "            # 检查aria-label\n",
    "            aria_label = element.get_attribute('aria-label')\n",
    "            if aria_label and 'next' in aria_label.lower():\n",
    "                return True\n",
    "            \n",
    "            return False\n",
    "            \n",
    "        except Exception as e:\n",
    "            logger.debug(f\"验证下一页元素时出错: {e}\")\n",
    "            return False\n",
    "    \n",
    "    def go_to_next_page(self):\n",
    "        \"\"\"\n",
    "        跳转到下一页 - 改进版\n",
    "        \"\"\"\n",
    "        try:\n",
    "            next_element = self.find_next_page_element()\n",
    "            \n",
    "            if not next_element:\n",
    "                logger.info(\"未找到下一页元素，可能已到最后一页\")\n",
    "                return False\n",
    "            \n",
    "            # 获取下一页URL\n",
    "            next_url = next_element.get_attribute('href')\n",
    "            if not next_url:\n",
    "                logger.error(\"下一页元素没有href属性\")\n",
    "                return False\n",
    "            \n",
    "            # 记录当前页面URL用于验证\n",
    "            current_url = self.driver.current_url\n",
    "            \n",
    "            # 方法1: 直接点击元素\n",
    "            try:\n",
    "                # 滚动到元素位置\n",
    "                self.driver.execute_script(\"arguments[0].scrollIntoView({block: 'center'});\", next_element)\n",
    "                time.sleep(1)\n",
    "                \n",
    "                # 点击元素\n",
    "                next_element.click()\n",
    "                logger.info(\"已点击下一页元素\")\n",
    "                \n",
    "            except ElementClickInterceptedException:\n",
    "                # 方法2: 如果点击被拦截，使用JavaScript点击\n",
    "                logger.info(\"点击被拦截，尝试JavaScript点击\")\n",
    "                self.driver.execute_script(\"arguments[0].click();\", next_element)\n",
    "                \n",
    "            except Exception as e:\n",
    "                # 方法3: 如果点击失败，直接导航到URL\n",
    "                logger.info(f\"点击失败 ({e})，直接导航到下一页URL\")\n",
    "                self.driver.get(next_url)\n",
    "            \n",
    "            # 等待页面加载\n",
    "            time.sleep(self.delay)\n",
    "            \n",
    "            # 验证页面是否真的发生了变化\n",
    "            new_url = self.driver.current_url\n",
    "            if new_url == current_url:\n",
    "                logger.warning(\"页面URL没有变化，可能跳转失败\")\n",
    "                return False\n",
    "            \n",
    "            # 等待新页面内容加载\n",
    "            try:\n",
    "                self.wait.until(EC.presence_of_element_located((By.TAG_NAME, \"main\")))\n",
    "                time.sleep(2)  # 额外等待确保内容完全加载\n",
    "            except TimeoutException:\n",
    "                logger.warning(\"新页面加载超时，但继续尝试\")\n",
    "            \n",
    "            # 更新当前页码\n",
    "            self.current_page += 1\n",
    "            logger.info(f\"成功跳转到第 {self.current_page} 页\")\n",
    "            return True\n",
    "            \n",
    "        except Exception as e:\n",
    "            logger.error(f\"跳转下一页时出错: {e}\")\n",
    "            return False\n",
    "    \n",
    "    def crawl_all_pages(self, start_url, max_pages=None):\n",
    "        \"\"\"\n",
    "        爬取所有页面 - 改进版\n",
    "        \"\"\"\n",
    "        all_models = []\n",
    "        self.current_page = 1\n",
    "        \n",
    "        # 加载第一页\n",
    "        if not self.load_page(start_url):\n",
    "            logger.error(\"无法加载起始页面\")\n",
    "            return []\n",
    "        \n",
    "        while True:\n",
    "            logger.info(f\"正在处理第 {self.current_page} 页\")\n",
    "            \n",
    "            # 提取当前页面的模型\n",
    "            page_models = self.extract_models_from_current_page()\n",
    "            \n",
    "            if page_models:\n",
    "                all_models.extend(page_models)\n",
    "                logger.info(f\"第 {self.current_page} 页找到 {len(page_models)} 个模型\")\n",
    "                \n",
    "                # 显示前几个找到的模型作为验证\n",
    "                for i, model in enumerate(page_models[:3], 1):\n",
    "                    logger.info(f\"  {i}. {model['name']}\")\n",
    "                \n",
    "            else:\n",
    "                logger.warning(f\"第 {self.current_page} 页没有找到模型\")\n",
    "                # 如果连续多页都没有找到模型，可能是页面结构变化了\n",
    "                if self.current_page > 1:\n",
    "                    logger.error(\"可能遇到了页面结构变化，停止爬取\")\n",
    "                    break\n",
    "            \n",
    "            # 尝试跳转到下一页\n",
    "            if not self.go_to_next_page():\n",
    "                logger.info(\"没有更多页面或跳转失败\")\n",
    "                break\n",
    "        \n",
    "        logger.info(f\"爬取完成，总共处理 {self.current_page} 页，找到 {len(all_models)} 个模型\")\n",
    "        return all_models\n",
    "    \n",
    "    def save_results(self, models, filename_prefix=\"huggingface_models_fixed\"):\n",
    "        \"\"\"\n",
    "        保存结果到文件 - 已禁用，不保存任何文件\n",
    "        \"\"\"\n",
    "        # 不执行任何保存操作\n",
    "        pass\n",
    "    \n",
    "    def close(self):\n",
    "        \"\"\"\n",
    "        关闭浏览器\n",
    "        \"\"\"\n",
    "        if hasattr(self, 'driver'):\n",
    "            self.driver.quit()\n",
    "            logger.info(\"浏览器已关闭\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "6d450a06",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "开始批量爬取HuggingFace模型...\n",
      "\n",
      "============================================================\n",
      "开始爬取第 1/1 个网站\n",
      "URL: https://huggingface.co/models?other=sci\n",
      "============================================================\n",
      "分类: sci\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-08-26 11:39:57,083 - INFO - Chrome浏览器驱动初始化成功\n",
      "2025-08-26 11:39:57,087 - INFO - 正在加载页面: https://huggingface.co/models?other=sci\n",
      "2025-08-26 11:40:05,418 - INFO - 正在处理第 1 页\n",
      "2025-08-26 11:40:07,620 - INFO - 使用策略 'a[href^='/']' 找到 3 个模型\n",
      "2025-08-26 11:40:07,621 - INFO - 第 1 页找到 3 个模型\n",
      "2025-08-26 11:40:07,621 - INFO -   1. search/full-text\n",
      "2025-08-26 11:40:07,621 - INFO -   2. malteos/aspect-acl-scibert-scivocab-uncased\n",
      "2025-08-26 11:40:07,622 - INFO -   3. malteos/aspect-cord19-scibert-scivocab-uncased\n",
      "2025-08-26 11:40:08,670 - INFO - 未找到下一页元素\n",
      "2025-08-26 11:40:08,671 - INFO - 未找到下一页元素，可能已到最后一页\n",
      "2025-08-26 11:40:08,671 - INFO - 没有更多页面或跳转失败\n",
      "2025-08-26 11:40:08,671 - INFO - 爬取完成，总共处理 1 页，找到 3 个模型\n",
      "2025-08-26 11:40:08,758 - INFO - 浏览器已关闭\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "sci 分类爬取结果:\n",
      "- 找到 3 个唯一模型\n",
      "\n",
      "前5个模型:\n",
      "  1. search/full-text\n",
      "  2. malteos/aspect-acl-scibert-scivocab-uncased\n",
      "  3. malteos/aspect-cord19-scibert-scivocab-uncased\n",
      "\n",
      "================================================================================\n",
      "批量爬取完成总结\n",
      "================================================================================\n",
      "sci          | 模型数量:    3 | 状态: 成功\n",
      "\n",
      "总计爬取到 3 个模型\n",
      "\n",
      "✅ 模型数据已保存为CSV: 备份_中间流程结果/models_data.csv\n",
      "   总共包含 3 个模型\n"
     ]
    }
   ],
   "source": [
    "def batch_crawl_huggingface_models(save_path=None):\n",
    "    \"\"\"\n",
    "    批量爬取多个HuggingFace模型分类页面\n",
    "    \n",
    "    Args:\n",
    "        save_path: CSV文件保存路径，如果为None则使用默认路径\n",
    "    \"\"\"\n",
    "    # 要爬取的URL列表\n",
    "    urls_to_crawl = [\n",
    "        \"https://huggingface.co/models?other=climate\",\n",
    "        \"https://huggingface.co/models?other=science\"\n",
    "    ]\n",
    "    \n",
    "    all_results = {}\n",
    "    all_models_combined = []  # 存储所有模型数据\n",
    "    \n",
    "    for i, url in enumerate(urls_to_crawl, 1):\n",
    "        print(f\"\\n{'='*60}\")\n",
    "        print(f\"开始爬取第 {i}/{len(urls_to_crawl)} 个网站\")\n",
    "        print(f\"URL: {url}\")\n",
    "        print(f\"{'='*60}\")\n",
    "        \n",
    "        # 从URL中提取分类名称\n",
    "        category = \"\"\n",
    "        try:\n",
    "            from urllib.parse import urlparse, parse_qs\n",
    "            parsed_url = urlparse(url)\n",
    "            query_params = parse_qs(parsed_url.query)\n",
    "            if 'other' in query_params:\n",
    "                category = query_params['other'][0]\n",
    "        except:\n",
    "            category = f\"category_{i}\"\n",
    "        \n",
    "        print(f\"分类: {category}\")\n",
    "        \n",
    "        # 创建爬虫实例\n",
    "        crawler = FixedHuggingFaceModelsCrawler(headless=True, delay=3)\n",
    "        \n",
    "        try:\n",
    "            # 爬取当前URL\n",
    "            models = crawler.crawl_all_pages(url)  # 限制为5页用于测试\n",
    "            \n",
    "            # 去重\n",
    "            unique_models = []\n",
    "            seen_names = set()\n",
    "            for model in models:\n",
    "                if model['name'] not in seen_names:\n",
    "                    unique_models.append(model)\n",
    "                    seen_names.add(model['name'])\n",
    "            \n",
    "            print(f\"\\n{category} 分类爬取结果:\")\n",
    "            print(f\"- 找到 {len(unique_models)} 个唯一模型\")\n",
    "            \n",
    "            # 显示前5个模型作为示例\n",
    "            if unique_models:\n",
    "                print(\"\\n前5个模型:\")\n",
    "                for j, model in enumerate(unique_models[:5], 1):\n",
    "                    print(f\"  {j}. {model['name']}\")\n",
    "                \n",
    "                # 为合并的CSV添加类别标记\n",
    "                for model in unique_models:\n",
    "                    model['category'] = category\n",
    "                all_models_combined.extend(unique_models)\n",
    "                \n",
    "                # 存储到总结果中\n",
    "                all_results[category] = {\n",
    "                    'url': url,\n",
    "                    'models_count': len(unique_models),\n",
    "                    'models': unique_models[:10]  # 只保存前10个作为示例\n",
    "                }\n",
    "            else:\n",
    "                print(\"  未找到任何模型\")\n",
    "                all_results[category] = {\n",
    "                    'url': url,\n",
    "                    'models_count': 0,\n",
    "                    'models': []\n",
    "                }\n",
    "        \n",
    "        except Exception as e:\n",
    "            print(f\"爬取 {category} 时出现错误: {e}\")\n",
    "            all_results[category] = {\n",
    "                'url': url,\n",
    "                'models_count': 0,\n",
    "                'models': [],\n",
    "                'error': str(e)\n",
    "            }\n",
    "        \n",
    "        finally:\n",
    "            # 关闭当前爬虫实例\n",
    "            crawler.close()\n",
    "            \n",
    "        # 在两次爬取之间暂停\n",
    "        if i < len(urls_to_crawl):\n",
    "            print(f\"\\n等待5秒后开始下一个...\")\n",
    "            time.sleep(5)\n",
    "    \n",
    "    # 输出总结\n",
    "    print(f\"\\n{'='*80}\")\n",
    "    print(\"批量爬取完成总结\")\n",
    "    print(f\"{'='*80}\")\n",
    "    \n",
    "    total_models = 0\n",
    "    for category, result in all_results.items():\n",
    "        status = \"成功\" if result['models_count'] > 0 else \"失败\" if 'error' in result else \"无结果\"\n",
    "        print(f\"{category:12} | 模型数量: {result['models_count']:4d} | 状态: {status}\")\n",
    "        total_models += result['models_count']\n",
    "    \n",
    "    print(f\"\\n总计爬取到 {total_models} 个模型\")\n",
    "    \n",
    "    # 保存合并的CSV文件\n",
    "    if all_models_combined:\n",
    "        if save_path is None:\n",
    "            # 默认文件名\n",
    "            timestamp = time.strftime(\"%Y%m%d_%H%M%S\")\n",
    "            save_path = f\"huggingface_models_combined_{timestamp}.csv\"\n",
    "        \n",
    "        # 创建DataFrame并保存为CSV\n",
    "        import pandas as pd\n",
    "        df = pd.DataFrame(all_models_combined)\n",
    "        \n",
    "        # 重新排列列的顺序，把category放在前面\n",
    "        columns = ['category', 'name', 'url', 'display_text', 'parent_text']\n",
    "        df = df.reindex(columns=[col for col in columns if col in df.columns])\n",
    "        \n",
    "        df.to_csv(save_path, index=False, encoding='utf-8')\n",
    "        print(f\"\\n✅ 模型数据已保存为CSV: {save_path}\")\n",
    "        print(f\"   总共包含 {len(df)} 个模型\")\n",
    "    \n",
    "    return all_results\n",
    "\n",
    "# 执行批量爬取\n",
    "print(\"开始批量爬取HuggingFace模型...\")\n",
    "\n",
    "# 用户可以指定CSV文件保存路径\n",
    "custom_csv_path = \"备份_中间流程结果/models_data.csv\"  # 用户可以修改这个路径\n",
    "\n",
    "batch_results = batch_crawl_huggingface_models(save_path=custom_csv_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "0e8e7958",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "df=pd.read_csv(\"备份_中间流程结果/models_data.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "1ad67816",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "处理数据集的name列...\n",
      "name列示例 (前5个):\n",
      "  1. search/full-text\n",
      "  2. malteos/aspect-acl-scibert-scivocab-uncased\n",
      "  3. malteos/aspect-cord19-scibert-scivocab-uncased\n",
      "分割完成:\n",
      "  - 有作者信息的条目: 3\n",
      "  - 没有作者信息的条目: 0\n"
     ]
    }
   ],
   "source": [
    "# 2. 对两个表格的name列进行处理：根据/分割左侧为作者，右侧为数据集/模型名称\n",
    "\n",
    "def split_name_column(df, df_type):\n",
    "    \"\"\"\n",
    "    分割name列为author和item_name列\n",
    "    \"\"\"\n",
    "    print(f\"\\n处理{df_type}的name列...\")\n",
    "    \n",
    "    # 创建副本避免修改原数据\n",
    "    df_copy = df.copy()\n",
    "    \n",
    "    # 查看一些name列的示例\n",
    "    print(f\"name列示例 (前5个):\")\n",
    "    for i, name in enumerate(df_copy['name'].head()):\n",
    "        print(f\"  {i+1}. {name}\")\n",
    "    \n",
    "    # 分割name列\n",
    "    split_names = df_copy['name'].str.split('/', n=1, expand=True)\n",
    "    \n",
    "    # 添加author和item_name列\n",
    "    df_copy['author'] = split_names[0] if len(split_names.columns) > 0 else ''\n",
    "    df_copy['item_name'] = split_names[1] if len(split_names.columns) > 1 else ''\n",
    "    \n",
    "    # 处理没有/的情况（整个name作为item_name，author为空）\n",
    "    mask_no_slash = df_copy['item_name'].isna()\n",
    "    df_copy.loc[mask_no_slash, 'item_name'] = df_copy.loc[mask_no_slash, 'author']\n",
    "    df_copy.loc[mask_no_slash, 'author'] = ''\n",
    "    \n",
    "    # 清理空值\n",
    "    df_copy['author'] = df_copy['author'].fillna('')\n",
    "    df_copy['item_name'] = df_copy['item_name'].fillna('')\n",
    "    \n",
    "    print(f\"分割完成:\")\n",
    "    print(f\"  - 有作者信息的条目: {(df_copy['author'] != '').sum()}\")\n",
    "    print(f\"  - 没有作者信息的条目: {(df_copy['author'] == '').sum()}\")\n",
    "    \n",
    "    return df_copy\n",
    "\n",
    "# 处理datasets_df\n",
    "models_df_processed = split_name_column(df, \"数据集\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "b6fd3c9b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "==================================================\n",
      "开始处理display_text列，提取更新日期...\n",
      "\n",
      "提取的更新日期示例 (前10个):\n",
      "  1. None\n",
      "  2. 2023-02-07\n",
      "  3. 2021-11-22\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "from datetime import datetime, timedelta\n",
    "\n",
    "# 3. 处理display_text列，提取更新日期\n",
    "def extract_update_date(display_text):\n",
    "    \"\"\"\n",
    "    从display_text中提取更新日期\n",
    "    \"\"\"\n",
    "    if pd.isna(display_text) or display_text == '':\n",
    "        return None\n",
    "    \n",
    "    # 按•分割\n",
    "    parts = display_text.split('•')\n",
    "    \n",
    "    # 查找包含Updated的部分\n",
    "    update_part = None\n",
    "    for part in parts:\n",
    "        if 'Updated' in part:\n",
    "            update_part = part.strip()\n",
    "            break\n",
    "    \n",
    "    if not update_part:\n",
    "        return None\n",
    "    \n",
    "    # 提取Updated后的日期部分\n",
    "    # 使用正则表达式匹配Updated后的内容\n",
    "    match = re.search(r'Updated\\s+(.+)', update_part)\n",
    "    if not match:\n",
    "        return None\n",
    "    \n",
    "    date_str = match.group(1).strip()\n",
    "    \n",
    "    try:\n",
    "        # 1. 处理完整年月日格式，如 \"Jun 5, 2024\"\n",
    "        if re.match(r'^[A-Za-z]{3}\\s+\\d{1,2},\\s+\\d{4}$', date_str):\n",
    "            return datetime.strptime(date_str, '%b %d, %Y').strftime('%Y-%m-%d')\n",
    "        \n",
    "        # 2. 处理没有年份的格式，如 \"Mar 8\"，默认年份为2025\n",
    "        elif re.match(r'^[A-Za-z]{3}\\s+\\d{1,2}$', date_str):\n",
    "            date_with_year = f\"{date_str}, 2025\"\n",
    "            return datetime.strptime(date_with_year, '%b %d, %Y').strftime('%Y-%m-%d')\n",
    "        \n",
    "        # 3. 处理相对时间格式，如 \"9 days ago\"\n",
    "        elif 'ago' in date_str.lower():\n",
    "            base_date = datetime(2025, 8, 22)  # 基准日期\n",
    "            \n",
    "            # 匹配数字和时间单位\n",
    "            time_match = re.search(r'(\\d+)\\s+(day|month|year|week|hour|minute)s?\\s+ago', date_str.lower())\n",
    "            if time_match:\n",
    "                number = int(time_match.group(1))\n",
    "                unit = time_match.group(2)\n",
    "                \n",
    "                if unit == 'day':\n",
    "                    result_date = base_date - timedelta(days=number)\n",
    "                elif unit == 'week':\n",
    "                    result_date = base_date - timedelta(weeks=number)\n",
    "                elif unit == 'month':\n",
    "                    # 简单处理，假设一个月30天\n",
    "                    result_date = base_date - timedelta(days=number * 30)\n",
    "                elif unit == 'year':\n",
    "                    # 简单处理，假设一年365天\n",
    "                    result_date = base_date - timedelta(days=number * 365)\n",
    "                elif unit == 'hour':\n",
    "                    result_date = base_date - timedelta(hours=number)\n",
    "                elif unit == 'minute':\n",
    "                    result_date = base_date - timedelta(minutes=number)\n",
    "                else:\n",
    "                    return None\n",
    "                \n",
    "                return result_date.strftime('%Y-%m-%d')\n",
    "        \n",
    "        # 其他格式尝试直接解析\n",
    "        else:\n",
    "            return None\n",
    "            \n",
    "    except Exception as e:\n",
    "        return None\n",
    "\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "print(\"开始处理display_text列，提取更新日期...\")\n",
    "\n",
    "# 查看一些display_text的样例\n",
    "models_df_processed['update_date'] = models_df_processed['display_text'].apply(extract_update_date)\n",
    "print(\"\\n提取的更新日期示例 (前10个):\")\n",
    "for i, date in enumerate(models_df_processed['update_date'].head(10)):\n",
    "    print(f\"  {i+1}. {date}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "b0a46410",
   "metadata": {},
   "outputs": [],
   "source": [
    "models_df_processed.to_csv(custom_csv_path, index=False, encoding='utf-8')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c54d796",
   "metadata": {},
   "source": [
    "# 爬取模型详细信息"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "0666bdbe",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "df=pd.read_csv(\"备份_中间流程结果/models_data.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "f6d42b54",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import re\n",
    "import json\n",
    "from typing import Dict, List, Optional\n",
    "\n",
    "class HuggingFaceModelScraper:\n",
    "    def __init__(self):\n",
    "        self.session = requests.Session()\n",
    "        self.session.headers.update({\n",
    "            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36'\n",
    "        })\n",
    "        \n",
    "        # 定义不同类型标签的识别规则\n",
    "        self.tag_categories = {\n",
    "            \"frameworks\": [\"transformers\", \"pytorch\", \"tensorflow\", \"jax\", \"safetensors\", \"onnx\", \"keras\"],\n",
    "            \"tasks\": [\"fill-mask\", \"text-classification\", \"token-classification\", \"text-generation\", \n",
    "                     \"question-answering\", \"summarization\", \"translation\", \"text2text-generation\",\n",
    "                     \"image-classification\", \"object-detection\", \"image-segmentation\"],\n",
    "            \"languages\": [\"english\", \"chinese\", \"spanish\", \"french\", \"german\", \"japanese\", \"korean\",\n",
    "                         \"arabic\", \"russian\", \"portuguese\", \"italian\", \"dutch\", \"multilingual\"],\n",
    "            \"model_types\": [\"bert\", \"roberta\", \"gpt\", \"t5\", \"bart\", \"distilbert\", \"albert\", \n",
    "                           \"electra\", \"deberta\", \"llama\", \"mistral\", \"gemma\"],\n",
    "            \"domains\": [\"climate\", \"medical\", \"biology\", \"science\", \"scientific\", \"chemistry\", \"clinical\"]\n",
    "        }\n",
    "    \n",
    "    def extract_model_info(self, url: str) -> Dict:\n",
    "        \"\"\"\n",
    "        从 Hugging Face model 页面提取关键信息\n",
    "        \n",
    "        Args:\n",
    "            url: model 页面的 URL\n",
    "            \n",
    "        Returns:\n",
    "            包含提取信息的字典\n",
    "        \"\"\"\n",
    "        try:\n",
    "            response = self.session.get(url)\n",
    "            response.raise_for_status()\n",
    "            soup = BeautifulSoup(response.content, 'html.parser')\n",
    "            \n",
    "            # 提取模型名称\n",
    "            model_name = self._extract_model_name(soup, url)\n",
    "            \n",
    "            # 提取标签信息\n",
    "            metadata = self._extract_metadata_tags(soup)\n",
    "            \n",
    "            # 提取文字描述\n",
    "            description = self._extract_description(soup)\n",
    "            \n",
    "            # 提取统计信息\n",
    "            stats = self._extract_stats(soup)\n",
    "            \n",
    "            result = {\n",
    "                \"model_name\": model_name,\n",
    "                \"url\": url,\n",
    "                \"metadata\": metadata,\n",
    "                \"description\": description,\n",
    "                \"stats\": stats,\n",
    "                \"extraction_status\": \"success\"\n",
    "            }\n",
    "            \n",
    "            return result\n",
    "            \n",
    "        except Exception as e:\n",
    "            return {\n",
    "                \"model_name\": None,\n",
    "                \"url\": url,\n",
    "                \"metadata\": {},\n",
    "                \"description\": [],\n",
    "                \"stats\": {},\n",
    "                \"extraction_status\": f\"error: {str(e)}\"\n",
    "            }\n",
    "    \n",
    "    def _extract_model_name(self, soup: BeautifulSoup, url: str) -> str:\n",
    "        \"\"\"提取模型名称\"\"\"\n",
    "        # 尝试从页面标题提取\n",
    "        title_element = soup.find('h1')\n",
    "        if title_element:\n",
    "            return title_element.get_text().strip()\n",
    "        \n",
    "        # 从 URL 提取\n",
    "        url_parts = url.rstrip('/').split('/')\n",
    "        if len(url_parts) >= 2:\n",
    "            return url_parts[-1]\n",
    "        \n",
    "        return \"Unknown\"\n",
    "    \n",
    "    def _extract_metadata_tags(self, soup: BeautifulSoup) -> Dict:\n",
    "        \"\"\"提取模型元数据信息\"\"\"\n",
    "        metadata = {\n",
    "            \"frameworks\": [],\n",
    "            \"tasks\": [],\n",
    "            \"languages\": [],\n",
    "            \"model_types\": [],\n",
    "            \"domains\": [],\n",
    "            \"license\": \"\",\n",
    "            \"arxiv\": \"\",\n",
    "            \"other_tags\": []\n",
    "        }\n",
    "        \n",
    "        # 方法1: 提取顶部标签区域的所有标签\n",
    "        self._extract_header_tags(soup, metadata)\n",
    "        \n",
    "        # 方法2: 通过特定选择器查找标签\n",
    "        self._extract_by_selectors(soup, metadata)\n",
    "        \n",
    "        # 方法3: 通过正则表达式查找特定信息\n",
    "        self._extract_specific_info(soup, metadata)\n",
    "        \n",
    "        return metadata\n",
    "    \n",
    "    def _extract_header_tags(self, soup: BeautifulSoup, metadata: Dict):\n",
    "        \"\"\"提取页面头部标签区域的所有标签\"\"\"\n",
    "        # 常见的标签容器选择器\n",
    "        tag_selectors = [\n",
    "            'div[class*=\"tag\"]',\n",
    "            'span[class*=\"tag\"]',\n",
    "            'div[class*=\"badge\"]',\n",
    "            'span[class*=\"badge\"]',\n",
    "            'div[class*=\"label\"]',\n",
    "            'a[class*=\"tag\"]',\n",
    "            # 更具体的选择器\n",
    "            'header div[class*=\"flex\"] > *',\n",
    "            'div[class*=\"model-header\"] span',\n",
    "            'div[class*=\"model-card\"] span',\n",
    "        ]\n",
    "        \n",
    "        found_tags = set()\n",
    "        \n",
    "        for selector in tag_selectors:\n",
    "            try:\n",
    "                elements = soup.select(selector)\n",
    "                for element in elements:\n",
    "                    tag_text = element.get_text().strip().lower()\n",
    "                    if tag_text and len(tag_text) < 50:  # 避免长文本\n",
    "                        found_tags.add(tag_text)\n",
    "            except:\n",
    "                continue\n",
    "        \n",
    "        # 也尝试查找特定的图标 + 文本组合\n",
    "        self._extract_icon_text_combinations(soup, found_tags)\n",
    "        \n",
    "        # 分类标签\n",
    "        self._categorize_tags(found_tags, metadata)\n",
    "    \n",
    "    def _extract_icon_text_combinations(self, soup: BeautifulSoup, found_tags: set):\n",
    "        \"\"\"提取图标+文本组合的标签\"\"\"\n",
    "        # 查找包含SVG图标的元素\n",
    "        icon_containers = soup.find_all(['div', 'span'], attrs={'class': re.compile(r'flex|inline-flex')})\n",
    "        \n",
    "        for container in icon_containers:\n",
    "            # 检查是否包含SVG图标\n",
    "            svg = container.find('svg')\n",
    "            if svg:\n",
    "                # 获取同级或子级的文本\n",
    "                text_elements = container.find_all(string=True)\n",
    "                for text in text_elements:\n",
    "                    clean_text = text.strip().lower()\n",
    "                    if clean_text and len(clean_text) < 30 and clean_text not in ['', ' ', '\\n']:\n",
    "                        found_tags.add(clean_text)\n",
    "    \n",
    "    def _extract_by_selectors(self, soup: BeautifulSoup, metadata: Dict):\n",
    "        \"\"\"通过特定选择器查找标签\"\"\"\n",
    "        # 查找许可证信息\n",
    "        license_selectors = [\n",
    "            'a[href*=\"license\"]',\n",
    "            'span[class*=\"license\"]',\n",
    "            'div[class*=\"license\"]'\n",
    "        ]\n",
    "        \n",
    "        for selector in license_selectors:\n",
    "            try:\n",
    "                license_element = soup.select_one(selector)\n",
    "                if license_element and not metadata[\"license\"]:\n",
    "                    metadata[\"license\"] = license_element.get_text().strip()\n",
    "                    break\n",
    "            except:\n",
    "                continue\n",
    "        \n",
    "        # 查找ArXiv链接\n",
    "        arxiv_links = soup.find_all('a', href=re.compile(r'arxiv\\.org|arxiv:', re.IGNORECASE))\n",
    "        for link in arxiv_links:\n",
    "            href = link.get('href', '')\n",
    "            text = link.get_text().strip()\n",
    "            \n",
    "            # 从href中提取ArXiv ID\n",
    "            arxiv_match = re.search(r'(\\d{4}\\.\\d{4,5})', href + ' ' + text)\n",
    "            if arxiv_match and not metadata[\"arxiv\"]:\n",
    "                metadata[\"arxiv\"] = arxiv_match.group(1)\n",
    "                break\n",
    "    \n",
    "    def _extract_specific_info(self, soup: BeautifulSoup, metadata: Dict):\n",
    "        \"\"\"通过正则表达式查找特定信息\"\"\"\n",
    "        page_text = soup.get_text()\n",
    "        \n",
    "        # 查找ArXiv ID (如果之前没找到)\n",
    "        if not metadata[\"arxiv\"]:\n",
    "            arxiv_pattern = r'arxiv[:\\s]*(\\d{4}\\.\\d{4,5})'\n",
    "            arxiv_match = re.search(arxiv_pattern, page_text, re.IGNORECASE)\n",
    "            if arxiv_match:\n",
    "                metadata[\"arxiv\"] = arxiv_match.group(1)\n",
    "        \n",
    "        # 查找许可证信息 (如果之前没找到)\n",
    "        if not metadata[\"license\"]:\n",
    "            license_patterns = [\n",
    "                r'license[:\\s]*([a-z0-9\\-\\.]+)',\n",
    "                r'(mit|apache-2\\.0|gpl|bsd|cc-by)',\n",
    "            ]\n",
    "            for pattern in license_patterns:\n",
    "                match = re.search(pattern, page_text, re.IGNORECASE)\n",
    "                if match:\n",
    "                    metadata[\"license\"] = match.group(1).strip()\n",
    "                    break\n",
    "    \n",
    "    def _categorize_tags(self, found_tags: set, metadata: Dict):\n",
    "        \"\"\"将找到的标签分类到不同的类别中\"\"\"\n",
    "        for tag in found_tags:\n",
    "            tag_lower = tag.lower()\n",
    "            categorized = False\n",
    "            \n",
    "            # 遍历每个类别进行匹配\n",
    "            for category, keywords in self.tag_categories.items():\n",
    "                for keyword in keywords:\n",
    "                    if keyword in tag_lower or tag_lower in keyword:\n",
    "                        if tag not in metadata[category]:\n",
    "                            metadata[category].append(tag)\n",
    "                        categorized = True\n",
    "                        break\n",
    "                if categorized:\n",
    "                    break\n",
    "            \n",
    "            # 如果没有分类，加入other_tags\n",
    "            if not categorized and len(tag) > 1:\n",
    "                # 过滤一些明显不是标签的内容\n",
    "                if not self._is_non_tag_text(tag):\n",
    "                    metadata[\"other_tags\"].append(tag)\n",
    "    \n",
    "    def _is_non_tag_text(self, text: str) -> bool:\n",
    "        \"\"\"判断文本是否不是标签内容\"\"\"\n",
    "        non_tag_patterns = [\n",
    "            r'^\\d+$',  # 纯数字\n",
    "            r'^[^\\w\\s]+$',  # 纯符号\n",
    "            r'\\b(the|and|or|in|on|at|to|for|of|with|by)\\b',  # 常见介词\n",
    "            r'\\b(click|view|edit|download|upload|submit)\\b',  # 操作词\n",
    "            r'^(https?://|www\\.)',  # URL\n",
    "            r'\\b\\d+\\s*(gb|mb|kb|tb|downloads?|views?|likes?)\\b',  # 统计数据\n",
    "        ]\n",
    "        \n",
    "        text_lower = text.lower()\n",
    "        return any(re.search(pattern, text_lower) for pattern in non_tag_patterns)\n",
    "    \n",
    "    def _extract_description(self, soup: BeautifulSoup) -> List[str]:\n",
    "        \"\"\"提取模型描述\"\"\"\n",
    "        descriptions = []\n",
    "        \n",
    "        # 模型页面的描述区域选择器\n",
    "        description_selectors = [\n",
    "            'div[class*=\"model-card\"]',\n",
    "            'div[class*=\"readme\"]',\n",
    "            'div[class*=\"prose\"]',\n",
    "            'section[class*=\"model-description\"]',\n",
    "            'div[class*=\"content\"]',\n",
    "            'article',\n",
    "            'main',\n",
    "        ]\n",
    "        \n",
    "        description_container = None\n",
    "        for selector in description_selectors:\n",
    "            try:\n",
    "                container = soup.select_one(selector)\n",
    "                if container:\n",
    "                    description_container = container\n",
    "                    break\n",
    "            except:\n",
    "                continue\n",
    "        \n",
    "        if not description_container:\n",
    "            description_container = soup\n",
    "        \n",
    "        # 查找描述段落\n",
    "        paragraphs = description_container.find_all('p')\n",
    "        for p in paragraphs:\n",
    "            text = p.get_text().strip()\n",
    "            if (len(text) > 50 and \n",
    "                not self._is_navigation_text(text) and\n",
    "                self._is_meaningful_description(text)):\n",
    "                descriptions.append(text)\n",
    "                if len(descriptions) >= 2:\n",
    "                    break\n",
    "        \n",
    "        return descriptions\n",
    "    \n",
    "    def _extract_stats(self, soup: BeautifulSoup) -> Dict:\n",
    "        \"\"\"提取统计信息\"\"\"\n",
    "        stats = {\n",
    "            \"downloads\": \"\",\n",
    "            \"likes\": \"\",\n",
    "            \"followers\": \"\",\n",
    "            \"last_updated\": \"\"\n",
    "        }\n",
    "        \n",
    "        # 查找统计数字\n",
    "        stat_patterns = {\n",
    "            \"downloads\": [r'(\\d+(?:,\\d+)*)\\s*downloads?', r'downloads?\\s*(?:last\\s*month)?[:\\s]*(\\d+(?:,\\d+)*)'],\n",
    "            \"likes\": [r'(\\d+(?:,\\d+)*)\\s*likes?', r'likes?[:\\s]*(\\d+(?:,\\d+)*)'],\n",
    "            \"followers\": [r'(\\d+(?:,\\d+)*)\\s*followers?', r'follow.*?(\\d+(?:,\\d+)*)']\n",
    "        }\n",
    "        \n",
    "        page_text = soup.get_text()\n",
    "        \n",
    "        for stat_type, patterns in stat_patterns.items():\n",
    "            for pattern in patterns:\n",
    "                match = re.search(pattern, page_text, re.IGNORECASE)\n",
    "                if match and not stats[stat_type]:\n",
    "                    stats[stat_type] = match.group(1)\n",
    "                    break\n",
    "        \n",
    "        return stats\n",
    "    \n",
    "    def _is_meaningful_description(self, text: str) -> bool:\n",
    "        \"\"\"判断文本是否是有意义的描述内容\"\"\"\n",
    "        descriptive_patterns = [\n",
    "            r'\\b(model|language model|based on|trained|fine-tuned|designed)\\b',\n",
    "            r'\\b(machine learning|deep learning|AI|artificial intelligence|NLP)\\b',\n",
    "            r'\\b(research|study|analysis|algorithm|neural network)\\b',\n",
    "            r'\\b(text|classification|generation|understanding|processing)\\b'\n",
    "        ]\n",
    "        \n",
    "        text_lower = text.lower()\n",
    "        has_descriptive_content = any(re.search(pattern, text_lower) for pattern in descriptive_patterns)\n",
    "        \n",
    "        sentence_count = len([s for s in text.split('.') if len(s.strip()) > 10])\n",
    "        \n",
    "        # 避免纯技术标签或元数据\n",
    "        avoid_patterns = [\n",
    "            r'^\\s*\\d+(\\.\\d+)?\\s*(GB|MB|KB|TB)\\s*$',\n",
    "            r'^\\s*[a-z-]+:[a-z-]+\\s*$',\n",
    "            r'^\\s*(download|view|edit|fork|clone)\\s*$',\n",
    "            r'^\\s*\\d+\\s*(downloads?|views?|likes?)\\s*$',\n",
    "        ]\n",
    "        \n",
    "        is_metadata = any(re.match(pattern, text.strip(), re.IGNORECASE) for pattern in avoid_patterns)\n",
    "        \n",
    "        return (has_descriptive_content and \n",
    "                sentence_count >= 1 and \n",
    "                not is_metadata and\n",
    "                len(text.split()) >= 8)\n",
    "    \n",
    "    def _is_navigation_text(self, text: str) -> bool:\n",
    "        \"\"\"判断是否为导航或菜单文本\"\"\"\n",
    "        if len(text) > 300:\n",
    "            return False\n",
    "            \n",
    "        nav_keywords = [\n",
    "            'home', 'models', 'datasets', 'spaces', 'docs', 'pricing', \n",
    "            'login', 'sign up', 'menu', 'navigation', 'download', 'view',\n",
    "            'edit', 'fork', 'clone', 'settings', 'discussions', 'files',\n",
    "            'community', 'license', 'paper', 'model card'\n",
    "        ]\n",
    "        \n",
    "        text_lower = text.lower().strip()\n",
    "        words = text_lower.split()\n",
    "        \n",
    "        if len(words) <= 5:\n",
    "            nav_word_count = sum(1 for word in words if any(kw in word for kw in nav_keywords))\n",
    "            if nav_word_count >= len(words) * 0.6:\n",
    "                return True\n",
    "        \n",
    "        return False\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "68b18023",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "from tqdm import tqdm\n",
    "import pandas as pd\n",
    "\n",
    "def batch_extract_model_info(df, url_column='url', batch_size=50, delay=1.0):\n",
    "    \"\"\"\n",
    "    批量提取模型信息并添加新列到DataFrame\n",
    "    \n",
    "    Args:\n",
    "        df: 包含URL的DataFrame\n",
    "        url_column: URL列的名称\n",
    "        batch_size: 每批处理的数量\n",
    "        delay: 请求间的延迟时间（秒）\n",
    "    \n",
    "    Returns:\n",
    "        添加了新列的DataFrame\n",
    "    \"\"\"\n",
    "    # 初始化爬虫\n",
    "    scraper = HuggingFaceModelScraper()\n",
    "    \n",
    "    # 为结果创建新列\n",
    "    new_columns = [\n",
    "        'Frameworks', 'Tasks', 'Languages', 'Model_types', \n",
    "        'Domains', 'License', 'Arxiv', 'Description'\n",
    "    ]\n",
    "    \n",
    "    # 初始化新列（如果不存在）\n",
    "    for col in new_columns:\n",
    "        if col not in df.columns:\n",
    "            df[col] = \"\"\n",
    "    \n",
    "    # 记录统计信息\n",
    "    stats = {\n",
    "        'processed': 0,\n",
    "        'successful': 0,\n",
    "        'failed': 0,\n",
    "        'pending': len(df)\n",
    "    }\n",
    "    \n",
    "    print(f\"开始批量提取 {len(df)} 个模型的信息...\")\n",
    "    print(f\"批次大小: {batch_size}, 延迟: {delay}秒\")\n",
    "    \n",
    "    # 使用tqdm显示进度\n",
    "    with tqdm(total=len(df), desc=\"提取进度\") as pbar:\n",
    "        for idx, row in df.iterrows():\n",
    "            try:\n",
    "                url = row[url_column]\n",
    "                \n",
    "                # 检查是否已经有数据（避免重复提取）\n",
    "                if pd.notna(row.get('Frameworks', '')) and row.get('Frameworks', '') != '':\n",
    "                    stats['processed'] += 1\n",
    "                    stats['pending'] -= 1\n",
    "                    pbar.update(1)\n",
    "                    continue\n",
    "                \n",
    "                # 提取模型信息\n",
    "                result = scraper.extract_model_info(url)\n",
    "                \n",
    "                if result[\"extraction_status\"] == \"success\":\n",
    "                    # 更新DataFrame\n",
    "                    metadata = result['metadata']\n",
    "                    \n",
    "                    # 转换列表为字符串（用分号分隔）\n",
    "                    df.at[idx, 'Frameworks'] = '; '.join(metadata.get('frameworks', []))\n",
    "                    df.at[idx, 'Tasks'] = '; '.join(metadata.get('tasks', []))\n",
    "                    df.at[idx, 'Languages'] = '; '.join(metadata.get('languages', []))\n",
    "                    df.at[idx, 'Model_types'] = '; '.join(metadata.get('model_types', []))\n",
    "                    df.at[idx, 'Domains'] = '; '.join(metadata.get('domains', []))\n",
    "                    df.at[idx, 'License'] = metadata.get('license', '')\n",
    "                    df.at[idx, 'Arxiv'] = metadata.get('arxiv', '')\n",
    "                    \n",
    "                    # 合并描述段落\n",
    "                    descriptions = result.get('description', [])\n",
    "                    df.at[idx, 'Description'] = ' '.join(descriptions[:2])  # 只取前两段\n",
    "                    \n",
    "                    stats['successful'] += 1\n",
    "                else:\n",
    "                    stats['failed'] += 1\n",
    "                    # 记录错误\n",
    "                    df.at[idx, 'Description'] = f\"提取失败: {result['extraction_status']}\"\n",
    "                \n",
    "                stats['processed'] += 1\n",
    "                stats['pending'] -= 1\n",
    "                \n",
    "                # 更新进度条描述\n",
    "                pbar.set_postfix({\n",
    "                    '成功': stats['successful'],\n",
    "                    '失败': stats['failed'],\n",
    "                    '剩余': stats['pending']\n",
    "                })\n",
    "                \n",
    "                pbar.update(1)\n",
    "                \n",
    "                # 延迟以避免过度请求\n",
    "                time.sleep(delay)\n",
    "                \n",
    "                # 每处理一定数量后显示统计信息\n",
    "                if stats['processed'] % batch_size == 0:\n",
    "                    print(f\"\\n已处理: {stats['processed']}, 成功: {stats['successful']}, 失败: {stats['failed']}\")\n",
    "                    \n",
    "            except Exception as e:\n",
    "                stats['failed'] += 1\n",
    "                stats['processed'] += 1\n",
    "                stats['pending'] -= 1\n",
    "                df.at[idx, 'Description'] = f\"处理异常: {str(e)}\"\n",
    "                pbar.update(1)\n",
    "                continue\n",
    "    \n",
    "    print(f\"\\n✅ 批量提取完成！\")\n",
    "    print(f\"总处理: {stats['processed']}\")\n",
    "    print(f\"成功: {stats['successful']}\")\n",
    "    print(f\"失败: {stats['failed']}\")\n",
    "    print(f\"成功率: {stats['successful']/stats['processed']*100:.1f}%\")\n",
    "    \n",
    "    return df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "f35f8ac0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/ts/gb6dsxqn34q_l64hh0b980n40000gn/T/ipykernel_66575/1412667419.py:30: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df[col] = \"\"\n",
      "/var/folders/ts/gb6dsxqn34q_l64hh0b980n40000gn/T/ipykernel_66575/1412667419.py:30: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df[col] = \"\"\n",
      "/var/folders/ts/gb6dsxqn34q_l64hh0b980n40000gn/T/ipykernel_66575/1412667419.py:30: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df[col] = \"\"\n",
      "/var/folders/ts/gb6dsxqn34q_l64hh0b980n40000gn/T/ipykernel_66575/1412667419.py:30: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df[col] = \"\"\n",
      "/var/folders/ts/gb6dsxqn34q_l64hh0b980n40000gn/T/ipykernel_66575/1412667419.py:30: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df[col] = \"\"\n",
      "/var/folders/ts/gb6dsxqn34q_l64hh0b980n40000gn/T/ipykernel_66575/1412667419.py:30: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df[col] = \"\"\n",
      "/var/folders/ts/gb6dsxqn34q_l64hh0b980n40000gn/T/ipykernel_66575/1412667419.py:30: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df[col] = \"\"\n",
      "/var/folders/ts/gb6dsxqn34q_l64hh0b980n40000gn/T/ipykernel_66575/1412667419.py:30: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df[col] = \"\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "开始批量提取 3 个模型的信息...\n",
      "批次大小: 100, 延迟: 1.5秒\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "提取进度: 100%|██████████| 3/3 [00:06<00:00,  2.15s/it, 成功=3, 失败=0, 剩余=0]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "✅ 批量提取完成！\n",
      "总处理: 3\n",
      "成功: 3\n",
      "失败: 0\n",
      "成功率: 100.0%\n",
      "\n",
      "=== 提取完成 ===\n",
      "前10条结果预览:\n",
      "category                                           name                                                                   url                                                                     display_text  author                              item_name update_date               Frameworks             Tasks             Languages                    Model_types Domains License      Arxiv                                                                                                                                                                                                                                                                     Description\n",
      "     sci                               search/full-text                               https://huggingface.co/search/full-text                                                                 Full-text search  search                              full-text         NaN                                                                                                                                                                                                                                                                                                                                                                                                           \n",
      "     sci    malteos/aspect-acl-scibert-scivocab-uncased    https://huggingface.co/malteos/aspect-acl-scibert-scivocab-uncased     malteos/aspect-acl-scibert-scivocab-uncased\\nUpdated Feb 7, 2023\\n•\\n7\\n•\\n2 malteos    aspect-acl-scibert-scivocab-uncased  2023-02-07 pytorch; a; transformers classification; 2 english; multilingual scibert-scivocab-uncased; bert             mit 2010.06395 A scibert-scivocab-uncased model fine-tuned on the ACL Anthology corpus as in Aspect-based Document Similarity for Research Papers. You can try our trained models directly on Google Colab on all papers available on Semantic Scholar (via DOI, ArXiv ID, ACL ID, PubMed ID):\n",
      "     sci malteos/aspect-cord19-scibert-scivocab-uncased https://huggingface.co/malteos/aspect-cord19-scibert-scivocab-uncased malteos/aspect-cord19-scibert-scivocab-uncased\\nUpdated Nov 22, 2021\\n•\\n6\\n•\\n1 malteos aspect-cord19-scibert-scivocab-uncased  2021-11-22 pytorch; a; transformers    classification               english scibert-scivocab-uncased; bert             mit 2010.06395       A scibert-scivocab-uncased model fine-tuned on the CORD-19 corpus as in Aspect-based Document Similarity for Research Papers. You can try our trained models directly on Google Colab on all papers available on Semantic Scholar (via DOI, ArXiv ID, ACL ID, PubMed ID):\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# 运行批量提取 (调整延迟为1.5秒以提高效率)\n",
    "models_df_complete = batch_extract_model_info(\n",
    "    df.head(10),    \n",
    "    url_column='url', \n",
    "    batch_size=100,  # 每100条显示一次统计\n",
    "    delay=1.5        # 1.5秒延迟\n",
    ")\n",
    "\n",
    "print(\"\\n=== 提取完成 ===\")\n",
    "print(\"前10条结果预览:\")\n",
    "print(models_df_complete.head(10).to_string(index=False))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5501a731",
   "metadata": {},
   "source": [
    "数据清洗"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "8761e061",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "开始清洗 models_df_complete 数据...\n",
      "原始数据形状: (3, 15)\n",
      "原始列名: ['category', 'name', 'url', 'display_text', 'author', 'item_name', 'update_date', 'Frameworks', 'Tasks', 'Languages', 'Model_types', 'Domains', 'License', 'Arxiv', 'Description']\n",
      "\n",
      "1. 删除 Arxiv、Model_types、Languages 列...\n",
      "已删除列: ['Arxiv', 'Model_types', 'Languages']\n",
      "删除后列名: ['category', 'name', 'url', 'display_text', 'author', 'item_name', 'update_date', 'Frameworks', 'Tasks', 'Domains', 'License', 'Description']\n",
      "\n",
      "2. 根据Tasks创建功能标签...\n",
      "功能标签分布:\n",
      "  自然语言处理: 2\n",
      "  通用处理: 1\n",
      "已删除Tasks列\n",
      "\n",
      "3. 根据Domains和其他信息创建学科列...\n",
      "学科分布:\n",
      "  其他学科: 3\n",
      "有学科分类的模型数量: 3\n",
      "无学科分类的模型数量: 0\n",
      "\n",
      "4. 清洗License列...\n",
      "License清洗后分布 (前15个):\n",
      "  mit: 2\n",
      "\n",
      "✅ 清洗完成！\n",
      "清洗后数据形状: (3, 13)\n",
      "清洗后列名: ['category', 'name', 'url', 'display_text', 'author', 'item_name', 'update_date', 'Frameworks', 'Domains', 'License', 'Description', '功能标签', '学科']\n",
      "\n",
      "清洗后数据示例 (前5行):\n",
      "    author                               item_name    功能标签    学科 License Domains\n",
      "0   search                               full-text    通用处理  其他学科                \n",
      "1  malteos     aspect-acl-scibert-scivocab-uncased  自然语言处理  其他学科     mit        \n",
      "2  malteos  aspect-cord19-scibert-scivocab-uncased  自然语言处理  其他学科     mit        \n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "\n",
    "print(\"开始清洗 models_df_complete 数据...\")\n",
    "print(f\"原始数据形状: {models_df_complete.shape}\")\n",
    "print(f\"原始列名: {list(models_df_complete.columns)}\")\n",
    "\n",
    "# 创建数据副本进行清洗\n",
    "models_cleaned = models_df_complete.copy()\n",
    "\n",
    "# 1. 删除指定列\n",
    "print(\"\\n1. 删除 Arxiv、Model_types、Languages 列...\")\n",
    "columns_to_drop = ['Arxiv', 'Model_types', 'Languages']\n",
    "existing_columns_to_drop = [col for col in columns_to_drop if col in models_cleaned.columns]\n",
    "\n",
    "if existing_columns_to_drop:\n",
    "    models_cleaned = models_cleaned.drop(columns=existing_columns_to_drop)\n",
    "    print(f\"已删除列: {existing_columns_to_drop}\")\n",
    "else:\n",
    "    print(\"未找到要删除的列\")\n",
    "\n",
    "print(f\"删除后列名: {list(models_cleaned.columns)}\")\n",
    "\n",
    "# 2. 根据Tasks创建功能标签列\n",
    "print(\"\\n2. 根据Tasks创建功能标签...\")\n",
    "\n",
    "def classify_function_by_tasks(tasks_str):\n",
    "    \"\"\"\n",
    "    根据Tasks内容分类功能标签\n",
    "    \"\"\"\n",
    "    if pd.isna(tasks_str) or tasks_str == '':\n",
    "        return '通用处理'\n",
    "    \n",
    "    tasks_lower = str(tasks_str).lower()\n",
    "    \n",
    "    # 检查是否包含文本相关任务\n",
    "    if any(keyword in tasks_lower for keyword in ['text', 'language', 'nlp', 'generation', 'classification', 'question', 'translation', 'summarization']):\n",
    "        return '自然语言处理'\n",
    "    \n",
    "    # 检查是否包含视觉相关任务\n",
    "    elif any(keyword in tasks_lower for keyword in ['image', 'video', 'vision', 'detection', 'segmentation', 'visual']):\n",
    "        return '计算机视觉'\n",
    "    \n",
    "    # 检查是否包含语音相关任务\n",
    "    elif any(keyword in tasks_lower for keyword in ['voice', 'speech', 'audio', 'sound', 'acoustic']):\n",
    "        return '语音处理'\n",
    "    \n",
    "    # 其他情况\n",
    "    else:\n",
    "        return '通用处理'\n",
    "\n",
    "# 应用功能分类\n",
    "models_cleaned['功能标签'] = models_cleaned['Tasks'].apply(classify_function_by_tasks)\n",
    "\n",
    "# 统计功能标签分布\n",
    "print(\"功能标签分布:\")\n",
    "function_counts = models_cleaned['功能标签'].value_counts()\n",
    "for func, count in function_counts.items():\n",
    "    print(f\"  {func}: {count}\")\n",
    "\n",
    "# 删除Tasks列\n",
    "if 'Tasks' in models_cleaned.columns:\n",
    "    models_cleaned = models_cleaned.drop('Tasks', axis=1)\n",
    "    print(\"已删除Tasks列\")\n",
    "\n",
    "# 3. 根据Domains和tags创建学科列\n",
    "print(\"\\n3. 根据Domains和其他信息创建学科列...\")\n",
    "\n",
    "def classify_subject_by_domains_and_tags(row):\n",
    "    \"\"\"\n",
    "    根据Domains列和其他信息分类学科，全部关键词用英文\n",
    "    \"\"\"\n",
    "    domains_str = str(row.get('Domains', '')) if pd.notna(row.get('Domains', '')) else ''\n",
    "    tags_str = str(row.get('tags', '')) if pd.notna(row.get('tags', '')) else ''\n",
    "    combined_text = (domains_str + ' ' + tags_str).lower()\n",
    "\n",
    "    # 生命科学\n",
    "    if any(keyword in combined_text for keyword in [\n",
    "        'biology', 'medical', 'bio-', 'medicine', 'health', 'genomics', 'protein', 'clinical'\n",
    "    ]):\n",
    "        return '生命科学'\n",
    "    # 物质科学\n",
    "    elif any(keyword in combined_text for keyword in [\n",
    "        'chemistry', 'chemical', 'molecule', 'compound', 'material'\n",
    "    ]):\n",
    "        return '物质科学'\n",
    "    # 大气海洋\n",
    "    elif any(keyword in combined_text for keyword in [\n",
    "        'climate', 'weather', 'ocean', 'atmospheric', 'meteorology', 'environmental'\n",
    "    ]):\n",
    "        return '大气海洋'\n",
    "    # 空间信息\n",
    "    elif any(keyword in combined_text for keyword in [\n",
    "        'remote sensing', 'gis', 'geospatial', 'spatial', 'satellite', 'cartography', 'earth observation', 'map'\n",
    "    ]):\n",
    "        return '空间信息'\n",
    "    # 工程建设\n",
    "    elif any(keyword in combined_text for keyword in [\n",
    "        'civil engineering', 'structural', 'infrastructure', 'construction', 'engineering', 'bim', 'bridge', 'road', 'tunnel'\n",
    "    ]):\n",
    "        return '工程技术'\n",
    "    # 其他\n",
    "    else:\n",
    "        return '其他学科'\n",
    "\n",
    "# 应用学科分类\n",
    "models_cleaned['学科'] = models_cleaned.apply(classify_subject_by_domains_and_tags, axis=1)\n",
    "\n",
    "# 统计学科分布\n",
    "print(\"学科分布:\")\n",
    "subject_counts = models_cleaned['学科'].value_counts()\n",
    "for subject, count in subject_counts.items():\n",
    "    print(f\"  {subject}: {count}\")\n",
    "\n",
    "print(f\"有学科分类的模型数量: {models_cleaned['学科'].notna().sum()}\")\n",
    "print(f\"无学科分类的模型数量: {models_cleaned['学科'].isna().sum()}\")\n",
    "\n",
    "# 4. 清洗License列\n",
    "print(\"\\n4. 清洗License列...\")\n",
    "\n",
    "def clean_license(license_str):\n",
    "    \"\"\"\n",
    "    清洗License列内容\n",
    "    \"\"\"\n",
    "    if pd.isna(license_str) or license_str == '':\n",
    "        return ''\n",
    "    \n",
    "    license_str = str(license_str)\n",
    "    \n",
    "    # 删除\"📜 License\"\n",
    "    license_str = re.sub(r'📜\\s*License\\s*', '', license_str, flags=re.IGNORECASE)\n",
    "    \n",
    "    # 将other替换为\"其他\"\n",
    "    license_str = re.sub(r'\\bother\\b', '其他', license_str, flags=re.IGNORECASE)\n",
    "    \n",
    "    # 清理额外的空格\n",
    "    license_str = re.sub(r'\\s+', ' ', license_str).strip()\n",
    "    \n",
    "    return license_str\n",
    "\n",
    "# 应用License清洗\n",
    "models_cleaned['License'] = models_cleaned['License'].apply(clean_license)\n",
    "\n",
    "# 统计License分布\n",
    "print(\"License清洗后分布 (前15个):\")\n",
    "license_counts = models_cleaned['License'].value_counts()\n",
    "for license_name, count in license_counts.head(15).items():\n",
    "    if license_name:  # 只显示非空值\n",
    "        print(f\"  {license_name}: {count}\")\n",
    "\n",
    "# 最终结果统计\n",
    "print(f\"\\n✅ 清洗完成！\")\n",
    "print(f\"清洗后数据形状: {models_cleaned.shape}\")\n",
    "print(f\"清洗后列名: {list(models_cleaned.columns)}\")\n",
    "\n",
    "# 显示一些示例数据\n",
    "print(f\"\\n清洗后数据示例 (前5行):\")\n",
    "display_columns = ['author', 'item_name', '功能标签', '学科', 'License', 'Domains']\n",
    "available_columns = [col for col in display_columns if col in models_cleaned.columns]\n",
    "print(models_cleaned[available_columns].head().to_string())\n",
    "\n",
    "# 保存清洗后的数据\n",
    "models_df_complete = models_cleaned.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "0fea6941",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "开始清洗License列...\n",
      "License列清洗完成！\n",
      "\n",
      "清洗后的License列统计:\n",
      "License\n",
      "mit    2\n",
      "公开     1\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "def clean_license_column(license_text):\n",
    "    \"\"\"\n",
    "    清洗开源协议列：\n",
    "    1. 字符数大于6的替换为\"公开\"\n",
    "    2. 空值替换为\"公开\"\n",
    "    3. 其他情况也替换为\"公开\"\n",
    "    \"\"\"\n",
    "    if pd.isna(license_text) or license_text == \"\" or license_text == \"其他\" or license_text is None:\n",
    "        return \"公开\"\n",
    "    \n",
    "    license_str = str(license_text).strip()\n",
    "\n",
    "    # 如果字符数大于30，替换为\"公开\"\n",
    "    if len(license_str) > 30:\n",
    "        return \"公开\"\n",
    "    return license_str\n",
    "\n",
    "# 应用开源协议列清洗\n",
    "if 'License' in models_df_complete.columns:\n",
    "    print(\"开始清洗License列...\")\n",
    "    models_df_complete['License'] = models_df_complete['License'].apply(clean_license_column)\n",
    "    print(\"License列清洗完成！\")\n",
    "    print(\"\\n清洗后的License列统计:\")\n",
    "    print(models_df_complete['License'].value_counts())\n",
    "else:\n",
    "    print(\"未找到'License'列\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "e83c2a2f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "开始清洗模型框架列...\n",
      "模型框架列清洗完成！\n",
      "\n",
      "清洗后的模型框架列统计:\n",
      "Frameworks\n",
      "PyTorch, Transformers    2\n",
      "Transformers             1\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "\n",
    "def clean_framework_column(framework_text):\n",
    "    \"\"\"\n",
    "    清洗模型框架列：\n",
    "    1. 创建包含所有主流框架的词典\n",
    "    2. 使用关键词匹配重新识别\n",
    "    3. 剔除无效字符\n",
    "    4. 使用逗号连接多个框架\n",
    "    5. 空值默认填写transformer\n",
    "    \"\"\"\n",
    "    # 主流机器学习/深度学习框架词典\n",
    "    framework_dict = {\n",
    "        # 深度学习框架\n",
    "        'tensorflow': 'TensorFlow',\n",
    "        'pytorch': 'PyTorch', \n",
    "        'torch': 'PyTorch',\n",
    "        'keras': 'Keras',\n",
    "        'jax': 'JAX',\n",
    "        'flax': 'Flax',\n",
    "        'mxnet': 'MXNet',\n",
    "        'caffe': 'Caffe',\n",
    "        'theano': 'Theano',\n",
    "        'chainer': 'Chainer',\n",
    "        'paddlepaddle': 'PaddlePaddle',\n",
    "        'paddle': 'PaddlePaddle',\n",
    "        'mindspore': 'MindSpore',\n",
    "        'oneflow': 'OneFlow',\n",
    "        \n",
    "        # 预训练模型库\n",
    "        'transformers': 'Transformers',\n",
    "        'transformer': 'Transformers',\n",
    "        'huggingface': 'Transformers',\n",
    "        'hf': 'Transformers',\n",
    "        'timm': 'TIMM',\n",
    "        'diffusers': 'Diffusers',\n",
    "        'sentence-transformers': 'Sentence-Transformers',\n",
    "        'spacy': 'spaCy',\n",
    "        'flair': 'Flair',\n",
    "        'allennlp': 'AllenNLP',\n",
    "        \n",
    "        # 传统机器学习\n",
    "        'sklearn': 'Scikit-learn',\n",
    "        'scikit-learn': 'Scikit-learn',\n",
    "        'xgboost': 'XGBoost',\n",
    "        'lightgbm': 'LightGBM',\n",
    "        'catboost': 'CatBoost',\n",
    "        'randomforest': 'Random Forest',\n",
    "        \n",
    "        # 强化学习\n",
    "        'stable-baselines': 'Stable-Baselines',\n",
    "        'stable_baselines': 'Stable-Baselines',\n",
    "        'ray': 'Ray',\n",
    "        'rllib': 'RLLib',\n",
    "        'openai gym': 'OpenAI Gym',\n",
    "        'gym': 'OpenAI Gym',\n",
    "        \n",
    "        # 计算机视觉\n",
    "        'opencv': 'OpenCV',\n",
    "        'cv2': 'OpenCV',\n",
    "        'pillow': 'PIL',\n",
    "        'pil': 'PIL',\n",
    "        'detectron': 'Detectron2',\n",
    "        'yolo': 'YOLO',\n",
    "        'mmdetection': 'MMDetection',\n",
    "        'torchvision': 'TorchVision',\n",
    "        \n",
    "        # 自然语言处理\n",
    "        'nltk': 'NLTK',\n",
    "        'gensim': 'Gensim',\n",
    "        'fasttext': 'FastText',\n",
    "        'word2vec': 'Word2Vec',\n",
    "        'bert': 'BERT',\n",
    "        'gpt': 'GPT',\n",
    "        'llama': 'LLaMA',\n",
    "        't5': 'T5',\n",
    "        \n",
    "        # 其他专用框架\n",
    "        'onnx': 'ONNX',\n",
    "        'tensorrt': 'TensorRT',\n",
    "        'openvino': 'OpenVINO',\n",
    "        'coreml': 'Core ML',\n",
    "        'mlflow': 'MLflow',\n",
    "        'wandb': 'Weights & Biases',\n",
    "        'tensorboard': 'TensorBoard',\n",
    "        \n",
    "        # 数据处理\n",
    "        'pandas': 'Pandas',\n",
    "        'numpy': 'NumPy',\n",
    "        'scipy': 'SciPy',\n",
    "        'dask': 'Dask',\n",
    "        'spark': 'Apache Spark',\n",
    "        'pyspark': 'PySpark',\n",
    "        \n",
    "        # 可视化\n",
    "        'matplotlib': 'Matplotlib',\n",
    "        'seaborn': 'Seaborn',\n",
    "        'plotly': 'Plotly',\n",
    "        'bokeh': 'Bokeh',\n",
    "        \n",
    "        # 音频处理\n",
    "        'librosa': 'Librosa',\n",
    "        'torchaudio': 'TorchAudio',\n",
    "        'speechbrain': 'SpeechBrain',\n",
    "        \n",
    "        # 图神经网络\n",
    "        'dgl': 'DGL',\n",
    "        'pyg': 'PyTorch Geometric',\n",
    "        'pytorch_geometric': 'PyTorch Geometric',\n",
    "        'spektral': 'Spektral',\n",
    "        \n",
    "        # Web框架 (用于模型部署)\n",
    "        'flask': 'Flask',\n",
    "        'fastapi': 'FastAPI',\n",
    "        'streamlit': 'Streamlit',\n",
    "        'gradio': 'Gradio',\n",
    "        \n",
    "        # 其他\n",
    "        'automl': 'AutoML',\n",
    "        'autokeras': 'AutoKeras',\n",
    "        'h2o': 'H2O',\n",
    "        'pycaret': 'PyCaret'\n",
    "    }\n",
    "    \n",
    "    if pd.isna(framework_text) or framework_text == \"\" or framework_text is None:\n",
    "        return \"Transformers\"\n",
    "    \n",
    "    framework_str = str(framework_text).lower().strip()\n",
    "    \n",
    "    # 移除无效字符，只保留字母、数字、连字符、下划线和空格\n",
    "    cleaned_framework = re.sub(r'[^a-zA-Z0-9\\-_\\s]', ' ', framework_str)\n",
    "    \n",
    "    # 识别框架\n",
    "    identified_frameworks = set()\n",
    "    \n",
    "    # 遍历框架词典进行匹配\n",
    "    for key, value in framework_dict.items():\n",
    "        # 使用单词边界进行精确匹配\n",
    "        pattern = r'\\b' + re.escape(key.lower()) + r'\\b'\n",
    "        if re.search(pattern, cleaned_framework):\n",
    "            identified_frameworks.add(value)\n",
    "    \n",
    "    # 如果没有识别到任何框架，返回默认值\n",
    "    if not identified_frameworks:\n",
    "        return \"Transformers\"\n",
    "    \n",
    "    # 使用逗号连接多个框架，按字母顺序排序\n",
    "    return \", \".join(sorted(identified_frameworks))\n",
    "\n",
    "# 应用模型框架列清洗\n",
    "if 'Frameworks' in models_df_complete.columns:\n",
    "    print(\"开始清洗模型框架列...\")\n",
    "    models_df_complete['Frameworks'] = models_df_complete['Frameworks'].apply(clean_framework_column)\n",
    "    print(\"模型框架列清洗完成！\")\n",
    "    print(\"\\n清洗后的模型框架列统计:\")\n",
    "    print(models_df_complete['Frameworks'].value_counts().head(20))\n",
    "else:\n",
    "    print(\"未找到'Frameworks'列\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "13035d2b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== 模态列填补处理 ===\n",
      "处理前模态列空值数量: 3\n",
      "\n",
      "开始填补模态列...\n",
      "\n",
      "填补统计:\n",
      "  自然语言处理 → 文本: 2 条\n",
      "  计算机视觉 → 图片/视频: 0 条\n",
      "  通用处理 → 文本/多模态: 1 条\n",
      "  其他 → 多模态: 0 条\n",
      "  总计填补: 3 条\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/ts/gb6dsxqn34q_l64hh0b980n40000gn/T/ipykernel_66575/3764677645.py:61: FutureWarning: Setting an item of incompatible dtype is deprecated and will raise an error in a future version of pandas. Value '文本' has dtype incompatible with float64, please explicitly cast to a compatible dtype first.\n",
      "  df_processed.at[idx, '模态'] = new_modality\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "import numpy as np\n",
    "models_df_complete['模态'] = models_df_complete.get('模态', pd.Series([np.nan]*len(models_df_complete)))\n",
    "def fill_modality_by_function_tag(df):\n",
    "    \"\"\"\n",
    "    根据功能标签对模态列进行填补：\n",
    "    - 自然语言处理：文本\n",
    "    - 通用处理：文本或多模态随机\n",
    "    - 计算机视觉：图片或视频随机\n",
    "    \"\"\"\n",
    "    df_processed = models_df_complete.copy()\n",
    "    \n",
    "    # 设置随机种子以确保结果可复现\n",
    "    random.seed(42)\n",
    "    np.random.seed(42)\n",
    "    \n",
    "    # 统计处理前的情况\n",
    "    print(\"=== 模态列填补处理 ===\")\n",
    "    print(f\"处理前模态列空值数量: {df_processed['模态'].isnull().sum()}\")\n",
    "    \n",
    "    # 定义填补规则\n",
    "    def get_modality_by_function_tag(function_tag, current_modality):\n",
    "        \"\"\"根据功能标签确定模态\"\"\"\n",
    "        # 如果已有非空模态值，保持不变\n",
    "        if pd.notna(current_modality) and current_modality != \"\":\n",
    "            return current_modality\n",
    "            \n",
    "        # 根据功能标签填补\n",
    "        if pd.isna(function_tag):\n",
    "            return \"多模态\"  # 默认值\n",
    "            \n",
    "        function_tag_str = str(function_tag).strip()\n",
    "        \n",
    "        if \"自然语言处理\" in function_tag_str:\n",
    "            return \"文本\"\n",
    "        elif \"计算机视觉\" in function_tag_str:\n",
    "            # 随机选择图片或视频\n",
    "            return random.choice([\"图片\", \"视频\"])\n",
    "        elif \"通用处理\" in function_tag_str:\n",
    "            # 随机选择文本或多模态\n",
    "            return random.choice([\"文本\", \"多模态\"])\n",
    "        else:\n",
    "            return \"多模态\"  # 其他情况默认为多模态\n",
    "    \n",
    "    # 应用填补规则\n",
    "    print(\"\\n开始填补模态列...\")\n",
    "    \n",
    "    # 分类统计处理情况\n",
    "    nlp_count = 0\n",
    "    cv_count = 0  \n",
    "    general_count = 0\n",
    "    other_count = 0\n",
    "    \n",
    "    for idx, row in df_processed.iterrows():\n",
    "        function_tag = row['功能标签']\n",
    "        current_modality = row['模态']\n",
    "        \n",
    "        # 只对空值进行填补\n",
    "        if pd.isna(current_modality) or current_modality == \"\":\n",
    "            new_modality = get_modality_by_function_tag(function_tag, current_modality)\n",
    "            df_processed.at[idx, '模态'] = new_modality\n",
    "            \n",
    "            # 统计各类别的处理数量\n",
    "            if pd.notna(function_tag):\n",
    "                function_tag_str = str(function_tag).strip()\n",
    "                if \"自然语言处理\" in function_tag_str:\n",
    "                    nlp_count += 1\n",
    "                elif \"计算机视觉\" in function_tag_str:\n",
    "                    cv_count += 1\n",
    "                elif \"通用处理\" in function_tag_str:\n",
    "                    general_count += 1\n",
    "                else:\n",
    "                    other_count += 1\n",
    "            else:\n",
    "                other_count += 1\n",
    "    \n",
    "    # 输出处理结果统计\n",
    "    print(f\"\\n填补统计:\")\n",
    "    print(f\"  自然语言处理 → 文本: {nlp_count} 条\")\n",
    "    print(f\"  计算机视觉 → 图片/视频: {cv_count} 条\")\n",
    "    print(f\"  通用处理 → 文本/多模态: {general_count} 条\")\n",
    "    print(f\"  其他 → 多模态: {other_count} 条\")\n",
    "    print(f\"  总计填补: {nlp_count + cv_count + general_count + other_count} 条\")\n",
    "    \n",
    "    return df_processed\n",
    "\n",
    "# 执行模态列填补\n",
    "models_df_complete = fill_modality_by_function_tag(models_df_complete)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1daf8193",
   "metadata": {},
   "source": [
    "规范化license列"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "94fe0e06",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "license_df=pd.read_excel('规范协议名称.xlsx')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "08bddc5a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "协议名称规范化函数已创建\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "from difflib import get_close_matches\n",
    "\n",
    "def normalize_license(license_name, standard_licenses):\n",
    "    \"\"\"\n",
    "    规范化协议名称\n",
    "    \"\"\"\n",
    "    if pd.isna(license_name) or license_name == '':\n",
    "        return license_name\n",
    "    \n",
    "    # 转换为字符串并去除首尾空格\n",
    "    license_str = str(license_name).strip()\n",
    "    \n",
    "    # 如果已经是标准格式，直接返回\n",
    "    if license_str in standard_licenses:\n",
    "        return license_str\n",
    "    \n",
    "    # 处理常见的大小写和格式问题\n",
    "    license_upper = license_str.upper()\n",
    "    \n",
    "    # 创建映射字典处理常见的变体\n",
    "    mapping = {\n",
    "        'APACHE-2.0': 'Apache-2.0',\n",
    "        'APACHE 2.0': 'Apache-2.0',\n",
    "        'APACHE2.0': 'Apache-2.0',\n",
    "        'APACHE': 'Apache-2.0',\n",
    "        'MIT': 'MIT',\n",
    "        'MIT LICENSE': 'MIT',\n",
    "        'CC-BY-4.0': 'CC-BY-4.0',\n",
    "        'CC BY 4.0': 'CC-BY-4.0',\n",
    "        'CC-BY-SA-4.0': 'CC-BY-SA-4.0',\n",
    "        'CC BY SA 4.0': 'CC-BY-SA-4.0',\n",
    "        'CC-BY-NC-SA-4.0': 'CC-BY-NC-SA-4.0',\n",
    "        'CC BY NC SA 4.0': 'CC-BY-NC-SA-4.0',\n",
    "        'CC0-1.0': 'CC0-1.0',\n",
    "        'CC0 1.0': 'CC0-1.0',\n",
    "        'BSD-3-CLAUSE': 'BSD-3-Clause',\n",
    "        'BSD 3 CLAUSE': 'BSD-3-Clause',\n",
    "        'BSD-2-CLAUSE': 'BSD-2-Clause',\n",
    "        'BSD 2 CLAUSE': 'BSD-2-Clause',\n",
    "        'GPL-3.0': 'GPL-3.0',\n",
    "        'GPL V3': 'GPL-3.0',\n",
    "        'LGPL-3.0': 'LGPL-3.0',\n",
    "        'AGPL-3.0': 'AGPL-3.0',\n",
    "        'UNLICENSE': 'Unlicense',\n",
    "        '公开': '公开',\n",
    "        'PUBLIC': '公开',\n",
    "        'OPENRAIL': 'OPENRAIL',\n",
    "        'CUSTOM': 'CUSTOM'\n",
    "    }\n",
    "    \n",
    "    # 先检查精确映射\n",
    "    if license_upper in mapping:\n",
    "        return mapping[license_upper]\n",
    "    \n",
    "    # 使用模糊匹配找到最相近的标准协议\n",
    "    # 首先尝试在标准协议列表中找到最相似的\n",
    "    close_matches = get_close_matches(license_str, standard_licenses, n=1, cutoff=0.8)\n",
    "    if close_matches:\n",
    "        return close_matches[0]\n",
    "    \n",
    "    # 如果没找到匹配，尝试大小写不敏感的匹配\n",
    "    for std_license in standard_licenses:\n",
    "        if license_upper == std_license.upper():\n",
    "            return std_license\n",
    "    \n",
    "    # 如果仍然没找到，返回原值\n",
    "    return license_str\n",
    "\n",
    "# 获取标准协议列表\n",
    "standard_licenses_list = license_df['开源协议'].tolist()\n",
    "\n",
    "print(\"协议名称规范化函数已创建\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "e96d9048",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "正在规范化 models_df 的协议名称...\n",
      "Models DataFrame 协议规范化对比:\n",
      "共有 2 条记录发生了变化:\n",
      "  原始协议 规范化协议\n",
      "1  mit   MIT\n",
      "\n",
      "规范化后的协议唯一值统计:\n",
      "开源协议\n",
      "MIT    2\n",
      "公开     1\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# 对models_df进行协议名称规范化\n",
    "print(\"正在规范化 models_df 的协议名称...\")\n",
    "\n",
    "# 创建备份\n",
    "models_df_backup = models_df_complete.copy()\n",
    "\n",
    "# 应用规范化函数\n",
    "models_df_complete['开源协议'] = models_df_complete['License'].apply(\n",
    "    lambda x: normalize_license(x, standard_licenses_list)\n",
    ")\n",
    "\n",
    "# 显示规范化前后的对比\n",
    "print(\"Models DataFrame 协议规范化对比:\")\n",
    "comparison_models = pd.DataFrame({\n",
    "    '原始协议': models_df_complete['License'],\n",
    "    '规范化协议': models_df_complete['开源协议']\n",
    "})\n",
    "comparison_models = pd.DataFrame({\n",
    "    '原始协议': models_df_complete['License'],\n",
    "    '规范化协议': models_df_complete['开源协议']\n",
    "})\n",
    "\n",
    "# 显示发生变化的记录\n",
    "changed_models = comparison_models[comparison_models['原始协议'] != comparison_models['规范化协议']]\n",
    "if len(changed_models) > 0:\n",
    "    print(f\"共有 {len(changed_models)} 条记录发生了变化:\")\n",
    "    print(changed_models.drop_duplicates().head(20))\n",
    "else:\n",
    "    print(\"没有记录发生变化\")\n",
    "\n",
    "# 显示规范化后的唯一值统计\n",
    "print(f\"\\n规范化后的协议唯一值统计:\")\n",
    "print(models_df_complete['开源协议'].value_counts().head(20))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "793b114b",
   "metadata": {},
   "source": [
    "去除高度重复的样本"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "a0ab14b1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "检查当前models_df状态...\n",
      "当前models_df形状: (3, 15)\n",
      "当前models_df列名: ['category', 'name', 'url', 'display_text', 'author', 'item_name', 'update_date', 'Frameworks', 'Domains', 'License', 'Description', '功能标签', '学科', '模态', '开源协议']\n",
      "去重前记录数: 3\n",
      "去重后记录数: 3\n",
      "删除的重复记录数: 0\n",
      "\n",
      "去重后的列数: 15\n",
      "\n",
      "去重完成！\n"
     ]
    }
   ],
   "source": [
    "# 检查当前models_df的状态\n",
    "print(\"检查当前models_df状态...\")\n",
    "print(f\"当前models_df形状: {models_df_complete.shape}\")\n",
    "print(f\"当前models_df列名: {models_df_complete.columns.tolist()}\")\n",
    "\n",
    "# 创建备份\n",
    "models_df_before_dedup = models_df_complete.copy()\n",
    "\n",
    "# 定义去重策略：对于相同的原始名称，我们保留：\n",
    "# 1. 优先保留模型发布/更新时间最新的记录  \n",
    "# 2. 优先保留信息更完整的记录（非空字段更多）\n",
    "\n",
    "def calculate_completeness_score(row):\n",
    "    \"\"\"计算记录的完整性分数\"\"\"\n",
    "    score = 0\n",
    "    # 重要字段的权重\n",
    "    important_fields = ['Description', '功能标签', '学科', '模态']\n",
    "    \n",
    "    for field in important_fields:\n",
    "        if pd.notna(row[field]) and str(row[field]).strip() != '':\n",
    "            score += 1\n",
    "    \n",
    "    return score\n",
    "\n",
    "# 为每条记录计算完整性分数\n",
    "models_df_complete['completeness_score'] = models_df_complete.apply(calculate_completeness_score, axis=1)\n",
    "\n",
    "# 排序并去重：对于每个原始名称，保留排序后的第一条记录\n",
    "# 只使用存在的列进行排序\n",
    "models_df_deduped = (models_df_complete.sort_values([\n",
    "    'name',\n",
    "    'completeness_score'\n",
    "], ascending=[True, False])\n",
    ".groupby('name', as_index=False)  # 设置as_index=False保留原始名称列\n",
    ".first())\n",
    "\n",
    "# 删除辅助列\n",
    "models_df_deduped = models_df_deduped.drop(['completeness_score'], axis=1)\n",
    "\n",
    "print(f\"去重前记录数: {len(models_df_before_dedup)}\")\n",
    "print(f\"去重后记录数: {len(models_df_deduped)}\")\n",
    "print(f\"删除的重复记录数: {len(models_df_before_dedup) - len(models_df_deduped)}\")\n",
    "\n",
    "# 验证列是否完整\n",
    "print(f\"\\n去重后的列数: {len(models_df_deduped.columns)}\")\n",
    "\n",
    "# 更新models_df\n",
    "models_df = models_df_deduped\n",
    "\n",
    "print(\"\\n去重完成！\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e551957e",
   "metadata": {},
   "source": [
    "# 调用AI翻译"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "f5007063",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = \"\"\"请你学习这个关于数据集的介绍，并用简洁的中文对进行总结，说明这个数据集的内容和用途。以JSON格式输出，严格遵循如下格式：```json{\"介绍\":\"你的总结的内容\"}``` \"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "c28d07d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "import json\n",
    "import time\n",
    "import logging\n",
    "from datetime import datetime, timedelta\n",
    "from concurrent.futures import ThreadPoolExecutor\n",
    "from tqdm import tqdm\n",
    "from openai import OpenAI\n",
    "\n",
    "# -------------------------------\n",
    "# JSON解析模块（独立模块）\n",
    "# -------------------------------\n",
    "def default_json_parser(content, idx=None):\n",
    "    \"\"\"\n",
    "    默认的 JSON 解析器：\n",
    "    清理输入内容后尝试解析 JSON，\n",
    "    若成功则返回完整的字典，若失败返回空字典。\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # 去除代码块标记，清理内容\n",
    "        cleaned_content = content.replace('```json\\n', '').replace('```', '').strip()\n",
    "        parsed_result = json.loads(cleaned_content)\n",
    "        return parsed_result\n",
    "    except json.JSONDecodeError:\n",
    "        if idx is not None:\n",
    "            logging.warning(f\"警告: 第 {idx} 行解析 JSON 失败\")\n",
    "        return {}\n",
    "    except Exception as e:\n",
    "        if idx is not None:\n",
    "            logging.error(f\"错误: 第 {idx} 行解析失败 - {str(e)}\")\n",
    "        return {}\n",
    "\n",
    "# -------------------------------\n",
    "# 限流处理器（控制请求频率）\n",
    "# -------------------------------\n",
    "class RateLimitedProcessor:\n",
    "    def __init__(self):\n",
    "        self.request_timestamps = []\n",
    "        self.MAX_RPM = 500\n",
    "        self.window_size = 60  # 60秒窗口\n",
    "\n",
    "    def _clean_old_records(self, current_time):\n",
    "        cutoff_time = current_time - timedelta(seconds=self.window_size)\n",
    "        self.request_timestamps = [ts for ts in self.request_timestamps if ts > cutoff_time]\n",
    "\n",
    "    def can_make_request(self):\n",
    "        \"\"\"检查是否可以发起新请求\"\"\"\n",
    "        current_time = datetime.now()\n",
    "        self._clean_old_records(current_time)\n",
    "        if len(self.request_timestamps) >= self.MAX_RPM:\n",
    "            return False\n",
    "        self.request_timestamps.append(current_time)\n",
    "        return True\n",
    "\n",
    "# -------------------------------\n",
    "# OpenAI文本处理器\n",
    "# -------------------------------\n",
    "class OpenAITextProcessor:\n",
    "    def __init__(self, api_key=None, model=None, base_url=None, json_parser=None):\n",
    "        self.client = OpenAI(api_key=api_key,base_url=base_url)\n",
    "        self.model = model\n",
    "        self.rate_limiter = RateLimitedProcessor()\n",
    "        self.n_workers = 14  # 优化后的线程数\n",
    "        # 如果未提供自定义解析器，则使用默认解析器\n",
    "        self.json_parser = json_parser if json_parser is not None else default_json_parser\n",
    "\n",
    "    def process_batch(self, df, text_column, prompt, batch_size=20, delay=1, json_parser=None):\n",
    "        \"\"\"\n",
    "        批量处理文本，支持灵活的 JSON 解析。\n",
    "        \n",
    "        参数:\n",
    "            df: 包含文本数据的 DataFrame\n",
    "            text_column: 文本所在的列名\n",
    "            prompt: 系统提示，用于 API 调用\n",
    "            batch_size: 每个批次处理的文本条数\n",
    "            delay: 每次请求后的延迟（秒）\n",
    "            json_parser: 可选的自定义 JSON 解析器，若不传入则使用实例内的解析器\n",
    "        \n",
    "        返回:\n",
    "            新的 DataFrame，包含原始数据及 API 返回结果（通过 JSON 解析获得的各字段）\n",
    "        \"\"\"\n",
    "        parser = json_parser if json_parser is not None else self.json_parser\n",
    "        results = []  # 保存每次请求解析后的结果（字典形式）\n",
    "\n",
    "        def process_chunk(chunk_data):\n",
    "            chunk_results = []\n",
    "            for idx, text in chunk_data:\n",
    "                # 限流检测：等待直到可以发送请求\n",
    "                while not self.rate_limiter.can_make_request():\n",
    "                    time.sleep(0.1)\n",
    "                try:\n",
    "                    response = self.client.chat.completions.create(\n",
    "                        model=self.model,\n",
    "                        messages=[\n",
    "                            {\"role\": \"system\", \"content\": prompt},\n",
    "                            {\"role\": \"user\", \"content\": text}\n",
    "                        ],\n",
    "                        temperature=1,\n",
    "                        max_tokens=500\n",
    "                    )\n",
    "                    # 使用解析器处理响应内容，得到字典格式结果\n",
    "                    parsed_result = parser(response.choices[0].message.content, idx)\n",
    "                    chunk_results.append(parsed_result)\n",
    "                    time.sleep(delay)\n",
    "                except Exception as e:\n",
    "                    logging.error(f\"错误: 处理第 {idx} 行时发生异常: {str(e)}\")\n",
    "                    chunk_results.append({})\n",
    "            return chunk_results\n",
    "\n",
    "        # 将数据分成批次，保留行号信息\n",
    "        chunks = [\n",
    "            list(enumerate(df[text_column][i:i+batch_size]))\n",
    "            for i in range(0, len(df), batch_size)\n",
    "        ]\n",
    "\n",
    "        with ThreadPoolExecutor(max_workers=self.n_workers) as executor:\n",
    "            futures = list(tqdm(\n",
    "                executor.map(process_chunk, chunks),\n",
    "                total=len(chunks),\n",
    "                desc=\"Processing batches\"\n",
    "            ))\n",
    "            for chunk_results in futures:\n",
    "                results.extend(chunk_results)\n",
    "\n",
    "        # 将解析结果列表转为 DataFrame，并与原 DataFrame 合并\n",
    "        df_result = df.copy().reset_index(drop=True)\n",
    "        results_df = pd.json_normalize(results)\n",
    "        df_result = pd.concat([df_result, results_df], axis=1)\n",
    "\n",
    "        # 统计处理情况\n",
    "        success_count = sum(1 for r in results if r)\n",
    "        total_count = len(results)\n",
    "        success_rate = (success_count / total_count) * 100 if total_count > 0 else 0\n",
    "        logging.info(f\"处理完成: 总数 {total_count}, 成功 {success_count}, 成功率 {success_rate:.2f}%\")\n",
    "        \n",
    "        return df_result\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "409d1e44",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing batches:   0%|          | 0/1 [00:00<?, ?it/s]2025-08-26 11:40:50,156 - INFO - HTTP Request: POST https://api.deepseek.com/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-08-26 11:40:56,229 - INFO - HTTP Request: POST https://api.deepseek.com/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-08-26 11:41:05,718 - INFO - HTTP Request: POST https://api.deepseek.com/chat/completions \"HTTP/1.1 200 OK\"\n",
      "Processing batches: 100%|██████████| 1/1 [00:25<00:00, 25.40s/it]\n",
      "2025-08-26 11:41:15,370 - INFO - 处理完成: 总数 3, 成功 3, 成功率 100.00%\n"
     ]
    }
   ],
   "source": [
    "processor = OpenAITextProcessor(api_key=\"此处填入你的deepseek api key\", base_url=\"https://api.deepseek.com\",model=\"deepseek-chat\")\n",
    "df_result = processor.process_batch(\n",
    "    df=models_df_complete,\n",
    "    text_column=\"Description\",\n",
    "    prompt=prompt,\n",
    "    batch_size=5,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "987c56a8",
   "metadata": {},
   "source": [
    "# 规范化底稿格式"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd2d5f65",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.microsoft.datawrangler.viewer.v0+json": {
       "columns": [
        {
         "name": "index",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "原始名称",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "翻译名称",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "是否上传门户",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "科学领域",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "模态",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "模型框架",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "是否开源",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "开源协议",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "国内/外模型",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "模型介绍",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "模型发布/更新时间",
         "rawType": "object",
         "type": "unknown"
        },
        {
         "name": "功能标签",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "模型链接",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "备注-author",
         "rawType": "object",
         "type": "string"
        }
       ],
       "ref": "58ae1496-3ebd-43a5-8e6e-cf352b653855",
       "rows": [
        [
         "0",
         "full-text",
         "N/A",
         "否",
         "其他学科",
         "文本",
         "Transformers",
         "是",
         "公开",
         "国外",
         "这是一个关于数据集介绍的学习任务，要求用户学习并总结数据集的内容和用途，然后以指定的JSON格式输出简洁中文总结。",
         null,
         "通用处理",
         "https://huggingface.co/search/full-text",
         "search"
        ],
        [
         "1",
         "aspect-acl-scibert-scivocab-uncased",
         "N/A",
         "否",
         "其他学科",
         "文本",
         "PyTorch, Transformers",
         "是",
         "MIT",
         "国外",
         "该数据集基于scibert-scivocab-uncased模型，在ACL Anthology语料库上进行了微调，专注于研究论文的基于方面的文档相似性计算。它可用于通过DOI、ArXiv ID、ACL ID或PubMed ID检索Semantic Scholar中的论文，并直接评估相似度，支持学术研究和文献分析。",
         "2023-02-07",
         "自然语言处理",
         "https://huggingface.co/malteos/aspect-acl-scibert-scivocab-uncased",
         "malteos"
        ],
        [
         "2",
         "aspect-cord19-scibert-scivocab-uncased",
         "N/A",
         "否",
         "其他学科",
         "文本",
         "PyTorch, Transformers",
         "是",
         "MIT",
         "国外",
         "该数据集是一个基于SciBERT模型（使用科学词汇表且不区分大小写）的微调版本，专门针对CORD-19语料库进行训练，用于研究论文的基于方面的文档相似性分析。其用途是计算和比较学术论文之间的相似度，用户可以通过Google Colab直接使用训练好的模型，输入Semantic Scholar上的论文标识符（如DOI、ArXiv ID、ACL ID或PubMed ID）进行测试。",
         "2021-11-22",
         "自然语言处理",
         "https://huggingface.co/malteos/aspect-cord19-scibert-scivocab-uncased",
         "malteos"
        ]
       ],
       "shape": {
        "columns": 14,
        "rows": 3
       }
      },
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>原始名称</th>\n",
       "      <th>翻译名称</th>\n",
       "      <th>是否上传门户</th>\n",
       "      <th>科学领域</th>\n",
       "      <th>模态</th>\n",
       "      <th>模型框架</th>\n",
       "      <th>是否开源</th>\n",
       "      <th>开源协议</th>\n",
       "      <th>国内/外模型</th>\n",
       "      <th>模型介绍</th>\n",
       "      <th>模型发布/更新时间</th>\n",
       "      <th>功能标签</th>\n",
       "      <th>模型链接</th>\n",
       "      <th>备注-author</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>full-text</td>\n",
       "      <td>N/A</td>\n",
       "      <td>否</td>\n",
       "      <td>其他学科</td>\n",
       "      <td>文本</td>\n",
       "      <td>Transformers</td>\n",
       "      <td>是</td>\n",
       "      <td>公开</td>\n",
       "      <td>国外</td>\n",
       "      <td>这是一个关于数据集介绍的学习任务，要求用户学习并总结数据集的内容和用途，然后以指定的JSON...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>通用处理</td>\n",
       "      <td>https://huggingface.co/search/full-text</td>\n",
       "      <td>search</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>aspect-acl-scibert-scivocab-uncased</td>\n",
       "      <td>N/A</td>\n",
       "      <td>否</td>\n",
       "      <td>其他学科</td>\n",
       "      <td>文本</td>\n",
       "      <td>PyTorch, Transformers</td>\n",
       "      <td>是</td>\n",
       "      <td>MIT</td>\n",
       "      <td>国外</td>\n",
       "      <td>该数据集基于scibert-scivocab-uncased模型，在ACL Antholog...</td>\n",
       "      <td>2023-02-07</td>\n",
       "      <td>自然语言处理</td>\n",
       "      <td>https://huggingface.co/malteos/aspect-acl-scib...</td>\n",
       "      <td>malteos</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>aspect-cord19-scibert-scivocab-uncased</td>\n",
       "      <td>N/A</td>\n",
       "      <td>否</td>\n",
       "      <td>其他学科</td>\n",
       "      <td>文本</td>\n",
       "      <td>PyTorch, Transformers</td>\n",
       "      <td>是</td>\n",
       "      <td>MIT</td>\n",
       "      <td>国外</td>\n",
       "      <td>该数据集是一个基于SciBERT模型（使用科学词汇表且不区分大小写）的微调版本，专门针对CO...</td>\n",
       "      <td>2021-11-22</td>\n",
       "      <td>自然语言处理</td>\n",
       "      <td>https://huggingface.co/malteos/aspect-cord19-s...</td>\n",
       "      <td>malteos</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                     原始名称 翻译名称 是否上传门户  科学领域  模态  \\\n",
       "0                               full-text  N/A      否  其他学科  文本   \n",
       "1     aspect-acl-scibert-scivocab-uncased  N/A      否  其他学科  文本   \n",
       "2  aspect-cord19-scibert-scivocab-uncased  N/A      否  其他学科  文本   \n",
       "\n",
       "                    模型框架 是否开源 开源协议 国内/外模型  \\\n",
       "0           Transformers    是   公开     国外   \n",
       "1  PyTorch, Transformers    是  MIT     国外   \n",
       "2  PyTorch, Transformers    是  MIT     国外   \n",
       "\n",
       "                                                模型介绍   模型发布/更新时间    功能标签  \\\n",
       "0  这是一个关于数据集介绍的学习任务，要求用户学习并总结数据集的内容和用途，然后以指定的JSON...         NaN    通用处理   \n",
       "1  该数据集基于scibert-scivocab-uncased模型，在ACL Antholog...  2023-02-07  自然语言处理   \n",
       "2  该数据集是一个基于SciBERT模型（使用科学词汇表且不区分大小写）的微调版本，专门针对CO...  2021-11-22  自然语言处理   \n",
       "\n",
       "                                                模型链接 备注-author  \n",
       "0            https://huggingface.co/search/full-text    search  \n",
       "1  https://huggingface.co/malteos/aspect-acl-scib...   malteos  \n",
       "2  https://huggingface.co/malteos/aspect-cord19-s...   malteos  "
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 1. 删除不需要的列\n",
    "cols_to_drop = ['category', 'name', 'display_textDomains', 'License', 'Description', 'completeness_score']\n",
    "df_result = df_result.drop(columns=[col for col in cols_to_drop if col in df_result.columns])\n",
    "\n",
    "# 2. 替换列名称\n",
    "df_result = df_result.rename(columns={\n",
    "    'url': '模型链接',\n",
    "    'author': '备注-author',\n",
    "    'item_name': '原始名称',\n",
    "    'update_date': '模型发布/更新时间',\n",
    "    'Frameworks': '模型框架',\n",
    "    '学科': '科学领域',\n",
    "    '介绍': '模型介绍'\n",
    "})\n",
    "\n",
    "# 3. 重新排列表格顺序\n",
    "desired_order = [\n",
    "    '原始名称', '科学领域', '模态', '模型框架', '开源协议', \n",
    "    '模型介绍', '模型发布/更新时间', '功能标签', '模型链接', '备注-author'\n",
    " ]\n",
    "df_result = df_result[[col for col in desired_order if col in df_result.columns]]\n",
    "df_result.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "8f00291f",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_result.to_excel(\"科学智能模型.xlsx\", index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
