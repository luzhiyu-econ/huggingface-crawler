{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d8a47324",
   "metadata": {},
   "source": [
    "# 说明\n",
    "### 本部分代码包括从Huggingface根据特定关键词获取相关数据集信息，并最终整理成规范格式的完整流程"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47f1afc2",
   "metadata": {},
   "source": [
    "## 根据关键词构建数据集列表"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9bccce6",
   "metadata": {},
   "source": [
    "定义爬虫函数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "a9bccce6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "from selenium.webdriver.chrome.options import Options\n",
    "from selenium.common.exceptions import TimeoutException, NoSuchElementException\n",
    "import time\n",
    "import csv\n",
    "import json\n",
    "import logging\n",
    "import re\n",
    "import os\n",
    "from urllib.parse import urljoin\n",
    "\n",
    "# 设置日志\n",
    "logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "class HuggingFaceSeleniumCrawler:\n",
    "    def __init__(self, headless=True, delay=2):\n",
    "        \"\"\"\n",
    "        初始化爬虫\n",
    "        :param headless: 是否使用无头模式\n",
    "        :param delay: 页面加载等待时间\n",
    "        \"\"\"\n",
    "        self.delay = delay\n",
    "        self.datasets = []\n",
    "        self.setup_driver(headless)\n",
    "        \n",
    "    def setup_driver(self, headless=True):\n",
    "        \"\"\"\n",
    "        设置Chrome浏览器驱动\n",
    "        \"\"\"\n",
    "        chrome_options = Options()\n",
    "        if headless:\n",
    "            chrome_options.add_argument(\"--headless\")\n",
    "        chrome_options.add_argument(\"--no-sandbox\")\n",
    "        chrome_options.add_argument(\"--disable-dev-shm-usage\")\n",
    "        chrome_options.add_argument(\"--disable-gpu\")\n",
    "        chrome_options.add_argument(\"--window-size=1920,1080\")\n",
    "        chrome_options.add_argument(\"--user-agent=Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36\")\n",
    "        \n",
    "        try:\n",
    "            self.driver = webdriver.Chrome(options=chrome_options)\n",
    "            self.wait = WebDriverWait(self.driver, 10)\n",
    "            logger.info(\"Chrome浏览器驱动初始化成功\")\n",
    "        except Exception as e:\n",
    "            logger.error(f\"浏览器驱动初始化失败: {e}\")\n",
    "            raise\n",
    "    \n",
    "    def load_page(self, url):\n",
    "        \"\"\"\n",
    "        加载页面\n",
    "        :param url: 页面URL\n",
    "        :return: 是否成功加载\n",
    "        \"\"\"\n",
    "        try:\n",
    "            logger.info(f\"正在加载页面: {url}\")\n",
    "            self.driver.get(url)\n",
    "            time.sleep(self.delay)\n",
    "            \n",
    "            # 等待页面主要内容加载\n",
    "            self.wait.until(EC.presence_of_element_located((By.TAG_NAME, \"main\")))\n",
    "            return True\n",
    "        except TimeoutException:\n",
    "            logger.error(f\"页面加载超时: {url}\")\n",
    "            return False\n",
    "        except Exception as e:\n",
    "            logger.error(f\"页面加载失败: {url}, 错误: {e}\")\n",
    "            return False\n",
    "    \n",
    "    def extract_datasets_from_current_page(self):\n",
    "        \"\"\"\n",
    "        从当前页面提取数据集信息\n",
    "        :return: 数据集列表\n",
    "        \"\"\"\n",
    "        datasets = []\n",
    "        \n",
    "        try:\n",
    "            # 等待数据集列表加载\n",
    "            time.sleep(2)\n",
    "            \n",
    "            # 多种选择器策略来查找数据集元素\n",
    "            selectors = [\n",
    "                \"a[href*='/datasets/']\",  # 包含datasets路径的链接\n",
    "                \".overview-card a\",       # 概览卡片中的链接\n",
    "                \"[data-testid*='dataset']\", # 测试ID包含dataset的元素\n",
    "                \".grid a[href^='/datasets/']\", # 网格中的数据集链接\n",
    "            ]\n",
    "            \n",
    "            dataset_elements = []\n",
    "            for selector in selectors:\n",
    "                try:\n",
    "                    elements = self.driver.find_elements(By.CSS_SELECTOR, selector)\n",
    "                    if elements:\n",
    "                        dataset_elements.extend(elements)\n",
    "                        logger.info(f\"使用选择器 '{selector}' 找到 {len(elements)} 个元素\")\n",
    "                except Exception as e:\n",
    "                    logger.debug(f\"选择器 '{selector}' 未找到元素: {e}\")\n",
    "                    continue\n",
    "            \n",
    "            # 如果上述方法都不行，尝试查找所有链接\n",
    "            if not dataset_elements:\n",
    "                all_links = self.driver.find_elements(By.TAG_NAME, \"a\")\n",
    "                dataset_elements = [link for link in all_links \n",
    "                                  if link.get_attribute(\"href\") and \"/datasets/\" in link.get_attribute(\"href\")]\n",
    "                logger.info(f\"通过所有链接查找，找到 {len(dataset_elements)} 个数据集链接\")\n",
    "            \n",
    "            # 处理找到的元素\n",
    "            processed_names = set()\n",
    "            \n",
    "            for element in dataset_elements:\n",
    "                try:\n",
    "                    href = element.get_attribute(\"href\")\n",
    "                    if not href or \"/datasets/\" not in href:\n",
    "                        continue\n",
    "                    \n",
    "                    # 提取数据集名称\n",
    "                    dataset_name = self.extract_dataset_name_from_url(href)\n",
    "                    \n",
    "                    if dataset_name and dataset_name not in processed_names:\n",
    "                        # 尝试获取显示的文本作为额外信息\n",
    "                        display_text = element.text.strip()\n",
    "                        \n",
    "                        dataset_info = {\n",
    "                            'name': dataset_name,\n",
    "                            'url': f\"https://huggingface.co/datasets/{dataset_name}\",\n",
    "                            'display_text': display_text[:100] if display_text else \"\",\n",
    "                            'href': href\n",
    "                        }\n",
    "                        \n",
    "                        datasets.append(dataset_info)\n",
    "                        processed_names.add(dataset_name)\n",
    "                        logger.info(f\"找到数据集: {dataset_name}\")\n",
    "                        \n",
    "                except Exception as e:\n",
    "                    logger.debug(f\"处理元素时出错: {e}\")\n",
    "                    continue\n",
    "            \n",
    "            # 如果仍然没有找到，尝试使用正则表达式从页面源代码中提取\n",
    "            if not datasets:\n",
    "                datasets = self.extract_from_page_source()\n",
    "            \n",
    "            return datasets\n",
    "            \n",
    "        except Exception as e:\n",
    "            logger.error(f\"提取数据集时出错: {e}\")\n",
    "            return []\n",
    "    \n",
    "    def extract_dataset_name_from_url(self, url):\n",
    "        \"\"\"\n",
    "        从URL中提取数据集名称\n",
    "        :param url: 数据集URL\n",
    "        :return: 数据集名称 (用户名/数据集名)\n",
    "        \"\"\"\n",
    "        try:\n",
    "            if \"/datasets/\" in url:\n",
    "                # 提取 /datasets/ 后面的部分\n",
    "                parts = url.split(\"/datasets/\", 1)\n",
    "                if len(parts) > 1:\n",
    "                    dataset_path = parts[1].strip(\"/\")\n",
    "                    # 移除查询参数和锚点\n",
    "                    dataset_path = dataset_path.split(\"?\")[0].split(\"#\")[0]\n",
    "                    \n",
    "                    # 确保格式为 用户名/数据集名\n",
    "                    path_parts = dataset_path.split(\"/\")\n",
    "                    if len(path_parts) >= 2:\n",
    "                        return f\"{path_parts[0]}/{path_parts[1]}\"\n",
    "            return None\n",
    "        except Exception as e:\n",
    "            logger.debug(f\"从URL提取数据集名称时出错: {e}\")\n",
    "            return None\n",
    "    \n",
    "    def extract_from_page_source(self):\n",
    "        \"\"\"\n",
    "        从页面源代码中使用正则表达式提取数据集信息\n",
    "        :return: 数据集列表\n",
    "        \"\"\"\n",
    "        datasets = []\n",
    "        try:\n",
    "            page_source = self.driver.page_source\n",
    "            \n",
    "            # 使用正则表达式查找数据集路径\n",
    "            pattern = r'/datasets/([a-zA-Z0-9_.-]+/[a-zA-Z0-9_.-]+)'\n",
    "            matches = re.findall(pattern, page_source)\n",
    "            \n",
    "            processed_names = set()\n",
    "            for match in matches:\n",
    "                if match not in processed_names:\n",
    "                    dataset_info = {\n",
    "                        'name': match,\n",
    "                        'url': f\"https://huggingface.co/datasets/{match}\",\n",
    "                        'display_text': \"\",\n",
    "                        'href': f\"/datasets/{match}\"\n",
    "                    }\n",
    "                    datasets.append(dataset_info)\n",
    "                    processed_names.add(match)\n",
    "                    logger.info(f\"从源代码中找到数据集: {match}\")\n",
    "            \n",
    "            return datasets\n",
    "            \n",
    "        except Exception as e:\n",
    "            logger.error(f\"从页面源代码提取数据集时出错: {e}\")\n",
    "            return []\n",
    "    \n",
    "    def find_next_page_button(self):\n",
    "        \"\"\"\n",
    "        查找下一页按钮\n",
    "        :return: 下一页按钮元素或None\n",
    "        \"\"\"\n",
    "        # 多种策略查找下一页按钮\n",
    "        next_button_selectors = [\n",
    "            \"a[aria-label='Next page']\",\n",
    "            \"a:contains('Next')\",\n",
    "            \"button[aria-label='Next page']\",\n",
    "            \"a[href*='p=']\",  # 包含页码参数的链接\n",
    "            \".pagination a:last-child\",\n",
    "            \"[data-testid='next-page']\"\n",
    "        ]\n",
    "        \n",
    "        for selector in next_button_selectors:\n",
    "            try:\n",
    "                if \":contains(\" in selector:\n",
    "                    # 对于包含文本的选择器，使用XPath\n",
    "                    xpath = \"//a[contains(text(), 'Next')] | //button[contains(text(), 'Next')]\"\n",
    "                    elements = self.driver.find_elements(By.XPATH, xpath)\n",
    "                else:\n",
    "                    elements = self.driver.find_elements(By.CSS_SELECTOR, selector)\n",
    "                \n",
    "                for element in elements:\n",
    "                    if element.is_displayed() and element.is_enabled():\n",
    "                        return element\n",
    "                        \n",
    "            except Exception as e:\n",
    "                logger.debug(f\"查找下一页按钮时出错 ({selector}): {e}\")\n",
    "                continue\n",
    "        \n",
    "        # 尝试通过文本查找\n",
    "        try:\n",
    "            xpath_selectors = [\n",
    "                \"//a[contains(text(), 'Next')]\",\n",
    "                \"//button[contains(text(), 'Next')]\",\n",
    "                \"//a[contains(text(), '>')]\",\n",
    "                \"//a[text()='>']\"\n",
    "            ]\n",
    "            \n",
    "            for xpath in xpath_selectors:\n",
    "                elements = self.driver.find_elements(By.XPATH, xpath)\n",
    "                for element in elements:\n",
    "                    if element.is_displayed() and element.is_enabled():\n",
    "                        return element\n",
    "                        \n",
    "        except Exception as e:\n",
    "            logger.debug(f\"通过XPath查找下一页按钮时出错: {e}\")\n",
    "        \n",
    "        return None\n",
    "    \n",
    "    def go_to_next_page(self):\n",
    "        \"\"\"\n",
    "        跳转到下一页\n",
    "        :return: 是否成功跳转\n",
    "        \"\"\"\n",
    "        try:\n",
    "            next_button = self.find_next_page_button()\n",
    "            \n",
    "            if next_button:\n",
    "                # 滚动到按钮位置\n",
    "                self.driver.execute_script(\"arguments[0].scrollIntoView(true);\", next_button)\n",
    "                time.sleep(1)\n",
    "                \n",
    "                # 点击按钮\n",
    "                next_button.click()\n",
    "                \n",
    "                # 等待页面加载\n",
    "                time.sleep(self.delay)\n",
    "                \n",
    "                # 验证页面是否发生了变化\n",
    "                self.wait.until(EC.presence_of_element_located((By.TAG_NAME, \"main\")))\n",
    "                \n",
    "                logger.info(\"成功跳转到下一页\")\n",
    "                return True\n",
    "            else:\n",
    "                logger.info(\"未找到下一页按钮，可能已到最后一页\")\n",
    "                return False\n",
    "                \n",
    "        except Exception as e:\n",
    "            logger.error(f\"跳转下一页时出错: {e}\")\n",
    "            return False\n",
    "    \n",
    "    def crawl_all_pages(self, start_url, max_pages=None):\n",
    "        \"\"\"\n",
    "        爬取所有页面\n",
    "        :param start_url: 起始URL\n",
    "        :param max_pages: 最大页面数（None表示无限制）\n",
    "        :return: 所有数据集列表\n",
    "        \"\"\"\n",
    "        all_datasets = []\n",
    "        page_count = 1\n",
    "        \n",
    "        # 加载第一页\n",
    "        if not self.load_page(start_url):\n",
    "            logger.error(\"无法加载起始页面\")\n",
    "            return []\n",
    "        \n",
    "        while True:\n",
    "            logger.info(f\"正在处理第 {page_count} 页\")\n",
    "            \n",
    "            # 提取当前页面的数据集\n",
    "            page_datasets = self.extract_datasets_from_current_page()\n",
    "            \n",
    "            if page_datasets:\n",
    "                all_datasets.extend(page_datasets)\n",
    "                logger.info(f\"第 {page_count} 页找到 {len(page_datasets)} 个数据集\")\n",
    "            else:\n",
    "                logger.warning(f\"第 {page_count} 页没有找到数据集\")\n",
    "            \n",
    "            # 检查是否达到最大页面数\n",
    "            if max_pages and page_count >= max_pages:\n",
    "                logger.info(f\"已达到最大页面数 {max_pages}\")\n",
    "                break\n",
    "            \n",
    "            # 尝试跳转到下一页\n",
    "            if not self.go_to_next_page():\n",
    "                logger.info(\"没有更多页面\")\n",
    "                break\n",
    "            \n",
    "            page_count += 1\n",
    "        \n",
    "        logger.info(f\"爬取完成，总共处理 {page_count} 页，找到 {len(all_datasets)} 个数据集\")\n",
    "        return all_datasets\n",
    "    \n",
    "    def extract_category_from_url(self, url):\n",
    "        \"\"\"\n",
    "        从URL中提取分类参数\n",
    "        :param url: URL字符串\n",
    "        :return: 分类名称\n",
    "        \"\"\"\n",
    "        try:\n",
    "            from urllib.parse import urlparse, parse_qs\n",
    "            \n",
    "            parsed_url = urlparse(url)\n",
    "            query_params = parse_qs(parsed_url.query)\n",
    "            \n",
    "            # 检查other参数\n",
    "            if 'other' in query_params:\n",
    "                return query_params['other'][0]\n",
    "            \n",
    "            # 检查其他可能的分类参数\n",
    "            category_params = ['category', 'type', 'tag']\n",
    "            for param in category_params:\n",
    "                if param in query_params:\n",
    "                    return query_params[param][0]\n",
    "                    \n",
    "            return \"\"\n",
    "            \n",
    "        except Exception as e:\n",
    "            logger.debug(f\"从URL提取分类时出错: {e}\")\n",
    "            return \"\"\n",
    "    \n",
    "    def save_results(self, datasets, filename_prefix=\"huggingface_datasets_selenium\", category=\"\"):\n",
    "        \"\"\"\n",
    "        保存结果到文件\n",
    "        :param datasets: 数据集列表\n",
    "        :param filename_prefix: 文件名前缀\n",
    "        :param category: 分类名称（从URL参数中提取）\n",
    "        \"\"\"\n",
    "        timestamp = time.strftime(\"%Y%m%d_%H%M%S\")\n",
    "        \n",
    "        # 如果有分类信息，添加到文件名中\n",
    "        if category:\n",
    "            csv_filename = f\"{filename_prefix}_{category}_{timestamp}.csv\"\n",
    "        else:\n",
    "            csv_filename = f\"{filename_prefix}_{timestamp}.csv\"\n",
    "            \n",
    "        with open(csv_filename, 'w', newline='', encoding='utf-8') as csvfile:\n",
    "            fieldnames = ['name', 'url', 'display_text']\n",
    "            writer = csv.DictWriter(csvfile, fieldnames=fieldnames)\n",
    "            writer.writeheader()\n",
    "            for dataset in datasets:\n",
    "                writer.writerow({\n",
    "                    'name': dataset['name'],\n",
    "                    'url': dataset['url'],\n",
    "                    'display_text': dataset.get('display_text', '')\n",
    "                })\n",
    "        \n",
    "        logger.info(f\"结果已保存到:\")\n",
    "        logger.info(f\"  CSV: {csv_filename}\")\n",
    "    \n",
    "    def save_results_to_path(self, datasets, file_path, category=\"\"):\n",
    "        \"\"\"\n",
    "        保存结果到指定路径\n",
    "        :param datasets: 数据集列表\n",
    "        :param file_path: 完整的文件路径（不包含扩展名）\n",
    "        :param category: 分类名称（从URL参数中提取）\n",
    "        \"\"\"\n",
    "        timestamp = time.strftime(\"%Y%m%d_%H%M%S\")\n",
    "        \n",
    "        # 确保目录存在\n",
    "        directory = os.path.dirname(file_path)\n",
    "        if directory and not os.path.exists(directory):\n",
    "            os.makedirs(directory, exist_ok=True)\n",
    "        \n",
    "        # 构建完整的文件名\n",
    "        if category:\n",
    "            csv_filename = f\"{file_path}.csv\"\n",
    "        else:\n",
    "            csv_filename = f\"{file_path}.csv\"\n",
    "            \n",
    "        with open(csv_filename, 'w', newline='', encoding='utf-8') as csvfile:\n",
    "            fieldnames = ['name', 'url', 'display_text']\n",
    "            writer = csv.DictWriter(csvfile, fieldnames=fieldnames)\n",
    "            writer.writeheader()\n",
    "            for dataset in datasets:\n",
    "                writer.writerow({\n",
    "                    'name': dataset['name'],\n",
    "                    'url': dataset['url'],\n",
    "                    'display_text': dataset.get('display_text', '')\n",
    "                })\n",
    "        \n",
    "        logger.info(f\"结果已保存到: {csv_filename}\")\n",
    "        return csv_filename\n",
    "    \n",
    "    def close(self):\n",
    "        \"\"\"\n",
    "        关闭浏览器\n",
    "        \"\"\"\n",
    "        if hasattr(self, 'driver'):\n",
    "            self.driver.quit()\n",
    "            logger.info(\"浏览器已关闭\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "8f21cc97",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from typing import List, Dict, Optional\n",
    "import os\n",
    "\n",
    "def crawl_multiple_keywords(keywords: List[str], \n",
    "                          save_path: Optional[str] = None,\n",
    "                          max_pages_per_keyword: Optional[int] = None,\n",
    "                          base_url: str = \"https://huggingface.co/datasets\",\n",
    "                          sort_by: str = \"trending\",\n",
    "                          headless: bool = True,\n",
    "                          delay: int = 3) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    处理多个关键词的Hugging Face数据集爬取函数\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    keywords : List[str]\n",
    "        要爬取的关键词列表，例如 ['climate', 'medical', 'chemistry']\n",
    "    save_path : Optional[str]\n",
    "        保存结果的文件路径，支持 .csv, .xlsx, .json 格式\n",
    "        如果为 None，则不保存文件，只返回 DataFrame\n",
    "    max_pages_per_keyword : Optional[int]\n",
    "        每个关键词最大爬取页面数，None 表示无限制\n",
    "    base_url : str\n",
    "        基础URL，默认为 Hugging Face 数据集页面\n",
    "    sort_by : str\n",
    "        排序方式，默认为 \"trending\"\n",
    "    headless : bool\n",
    "        是否使用无头模式运行浏览器\n",
    "    delay : int\n",
    "        页面加载等待时间（秒）\n",
    "    \n",
    "    Returns:\n",
    "    --------\n",
    "    pd.DataFrame\n",
    "        包含所有关键词数据集的综合DataFrame\n",
    "    \"\"\"\n",
    "    \n",
    "    all_results = []\n",
    "    failed_keywords = []\n",
    "    \n",
    "    print(f\"开始爬取 {len(keywords)} 个关键词的数据集...\")\n",
    "    \n",
    "    for i, keyword in enumerate(keywords, 1):\n",
    "        print(f\"\\n[{i}/{len(keywords)}] 正在处理关键词: '{keyword}'\")\n",
    "        \n",
    "        # 构建URL\n",
    "        url = f\"{base_url}?other={keyword}&sort={sort_by}\"\n",
    "        \n",
    "        # 创建爬虫实例\n",
    "        crawler = HuggingFaceSeleniumCrawler(headless=headless, delay=delay)\n",
    "        \n",
    "        try:\n",
    "            # 爬取当前关键词的数据集\n",
    "            datasets = crawler.crawl_all_pages(url, max_pages=max_pages_per_keyword)\n",
    "            \n",
    "            if datasets:\n",
    "                # 为每个数据集添加关键词标记\n",
    "                for dataset in datasets:\n",
    "                    dataset['keyword'] = keyword\n",
    "                    dataset['category'] = keyword  # 添加分类字段\n",
    "                \n",
    "                all_results.extend(datasets)\n",
    "                print(f\"✓ 关键词 '{keyword}' 完成，获得 {len(datasets)} 个数据集\")\n",
    "            else:\n",
    "                print(f\"✗ 关键词 '{keyword}' 未获得任何数据集\")\n",
    "                failed_keywords.append(keyword)\n",
    "                \n",
    "        except Exception as e:\n",
    "            print(f\"✗ 关键词 '{keyword}' 处理失败: {e}\")\n",
    "            failed_keywords.append(keyword)\n",
    "            logger.error(f\"处理关键词 '{keyword}' 时出错: {e}\")\n",
    "            \n",
    "        finally:\n",
    "            # 确保关闭浏览器\n",
    "            crawler.close()\n",
    "    \n",
    "    # 创建DataFrame\n",
    "    if all_results:\n",
    "        df = pd.DataFrame(all_results)\n",
    "        \n",
    "        # 数据清理和去重\n",
    "        print(f\"\\n数据处理中...\")\n",
    "        print(f\"原始数据: {len(df)} 条记录\")\n",
    "        \n",
    "        # 去重（基于name和keyword的组合）\n",
    "        df = df.drop_duplicates(subset=['name', 'keyword'], keep='first')\n",
    "        print(f\"去重后: {len(df)} 条记录\")\n",
    "        \n",
    "        # 重新排列列的顺序\n",
    "        column_order = ['keyword', 'category', 'name', 'url', 'display_text']\n",
    "        df = df.reindex(columns=column_order)\n",
    "        \n",
    "        # 按关键词和名称排序\n",
    "        df = df.sort_values(['keyword', 'name']).reset_index(drop=True)\n",
    "        \n",
    "        # 统计信息\n",
    "        print(f\"\\n=== 爬取结果统计 ===\")\n",
    "        keyword_stats = df['keyword'].value_counts()\n",
    "        for keyword, count in keyword_stats.items():\n",
    "            print(f\"  {keyword}: {count} 个数据集\")\n",
    "        \n",
    "        if failed_keywords:\n",
    "            print(f\"\\n失败的关键词: {failed_keywords}\")\n",
    "        \n",
    "        # 保存文件\n",
    "        if save_path:\n",
    "            save_dataframe_to_file(df, save_path)\n",
    "        \n",
    "        return df\n",
    "    \n",
    "    else:\n",
    "        print(\"\\n没有获取到任何数据集！\")\n",
    "        return pd.DataFrame()\n",
    "\n",
    "\n",
    "def save_dataframe_to_file(df: pd.DataFrame, file_path: str) -> None:\n",
    "    \"\"\"\n",
    "    将DataFrame保存到指定格式的文件\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    df : pd.DataFrame\n",
    "        要保存的DataFrame\n",
    "    file_path : str\n",
    "        保存路径，根据后缀名确定格式\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # 确保目录存在\n",
    "        os.makedirs(os.path.dirname(file_path) if os.path.dirname(file_path) else '.', exist_ok=True)\n",
    "        \n",
    "        file_extension = os.path.splitext(file_path)[1].lower()\n",
    "        \n",
    "        if file_extension == '.csv':\n",
    "            df.to_csv(file_path, index=False, encoding='utf-8')\n",
    "            print(f\"✓ 结果已保存为CSV: {file_path}\")\n",
    "            \n",
    "        elif file_extension in ['.xlsx', '.xls']:\n",
    "            df.to_excel(file_path, index=False, engine='openpyxl')\n",
    "            print(f\"✓ 结果已保存为Excel: {file_path}\")\n",
    "            \n",
    "        elif file_extension == '.json':\n",
    "            df.to_json(file_path, orient='records', indent=2, force_ascii=False)\n",
    "            print(f\"✓ 结果已保存为JSON: {file_path}\")\n",
    "            \n",
    "        else:\n",
    "            # 默认保存为CSV\n",
    "            csv_path = file_path + '.csv'\n",
    "            df.to_csv(csv_path, index=False, encoding='utf-8')\n",
    "            print(f\"✓ 未识别的文件格式，已保存为CSV: {csv_path}\")\n",
    "            \n",
    "    except Exception as e:\n",
    "        print(f\"✗ 保存文件时出错: {e}\")\n",
    "        logger.error(f\"保存文件时出错: {e}\")\n",
    "\n",
    "\n",
    "def get_keyword_urls_batch(keywords: List[str], \n",
    "                          base_url: str = \"https://huggingface.co/datasets\",\n",
    "                          sort_by: str = \"trending\") -> Dict[str, str]:\n",
    "    \"\"\"\n",
    "    批量生成关键词对应的URL\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    keywords : List[str]\n",
    "        关键词列表\n",
    "    base_url : str\n",
    "        基础URL\n",
    "    sort_by : str\n",
    "        排序方式\n",
    "    \n",
    "    Returns:\n",
    "    --------\n",
    "    Dict[str, str]\n",
    "        关键词到URL的映射\n",
    "    \"\"\"\n",
    "    return {keyword: f\"{base_url}?other={keyword}&sort={sort_by}\" for keyword in keywords}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd2fe008",
   "metadata": {},
   "source": [
    "执行爬虫函数获取数据。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98028c74",
   "metadata": {},
   "source": [
    "根据关键词选择特定学科"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e935e3a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 实际使用示例\n",
    "# 定义要爬取的关键词列表\n",
    "target_keywords = ['climate', 'forecast']\n",
    "\n",
    "# 执行多关键词爬取\n",
    "# 可以自定义保存路径和文件格式\n",
    "save_location = \"备份_中间流程结果/datasets_list.csv\"  # 可以是 .csv, .xlsx, .json\n",
    "\n",
    "print(\"开始执行多关键词数据集爬取...\")\n",
    "\n",
    "# 执行爬取\n",
    "df = crawl_multiple_keywords(\n",
    "    keywords=target_keywords,\n",
    "    save_path=save_location,\n",
    "    max_pages_per_keyword=None,  # None表示爬取所有页面，也可以设置数字限制\n",
    "    headless=True,  # True表示不显示浏览器窗口\n",
    "    delay=3  # 页面加载等待时间\n",
    ")\n",
    "\n",
    "# 显示结果\n",
    "if not df.empty:\n",
    "    print(f\"\\n爬取完成！共获得 {len(df)} 个数据集\")\n",
    "    print(f\"文件已保存至: {save_location}\")\n",
    "    \n",
    "    # 显示前几条数据作为预览\n",
    "    print(\"\\n数据预览:\")\n",
    "    print(df.head())\n",
    "    \n",
    "    # 显示各关键词的数据集数量统计\n",
    "    print(\"\\n各关键词数据集数量:\")\n",
    "    print(df['keyword'].value_counts())\n",
    "else:\n",
    "    print(\"未获取到任何数据！\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "337d5f58",
   "metadata": {},
   "source": [
    "爬取指定url页面下的所有数据集"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "fef6a6fc",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-08-26 11:36:15,478 - INFO - Chrome浏览器驱动初始化成功\n",
      "2025-08-26 11:36:15,484 - INFO - 检测到分类: \n",
      "2025-08-26 11:36:15,487 - INFO - 正在加载页面: https://huggingface.co/datasets?size_categories=or:%28size_categories:n%3E1T%29&sort=trending\n",
      "2025-08-26 11:36:20,907 - INFO - 正在处理第 1 页\n",
      "2025-08-26 11:36:22,919 - INFO - 使用选择器 'a[href*='/datasets/']' 找到 30 个元素\n",
      "2025-08-26 11:36:22,946 - INFO - 使用选择器 '.grid a[href^='/datasets/']' 找到 30 个元素\n",
      "2025-08-26 11:36:22,965 - INFO - 找到数据集: allenai/dolma\n",
      "2025-08-26 11:36:22,974 - INFO - 找到数据集: nasa-ibm-ai4science/SDO_training\n",
      "2025-08-26 11:36:22,983 - INFO - 找到数据集: 3DTopia/4DNeX-10M\n",
      "2025-08-26 11:36:22,991 - INFO - 找到数据集: allenai/MADLAD-400\n",
      "2025-08-26 11:36:23,000 - INFO - 找到数据集: urbanaudiosensing/ASPED\n",
      "2025-08-26 11:36:23,008 - INFO - 找到数据集: x-humanoid-robomind/RoboMIND\n",
      "2025-08-26 11:36:23,017 - INFO - 找到数据集: InternRobotics/InternData-N1\n",
      "2025-08-26 11:36:23,025 - INFO - 找到数据集: DeliberatorArchiver/asmr-archive-data-01\n",
      "2025-08-26 11:36:23,034 - INFO - 找到数据集: DeliberatorArchiver/asmr-archive-data-02\n",
      "2025-08-26 11:36:23,043 - INFO - 找到数据集: oscar-corpus/OSCAR-2109\n",
      "2025-08-26 11:36:23,052 - INFO - 找到数据集: poloclub/diffusiondb\n",
      "2025-08-26 11:36:23,062 - INFO - 找到数据集: CarperAI/pilev2-dev\n",
      "2025-08-26 11:36:23,071 - INFO - 找到数据集: oscar-corpus/OSCAR-2301\n",
      "2025-08-26 11:36:23,080 - INFO - 找到数据集: oscar-corpus/oscar-2301-hpc\n",
      "2025-08-26 11:36:23,089 - INFO - 找到数据集: ColtonAi/Oi\n",
      "2025-08-26 11:36:23,097 - INFO - 找到数据集: NOABOL35631y/T\n",
      "2025-08-26 11:36:23,106 - INFO - 找到数据集: oscar-corpus/colossal-oscar-1.0\n",
      "2025-08-26 11:36:23,115 - INFO - 找到数据集: min06/Hscb\n",
      "2025-08-26 11:36:23,125 - INFO - 找到数据集: CropNet/CropNet\n",
      "2025-08-26 11:36:23,133 - INFO - 找到数据集: DL3DV/DL3DV-Benchmark\n",
      "2025-08-26 11:36:23,142 - INFO - 找到数据集: Pytagora/Datalab\n",
      "2025-08-26 11:36:23,152 - INFO - 找到数据集: DL3DV/DL3DV-ALL-4K\n",
      "2025-08-26 11:36:23,161 - INFO - 找到数据集: shoyimobloqulov/text-to-speech-tts\n",
      "2025-08-26 11:36:23,170 - INFO - 找到数据集: DL3DV/DL3DV-ALL-960P\n",
      "2025-08-26 11:36:23,179 - INFO - 找到数据集: NikkoIGuess/NikkoDoesRandom_Ai\n",
      "2025-08-26 11:36:23,188 - INFO - 找到数据集: DL3DV/DL3DV-ALL-2K\n",
      "2025-08-26 11:36:23,198 - INFO - 找到数据集: DL3DV/DL3DV-ALL-video\n",
      "2025-08-26 11:36:23,206 - INFO - 找到数据集: HPLT/hplt_monolingual_v1_2\n",
      "2025-08-26 11:36:23,216 - INFO - 找到数据集: DL3DV/DL3DV-ALL-ColmapCache\n",
      "2025-08-26 11:36:23,224 - INFO - 找到数据集: Pragadishwaran/manual\n",
      "2025-08-26 11:36:23,293 - INFO - 第 1 页找到 30 个数据集\n",
      "2025-08-26 11:36:27,437 - INFO - 成功跳转到下一页\n",
      "2025-08-26 11:36:27,438 - INFO - 正在处理第 2 页\n",
      "2025-08-26 11:36:29,450 - INFO - 使用选择器 'a[href*='/datasets/']' 找到 30 个元素\n",
      "2025-08-26 11:36:29,492 - INFO - 使用选择器 '.grid a[href^='/datasets/']' 找到 30 个元素\n",
      "2025-08-26 11:36:29,540 - INFO - 找到数据集: psoldunov/testset\n",
      "2025-08-26 11:36:29,560 - INFO - 找到数据集: meg/dolma-v1_6-sample\n",
      "2025-08-26 11:36:29,570 - INFO - 找到数据集: akhilhsingh/homeo-dataset\n",
      "2025-08-26 11:36:29,580 - INFO - 找到数据集: oscar-corpus/community-oscar\n",
      "2025-08-26 11:36:29,588 - INFO - 找到数据集: Mohamedfadil369/BrainData\n",
      "2025-08-26 11:36:29,597 - INFO - 找到数据集: nkandpa2/cccc_all_domains\n",
      "2025-08-26 11:36:29,606 - INFO - 找到数据集: artanekrem/SDFSDF\n",
      "2025-08-26 11:36:29,614 - INFO - 找到数据集: liuqingquan/test2\n",
      "2025-08-26 11:36:29,623 - INFO - 找到数据集: Artificial-superintelligence/Athtest\n",
      "2025-08-26 11:36:29,632 - INFO - 找到数据集: DataoceanAI/Lip_reading_Speech_Video_Corpus\n",
      "2025-08-26 11:36:29,641 - INFO - 找到数据集: shuxunoo/NFT-Net\n",
      "2025-08-26 11:36:29,650 - INFO - 找到数据集: BAAI/IndustryCorpus_programming\n",
      "2025-08-26 11:36:29,659 - INFO - 找到数据集: BAAI/IndustryCorpus_emotion\n",
      "2025-08-26 11:36:29,669 - INFO - 找到数据集: BAAI/IndustryCorpus_mathematics\n",
      "2025-08-26 11:36:29,677 - INFO - 找到数据集: BAAI/IndustryCorpus_ai\n",
      "2025-08-26 11:36:29,686 - INFO - 找到数据集: Yuqi1997/DrivingDojo\n",
      "2025-08-26 11:36:29,695 - INFO - 找到数据集: Zyphra/Zyda-2\n",
      "2025-08-26 11:36:29,705 - INFO - 找到数据集: Maple728/Time-300B\n",
      "2025-08-26 11:36:29,713 - INFO - 找到数据集: DL3DV/DL3DV-Drone\n",
      "2025-08-26 11:36:29,722 - INFO - 找到数据集: Lightarmortech/Bot.com\n",
      "2025-08-26 11:36:29,731 - INFO - 找到数据集: LLM360/TxT360\n",
      "2025-08-26 11:36:29,740 - INFO - 找到数据集: avalab/Allo-AVA\n",
      "2025-08-26 11:36:29,748 - INFO - 找到数据集: anchovy/maple728-time_300B\n",
      "2025-08-26 11:36:29,758 - INFO - 找到数据集: Khole1234/Chloe\n",
      "2025-08-26 11:36:29,765 - INFO - 找到数据集: Moonlightsonata/vso\n",
      "2025-08-26 11:36:29,774 - INFO - 找到数据集: haggs/test\n",
      "2025-08-26 11:36:29,783 - INFO - 找到数据集: Prabhuram/Medicine-List\n",
      "2025-08-26 11:36:29,792 - INFO - 找到数据集: dijihax/Dataset\n",
      "2025-08-26 11:36:29,801 - INFO - 找到数据集: prasenjeet5/DeLTa-Zz-Core\n",
      "2025-08-26 11:36:29,810 - INFO - 找到数据集: coan/bot_claiton\n",
      "2025-08-26 11:36:29,879 - INFO - 第 2 页找到 30 个数据集\n",
      "2025-08-26 11:36:33,957 - INFO - 成功跳转到下一页\n",
      "2025-08-26 11:36:33,958 - INFO - 正在处理第 3 页\n",
      "2025-08-26 11:36:35,974 - INFO - 使用选择器 'a[href*='/datasets/']' 找到 30 个元素\n",
      "2025-08-26 11:36:35,987 - INFO - 使用选择器 '.grid a[href^='/datasets/']' 找到 30 个元素\n",
      "2025-08-26 11:36:36,000 - INFO - 找到数据集: neis-lab/mmcows\n",
      "2025-08-26 11:36:36,058 - INFO - 找到数据集: m-a-p/FineFineWeb-fasttext-seeddata\n",
      "2025-08-26 11:36:36,077 - INFO - 找到数据集: 1989shack/Database\n",
      "2025-08-26 11:36:36,085 - INFO - 找到数据集: sarahgillet/BrainstormingDataset\n",
      "2025-08-26 11:36:36,095 - INFO - 找到数据集: BIOMEDICA/biomedica_webdataset_24M\n",
      "2025-08-26 11:36:36,103 - INFO - 找到数据集: Majidu/Ohr\n",
      "2025-08-26 11:36:36,111 - INFO - 找到数据集: GraspClutter6D/GraspClutter6D\n",
      "2025-08-26 11:36:36,120 - INFO - 找到数据集: Johnnyboystar/Images\n",
      "2025-08-26 11:36:36,129 - INFO - 找到数据集: agibot-world/AgiBotDigitalWorld\n",
      "2025-08-26 11:36:36,137 - INFO - 找到数据集: ZeppelinCorp/Eclipse_Corpuz\n",
      "2025-08-26 11:36:36,146 - INFO - 找到数据集: IntelLabs/BlueLens\n",
      "2025-08-26 11:36:36,155 - INFO - 找到数据集: jobs-git/Zyda-2\n",
      "2025-08-26 11:36:36,163 - INFO - 找到数据集: jobs-git/HPLT2.0_cleaned\n",
      "2025-08-26 11:36:36,171 - INFO - 找到数据集: embed2scale/SSL4EO-S12-v1.1\n",
      "2025-08-26 11:36:36,180 - INFO - 找到数据集: torchgeo/CropClimateX\n",
      "2025-08-26 11:36:36,188 - INFO - 找到数据集: everrer/121212\n",
      "2025-08-26 11:36:36,196 - INFO - 找到数据集: maxkaufmann/allenai_dolma_test_set\n",
      "2025-08-26 11:36:36,204 - INFO - 找到数据集: BIOMEDICA/biomedica_microscopy_subset_webdataset\n",
      "2025-08-26 11:36:36,212 - INFO - 找到数据集: BIOMEDICA/biomedica_dermatology_subset_webdataset\n",
      "2025-08-26 11:36:36,220 - INFO - 找到数据集: BIOMEDICA/biomedica_surgery_subset_webdataset\n",
      "2025-08-26 11:36:36,228 - INFO - 找到数据集: trillarmybewm/trillium\n",
      "2025-08-26 11:36:36,236 - INFO - 找到数据集: DL3DV/DL3DV-GS-960P\n",
      "2025-08-26 11:36:36,245 - INFO - 找到数据集: criteo/CriteoClickLogs\n",
      "2025-08-26 11:36:36,252 - INFO - 找到数据集: jobs-git/diffusiondb\n",
      "2025-08-26 11:36:36,261 - INFO - 找到数据集: introspector/solfunmeme\n",
      "2025-08-26 11:36:36,269 - INFO - 找到数据集: JMaeen25/Mycollection\n",
      "2025-08-26 11:36:36,277 - INFO - 找到数据集: tugrul93/TaCarla\n",
      "2025-08-26 11:36:36,286 - INFO - 找到数据集: cambrain/DigiFakeAV\n",
      "2025-08-26 11:36:36,294 - INFO - 找到数据集: nasa-ibm-ai4science/ar_emergence\n",
      "2025-08-26 11:36:36,304 - INFO - 找到数据集: Groovy-123/deep-think\n",
      "2025-08-26 11:36:36,379 - INFO - 第 3 页找到 30 个数据集\n",
      "2025-08-26 11:36:40,486 - INFO - 成功跳转到下一页\n",
      "2025-08-26 11:36:40,487 - INFO - 正在处理第 4 页\n",
      "2025-08-26 11:36:42,503 - INFO - 使用选择器 'a[href*='/datasets/']' 找到 11 个元素\n",
      "2025-08-26 11:36:42,528 - INFO - 使用选择器 '.grid a[href^='/datasets/']' 找到 11 个元素\n",
      "2025-08-26 11:36:42,538 - INFO - 找到数据集: hiepp2/tvp4\n",
      "2025-08-26 11:36:42,546 - INFO - 找到数据集: Groovy-123/Advance\n",
      "2025-08-26 11:36:42,555 - INFO - 找到数据集: nvidia/PhysicalAI-Autonomous-Vehicle-Cosmos-Drive-Dreams\n",
      "2025-08-26 11:36:42,563 - INFO - 找到数据集: Prompthumanizer/jain_architecture\n",
      "2025-08-26 11:36:42,571 - INFO - 找到数据集: Mfonkown/Zark-Dataset_1.0\n",
      "2025-08-26 11:36:42,579 - INFO - 找到数据集: 1il7tw6n/makevideo\n",
      "2025-08-26 11:36:42,587 - INFO - 找到数据集: JQL-AI/fw2_embeddings\n",
      "2025-08-26 11:36:42,596 - INFO - 找到数据集: JQL-AI/hplt2_embeddings\n",
      "2025-08-26 11:36:42,603 - INFO - 找到数据集: PLM-Team/Pretrain-Dataset\n",
      "2025-08-26 11:36:42,611 - INFO - 找到数据集: Groovy-123/QuantumAI\n",
      "2025-08-26 11:36:42,619 - INFO - 找到数据集: theairlabcmu/tartanair\n",
      "2025-08-26 11:36:42,643 - INFO - 第 4 页找到 11 个数据集\n",
      "2025-08-26 11:36:44,757 - ERROR - 跳转下一页时出错: Message: element click intercepted: Element <a class=\"flex items-center rounded-lg px-2.5 py-1 hover:bg-gray-50 dark:hover:bg-gray-800 pointer-events-none cursor-default text-gray-400 hover:text-gray-700\" href=\"\">...</a> is not clickable at point (1315, 676). Other element would receive the click: <li>...</li>\n",
      "  (Session info: chrome=139.0.7258.139)\n",
      "Stacktrace:\n",
      "0   chromedriver                        0x0000000100c44c20 cxxbridge1$str$ptr + 2747652\n",
      "1   chromedriver                        0x0000000100c3cb20 cxxbridge1$str$ptr + 2714628\n",
      "2   chromedriver                        0x00000001007850c8 cxxbridge1$string$len + 90536\n",
      "3   chromedriver                        0x00000001007d21a8 cxxbridge1$string$len + 406152\n",
      "4   chromedriver                        0x00000001007d0714 cxxbridge1$string$len + 399348\n",
      "5   chromedriver                        0x00000001007ce528 cxxbridge1$string$len + 390664\n",
      "6   chromedriver                        0x00000001007cd924 cxxbridge1$string$len + 387588\n",
      "7   chromedriver                        0x00000001007c2458 cxxbridge1$string$len + 341304\n",
      "8   chromedriver                        0x00000001007c1ee4 cxxbridge1$string$len + 339908\n",
      "9   chromedriver                        0x000000010080d99c cxxbridge1$string$len + 649852\n",
      "10  chromedriver                        0x00000001007c0934 cxxbridge1$string$len + 334356\n",
      "11  chromedriver                        0x0000000100c0788c cxxbridge1$str$ptr + 2496880\n",
      "12  chromedriver                        0x0000000100c0aab8 cxxbridge1$str$ptr + 2509724\n",
      "13  chromedriver                        0x0000000100be8510 cxxbridge1$str$ptr + 2369012\n",
      "14  chromedriver                        0x0000000100c0b360 cxxbridge1$str$ptr + 2511940\n",
      "15  chromedriver                        0x0000000100bd9610 cxxbridge1$str$ptr + 2307828\n",
      "16  chromedriver                        0x0000000100c2b230 cxxbridge1$str$ptr + 2642708\n",
      "17  chromedriver                        0x0000000100c2b3bc cxxbridge1$str$ptr + 2643104\n",
      "18  chromedriver                        0x0000000100c3c76c cxxbridge1$str$ptr + 2713680\n",
      "19  libsystem_pthread.dylib             0x0000000182413c0c _pthread_start + 136\n",
      "20  libsystem_pthread.dylib             0x000000018240eb80 thread_start + 8\n",
      "\n",
      "2025-08-26 11:36:44,761 - INFO - 没有更多页面\n",
      "2025-08-26 11:36:44,762 - INFO - 爬取完成，总共处理 4 页，找到 101 个数据集\n",
      "2025-08-26 11:36:44,769 - INFO - 去重后共有 101 个唯一数据集\n",
      "2025-08-26 11:36:44,773 - INFO - 结果已保存到: 备份_中间流程结果/datasets_list.csv\n",
      "2025-08-26 11:36:44,880 - INFO - 浏览器已关闭\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "前10个数据集:\n",
      "1. allenai/dolma -> https://huggingface.co/datasets/allenai/dolma\n",
      "2. nasa-ibm-ai4science/SDO_training -> https://huggingface.co/datasets/nasa-ibm-ai4science/SDO_training\n",
      "3. 3DTopia/4DNeX-10M -> https://huggingface.co/datasets/3DTopia/4DNeX-10M\n",
      "4. allenai/MADLAD-400 -> https://huggingface.co/datasets/allenai/MADLAD-400\n",
      "5. urbanaudiosensing/ASPED -> https://huggingface.co/datasets/urbanaudiosensing/ASPED\n",
      "6. x-humanoid-robomind/RoboMIND -> https://huggingface.co/datasets/x-humanoid-robomind/RoboMIND\n",
      "7. InternRobotics/InternData-N1 -> https://huggingface.co/datasets/InternRobotics/InternData-N1\n",
      "8. DeliberatorArchiver/asmr-archive-data-01 -> https://huggingface.co/datasets/DeliberatorArchiver/asmr-archive-data-01\n",
      "9. DeliberatorArchiver/asmr-archive-data-02 -> https://huggingface.co/datasets/DeliberatorArchiver/asmr-archive-data-02\n",
      "10. oscar-corpus/OSCAR-2109 -> https://huggingface.co/datasets/oscar-corpus/OSCAR-2109\n",
      "... 还有 91 个数据集\n"
     ]
    }
   ],
   "source": [
    "def main(output_file_path=\"备份_中间流程结果/datasets_list\"):\n",
    "    \"\"\"\n",
    "    主函数\n",
    "    :param output_file_path: 指定保存结果的文件路径（不包含扩展名）\n",
    "    \"\"\"\n",
    "    # 目标URL\n",
    "    start_url = \"https://huggingface.co/datasets?size_categories=or:%28size_categories:n%3E1T%29&sort=trending\"\n",
    "    \n",
    "    # 创建爬虫实例\n",
    "    crawler = HuggingFaceSeleniumCrawler(headless=True, delay=3)\n",
    "    \n",
    "    try:\n",
    "        # 从URL中提取分类信息\n",
    "        category = crawler.extract_category_from_url(start_url)\n",
    "        logger.info(f\"检测到分类: {category}\")\n",
    "        \n",
    "        # 开始爬取（可以设置max_pages限制页面数，用于测试）\n",
    "        all_datasets = crawler.crawl_all_pages(start_url, max_pages=None)\n",
    "        \n",
    "        # 去重\n",
    "        unique_datasets = []\n",
    "        seen_names = set()\n",
    "        for dataset in all_datasets:\n",
    "            if dataset['name'] not in seen_names:\n",
    "                unique_datasets.append(dataset)\n",
    "                seen_names.add(dataset['name'])\n",
    "        \n",
    "        logger.info(f\"去重后共有 {len(unique_datasets)} 个唯一数据集\")\n",
    "        \n",
    "        # 打印前10个结果作为示例\n",
    "        print(\"\\n前10个数据集:\")\n",
    "        for i, dataset in enumerate(unique_datasets[:10], 1):\n",
    "            print(f\"{i}. {dataset['name']} -> {dataset['url']}\")\n",
    "        \n",
    "        if len(unique_datasets) > 10:\n",
    "            print(f\"... 还有 {len(unique_datasets) - 10} 个数据集\")\n",
    "        \n",
    "        # 保存结果到指定路径\n",
    "        crawler.save_results_to_path(unique_datasets, output_file_path, category=category)\n",
    "        \n",
    "    except KeyboardInterrupt:\n",
    "        logger.info(\"爬取被用户中断\")\n",
    "    except Exception as e:\n",
    "        logger.error(f\"爬取过程中出现错误: {e}\")\n",
    "    finally:\n",
    "        # 确保关闭浏览器\n",
    "        crawler.close()\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # 你可以这样调用：\n",
    "    # main()  # 使用默认路径\n",
    "    # main(\"my_custom_datasets\")  # 使用自定义路径\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "fef6a6fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "df=pd.read_csv('备份_中间流程结果/datasets_list.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "3cfb6064",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "处理数据集的name列...\n",
      "name列示例 (前5个):\n",
      "  1. allenai/dolma\n",
      "  2. nasa-ibm-ai4science/SDO_training\n",
      "  3. 3DTopia/4DNeX-10M\n",
      "  4. allenai/MADLAD-400\n",
      "  5. urbanaudiosensing/ASPED\n",
      "分割完成:\n",
      "  - 有作者信息的条目: 101\n",
      "  - 没有作者信息的条目: 0\n"
     ]
    }
   ],
   "source": [
    "# 2. 对两个表格的name列进行处理：根据/分割左侧为作者，右侧为数据集/模型名称\n",
    "\n",
    "def split_name_column(df, df_type):\n",
    "    \"\"\"\n",
    "    分割name列为author和item_name列\n",
    "    \"\"\"\n",
    "    print(f\"\\n处理{df_type}的name列...\")\n",
    "    \n",
    "    # 创建副本避免修改原数据\n",
    "    df_copy = df.copy()\n",
    "    \n",
    "    # 查看一些name列的示例\n",
    "    print(f\"name列示例 (前5个):\")\n",
    "    for i, name in enumerate(df_copy['name'].head()):\n",
    "        print(f\"  {i+1}. {name}\")\n",
    "    \n",
    "    # 分割name列\n",
    "    split_names = df_copy['name'].str.split('/', n=1, expand=True)\n",
    "    \n",
    "    # 添加author和item_name列\n",
    "    df_copy['author'] = split_names[0] if len(split_names.columns) > 0 else ''\n",
    "    df_copy['item_name'] = split_names[1] if len(split_names.columns) > 1 else ''\n",
    "    \n",
    "    # 处理没有/的情况（整个name作为item_name，author为空）\n",
    "    mask_no_slash = df_copy['item_name'].isna()\n",
    "    df_copy.loc[mask_no_slash, 'item_name'] = df_copy.loc[mask_no_slash, 'author']\n",
    "    df_copy.loc[mask_no_slash, 'author'] = ''\n",
    "    \n",
    "    # 清理空值\n",
    "    df_copy['author'] = df_copy['author'].fillna('')\n",
    "    df_copy['item_name'] = df_copy['item_name'].fillna('')\n",
    "    \n",
    "    print(f\"分割完成:\")\n",
    "    print(f\"  - 有作者信息的条目: {(df_copy['author'] != '').sum()}\")\n",
    "    print(f\"  - 没有作者信息的条目: {(df_copy['author'] == '').sum()}\")\n",
    "    \n",
    "    return df_copy\n",
    "\n",
    "# 处理datasets_df\n",
    "datasets_df_processed = split_name_column(df, \"数据集\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "caba7c0c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "==================================================\n",
      "开始处理display_text列，提取更新日期...\n",
      "\n",
      "提取的更新日期示例 (前10个):\n",
      "  1. 2024-04-17\n",
      "  2. 2025-05-21\n",
      "  3. 2025-08-16\n",
      "  4. 2024-09-10\n",
      "  5. 2024-01-24\n",
      "  6. 2025-07-09\n",
      "  7. 2025-08-15\n",
      "  8. 2025-02-01\n",
      "  9. 2025-03-31\n",
      "  10. 2025-08-02\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "from datetime import datetime, timedelta\n",
    "\n",
    "# 3. 处理display_text列，提取更新日期\n",
    "def extract_update_date(display_text):\n",
    "    \"\"\"\n",
    "    从display_text中提取更新日期\n",
    "    \"\"\"\n",
    "    if pd.isna(display_text) or display_text == '':\n",
    "        return None\n",
    "    \n",
    "    # 按•分割\n",
    "    parts = display_text.split('•')\n",
    "    \n",
    "    # 查找包含Updated的部分\n",
    "    update_part = None\n",
    "    for part in parts:\n",
    "        if 'Updated' in part:\n",
    "            update_part = part.strip()\n",
    "            break\n",
    "    \n",
    "    if not update_part:\n",
    "        return None\n",
    "    \n",
    "    # 提取Updated后的日期部分\n",
    "    # 使用正则表达式匹配Updated后的内容\n",
    "    match = re.search(r'Updated\\s+(.+)', update_part)\n",
    "    if not match:\n",
    "        return None\n",
    "    \n",
    "    date_str = match.group(1).strip()\n",
    "    \n",
    "    try:\n",
    "        # 1. 处理完整年月日格式，如 \"Jun 5, 2024\"\n",
    "        if re.match(r'^[A-Za-z]{3}\\s+\\d{1,2},\\s+\\d{4}$', date_str):\n",
    "            return datetime.strptime(date_str, '%b %d, %Y').strftime('%Y-%m-%d')\n",
    "        \n",
    "        # 2. 处理没有年份的格式，如 \"Mar 8\"，默认年份为2025\n",
    "        elif re.match(r'^[A-Za-z]{3}\\s+\\d{1,2}$', date_str):\n",
    "            date_with_year = f\"{date_str}, 2025\"\n",
    "            return datetime.strptime(date_with_year, '%b %d, %Y').strftime('%Y-%m-%d')\n",
    "        \n",
    "        # 3. 处理相对时间格式，如 \"9 days ago\"\n",
    "        elif 'ago' in date_str.lower():\n",
    "            base_date = datetime(2025, 8, 22)  # 基准日期\n",
    "            \n",
    "            # 匹配数字和时间单位\n",
    "            time_match = re.search(r'(\\d+)\\s+(day|month|year|week|hour|minute)s?\\s+ago', date_str.lower())\n",
    "            if time_match:\n",
    "                number = int(time_match.group(1))\n",
    "                unit = time_match.group(2)\n",
    "                \n",
    "                if unit == 'day':\n",
    "                    result_date = base_date - timedelta(days=number)\n",
    "                elif unit == 'week':\n",
    "                    result_date = base_date - timedelta(weeks=number)\n",
    "                elif unit == 'month':\n",
    "                    # 简单处理，假设一个月30天\n",
    "                    result_date = base_date - timedelta(days=number * 30)\n",
    "                elif unit == 'year':\n",
    "                    # 简单处理，假设一年365天\n",
    "                    result_date = base_date - timedelta(days=number * 365)\n",
    "                elif unit == 'hour':\n",
    "                    result_date = base_date - timedelta(hours=number)\n",
    "                elif unit == 'minute':\n",
    "                    result_date = base_date - timedelta(minutes=number)\n",
    "                else:\n",
    "                    return None\n",
    "                \n",
    "                return result_date.strftime('%Y-%m-%d')\n",
    "        \n",
    "        # 其他格式尝试直接解析\n",
    "        else:\n",
    "            return None\n",
    "            \n",
    "    except Exception as e:\n",
    "        return None\n",
    "\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "print(\"开始处理display_text列，提取更新日期...\")\n",
    "\n",
    "# 查看一些display_text的样例\n",
    "datasets_df_processed['update_date'] = datasets_df_processed['display_text'].apply(extract_update_date)\n",
    "print(\"\\n提取的更新日期示例 (前10个):\")\n",
    "for i, date in enumerate(datasets_df_processed['update_date'].head(10)):\n",
    "    print(f\"  {i+1}. {date}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "caba7c0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "datasets_df_processed.to_csv(\"备份_中间流程结果/datasets_list.csv\", index=False, encoding='utf-8')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "150a22fb",
   "metadata": {},
   "source": [
    "# 进一步整理数据内容"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "514d826e",
   "metadata": {},
   "source": [
    "读取数据"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "514d826e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df=pd.read_csv(\"备份_中间流程结果/datasets_list.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11044a25",
   "metadata": {},
   "source": [
    "定义数据集详细信息爬虫"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "bc4f6e2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import re\n",
    "import json\n",
    "from typing import Dict, List, Optional\n",
    "\n",
    "class HuggingFaceDatasetScraper:\n",
    "    def __init__(self):\n",
    "        self.session = requests.Session()\n",
    "        self.session.headers.update({\n",
    "            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36'\n",
    "        })\n",
    "    \n",
    "    def extract_dataset_info(self, url: str) -> Dict:\n",
    "        \"\"\"\n",
    "        从 Hugging Face dataset 页面提取关键信息\n",
    "        \n",
    "        Args:\n",
    "            url: dataset 页面的 URL\n",
    "            \n",
    "        Returns:\n",
    "            包含提取信息的字典\n",
    "        \"\"\"\n",
    "        try:\n",
    "            response = self.session.get(url)\n",
    "            response.raise_for_status()\n",
    "            soup = BeautifulSoup(response.content, 'html.parser')\n",
    "            \n",
    "            # 提取数据集名称\n",
    "            dataset_name = self._extract_dataset_name(soup, url)\n",
    "            \n",
    "            # 提取标签信息\n",
    "            metadata = self._extract_metadata_tags(soup)\n",
    "            \n",
    "            # 提取文字描述（前两段）\n",
    "            description = self._extract_description(soup)\n",
    "            \n",
    "            result = {\n",
    "                \"dataset_name\": dataset_name,\n",
    "                \"url\": url,\n",
    "                \"metadata\": metadata,\n",
    "                \"description\": description,\n",
    "                \"extraction_status\": \"success\"\n",
    "            }\n",
    "            \n",
    "            return result\n",
    "            \n",
    "        except Exception as e:\n",
    "            return {\n",
    "                \"dataset_name\": None,\n",
    "                \"url\": url,\n",
    "                \"metadata\": {},\n",
    "                \"description\": [],\n",
    "                \"extraction_status\": f\"error: {str(e)}\"\n",
    "            }\n",
    "    \n",
    "    def _extract_dataset_name(self, soup: BeautifulSoup, url: str) -> str:\n",
    "        \"\"\"提取数据集名称\"\"\"\n",
    "        # 尝试从页面标题提取\n",
    "        title_element = soup.find('h1')\n",
    "        if title_element:\n",
    "            return title_element.get_text().strip()\n",
    "        \n",
    "        # 从 URL 提取\n",
    "        url_parts = url.rstrip('/').split('/')\n",
    "        if len(url_parts) >= 2:\n",
    "            return url_parts[-1]\n",
    "        \n",
    "        return \"Unknown\"\n",
    "    \n",
    "    def _extract_metadata_tags(self, soup: BeautifulSoup) -> Dict:\n",
    "        \"\"\"提取上方标签中的元数据信息\"\"\"\n",
    "        metadata = {\n",
    "            \"tasks\": [],\n",
    "            \"modalities\": [],\n",
    "            \"formats\": [],\n",
    "            \"languages\": [],\n",
    "            \"size\": \"\",\n",
    "            \"tags\": [],\n",
    "            \"libraries\": [],\n",
    "            \"license\": \"\",\n",
    "            \"arxiv\": \"\"\n",
    "        }\n",
    "        \n",
    "        # 查找包含标签信息的容器\n",
    "        # 方法1: 通过标签文本查找\n",
    "        self._extract_by_label_text(soup, metadata)\n",
    "        \n",
    "        # 方法2: 通过特定的 class 或 data 属性查找\n",
    "        self._extract_by_attributes(soup, metadata)\n",
    "        \n",
    "        # 方法3: 通过正则表达式在文本中查找\n",
    "        self._extract_by_regex(soup, metadata)\n",
    "        \n",
    "        return metadata\n",
    "    \n",
    "    def _extract_by_label_text(self, soup: BeautifulSoup, metadata: Dict):\n",
    "        \"\"\"通过标签文本查找信息\"\"\"\n",
    "        labels_to_find = {\n",
    "            \"modalities\": [\"Modalities:\", \"modalities\"],\n",
    "            \"formats\": [\"Formats:\", \"formats\"],\n",
    "            \"size\": [\"Size:\", \"size\"],\n",
    "            \"tags\": [\"Tags:\", \"tags\"],\n",
    "            \"libraries\": [\"Libraries:\", \"libraries\"],\n",
    "            \"license\": [\"License:\", \"license\"],\n",
    "            \"languages\": [\"Languages:\", \"languages\"],\n",
    "            \"tasks\": [\"Tasks:\", \"tasks\"],\n",
    "            \"arxiv\": [\"ArXiv:\", \"arxiv\", \"Paper:\"]\n",
    "        }\n",
    "        \n",
    "        for key, possible_labels in labels_to_find.items():\n",
    "            for label in possible_labels:\n",
    "                # 查找包含标签的元素\n",
    "                label_elements = soup.find_all(string=re.compile(label, re.IGNORECASE))\n",
    "                for label_element in label_elements:\n",
    "                    parent = label_element.parent\n",
    "                    if parent:\n",
    "                        # 查找同级或相邻的元素获取值\n",
    "                        value_elements = self._find_value_elements(parent)\n",
    "                        if value_elements:\n",
    "                            if key in [\"size\", \"license\", \"arxiv\"]:\n",
    "                                metadata[key] = \" \".join([elem.get_text().strip() \n",
    "                                                        for elem in value_elements])\n",
    "                            else:\n",
    "                                metadata[key] = [elem.get_text().strip() \n",
    "                                               for elem in value_elements if elem.get_text().strip()]\n",
    "                            break\n",
    "    \n",
    "    def _extract_by_attributes(self, soup: BeautifulSoup, metadata: Dict):\n",
    "        \"\"\"通过HTML属性查找信息\"\"\"\n",
    "        # 查找带有特定 data 属性的元素\n",
    "        data_attrs = soup.find_all(attrs={\"data-testid\": True})\n",
    "        for element in data_attrs:\n",
    "            testid = element.get('data-testid', '')\n",
    "            if 'tag' in testid.lower():\n",
    "                text = element.get_text().strip()\n",
    "                if text and text not in metadata[\"tags\"]:\n",
    "                    metadata[\"tags\"].append(text)\n",
    "        \n",
    "        # 查找许可证链接\n",
    "        license_links = soup.find_all('a', href=re.compile(r'license|cc-by', re.IGNORECASE))\n",
    "        for link in license_links:\n",
    "            license_text = link.get_text().strip()\n",
    "            if license_text and not metadata[\"license\"]:\n",
    "                metadata[\"license\"] = license_text\n",
    "    \n",
    "    def _extract_by_regex(self, soup: BeautifulSoup, metadata: Dict):\n",
    "        \"\"\"通过正则表达式在页面文本中查找信息\"\"\"\n",
    "        page_text = soup.get_text()\n",
    "        \n",
    "        # 查找文件大小\n",
    "        size_patterns = [\n",
    "            r'(\\d+(?:\\.\\d+)?[KMGT]?B?)',\n",
    "            r'(\\d+K?-\\d+[KMG])',\n",
    "            r'Size:\\s*([^\\n\\r]+)'\n",
    "        ]\n",
    "        for pattern in size_patterns:\n",
    "            match = re.search(pattern, page_text, re.IGNORECASE)\n",
    "            if match and not metadata[\"size\"]:\n",
    "                metadata[\"size\"] = match.group(1).strip()\n",
    "                break\n",
    "        \n",
    "        # 查找 ArXiv ID\n",
    "        arxiv_pattern = r'arxiv[:\\s]*(\\d+\\.\\d+)'\n",
    "        arxiv_match = re.search(arxiv_pattern, page_text, re.IGNORECASE)\n",
    "        if arxiv_match and not metadata[\"arxiv\"]:\n",
    "            metadata[\"arxiv\"] = arxiv_match.group(1)\n",
    "    \n",
    "    def _find_value_elements(self, parent_element):\n",
    "        \"\"\"在父元素中查找值元素\"\"\"\n",
    "        value_elements = []\n",
    "        \n",
    "        # 查找同级元素\n",
    "        for sibling in parent_element.find_next_siblings():\n",
    "            if sibling.name in ['span', 'div', 'a', 'code']:\n",
    "                value_elements.append(sibling)\n",
    "            if len(value_elements) >= 10:  # 限制数量\n",
    "                break\n",
    "        \n",
    "        # 查找子元素\n",
    "        if not value_elements:\n",
    "            value_elements = parent_element.find_all(['span', 'div', 'a', 'code'])\n",
    "        \n",
    "        return value_elements[:10]  # 限制返回数量\n",
    "    \n",
    "    def _extract_description(self, soup: BeautifulSoup) -> List[str]:\n",
    "        \"\"\"提取文字描述的前两段\"\"\"\n",
    "        descriptions = []\n",
    "        \n",
    "        # 先尝试查找具体的描述容器\n",
    "        description_selectors = [\n",
    "            'div[class*=\"prose\"]',\n",
    "            'div[class*=\"dataset-description\"]',\n",
    "            'div[class*=\"readme\"]',\n",
    "            'section[class*=\"dataset-description\"]',\n",
    "            'div[class*=\"content\"]',\n",
    "            'article',\n",
    "            'div:has(h1, h2, h3)',\n",
    "            'main',\n",
    "        ]\n",
    "        \n",
    "        description_container = None\n",
    "        for selector in description_selectors:\n",
    "            try:\n",
    "                container = soup.select_one(selector)\n",
    "                if container:\n",
    "                    description_container = container\n",
    "                    break\n",
    "            except:\n",
    "                continue\n",
    "        \n",
    "        # 如果没找到特定容器，使用整个页面\n",
    "        if not description_container:\n",
    "            description_container = soup\n",
    "        \n",
    "        # 查找段落标签\n",
    "        paragraphs = description_container.find_all('p')\n",
    "        for p in paragraphs:\n",
    "            text = p.get_text().strip()\n",
    "            # 过滤条件：足够长，不是导航文本，包含有意义的内容\n",
    "            if (len(text) > 50 and \n",
    "                not self._is_navigation_text(text) and\n",
    "                self._is_meaningful_description(text)):\n",
    "                descriptions.append(text)\n",
    "                if len(descriptions) >= 2:\n",
    "                    return descriptions\n",
    "        \n",
    "        return descriptions[:2]\n",
    "    \n",
    "    def _is_meaningful_description(self, text: str) -> bool:\n",
    "        \"\"\"判断文本是否是有意义的描述内容\"\"\"\n",
    "        # 检查是否包含描述性的关键词\n",
    "        descriptive_patterns = [\n",
    "            r'\\b(dataset|data|contains|provides|includes|designed|used|application)\\b',\n",
    "            r'\\b(machine learning|deep learning|AI|artificial intelligence)\\b',\n",
    "            r'\\b(research|study|analysis|model|algorithm)\\b',\n",
    "            r'\\b(images|text|video|audio|data)\\b'\n",
    "        ]\n",
    "        \n",
    "        text_lower = text.lower()\n",
    "        has_descriptive_content = any(re.search(pattern, text_lower) for pattern in descriptive_patterns)\n",
    "        \n",
    "        # 检查是否有足够的句子结构\n",
    "        sentence_count = len([s for s in text.split('.') if len(s.strip()) > 10])\n",
    "        \n",
    "        # 避免纯技术标签或元数据\n",
    "        avoid_patterns = [\n",
    "            r'^\\s*\\d+(\\.\\d+)?\\s*(GB|MB|KB|TB)\\s*$',  # 纯文件大小\n",
    "            r'^\\s*[a-z-]+:[a-z-]+\\s*$',  # 纯标签格式\n",
    "            r'^\\s*(true|false|yes|no)\\s*$',  # 纯布尔值\n",
    "            r'^\\s*(download|view|edit|fork|clone)\\s*$',  # 纯操作按钮\n",
    "            r'^\\s*\\d+\\s*(downloads?|views?|likes?)\\s*$',  # 纯统计数字\n",
    "            r'^\\s*(last updated|created|modified):\\s*',  # 纯时间戳\n",
    "            r'^\\s*size:\\s*\\d+.*$',  # 纯大小信息\n",
    "            r'^\\s*(cc-by|mit|apache|gpl).*license\\s*$'  # 纯许可证\n",
    "        ]\n",
    "        \n",
    "        is_metadata = any(re.match(pattern, text.strip(), re.IGNORECASE) for pattern in avoid_patterns)\n",
    "        \n",
    "        return (has_descriptive_content and \n",
    "                sentence_count >= 1 and \n",
    "                not is_metadata and\n",
    "                len(text.split()) >= 10)  # 至少10个单词\n",
    "    \n",
    "    def _is_navigation_text(self, text: str) -> bool:\n",
    "        \"\"\"判断是否为导航或菜单文本\"\"\"\n",
    "        if len(text) > 300:  # 长文本不太可能是导航\n",
    "            return False\n",
    "            \n",
    "        nav_keywords = [\n",
    "            'home', 'datasets', 'models', 'spaces', 'docs', 'pricing', \n",
    "            'login', 'sign up', 'menu', 'navigation', 'download', 'view',\n",
    "            'edit', 'fork', 'clone', 'settings', 'discussions', 'files',\n",
    "            'community', 'license', 'paper', 'leaderboard'\n",
    "        ]\n",
    "        \n",
    "        text_lower = text.lower().strip()\n",
    "        \n",
    "        # 检查是否主要由导航关键词组成\n",
    "        words = text_lower.split()\n",
    "        if len(words) <= 5:  # 短文本更可能是导航\n",
    "            nav_word_count = sum(1 for word in words if any(kw in word for kw in nav_keywords))\n",
    "            if nav_word_count >= len(words) * 0.6:  # 60%以上是导航词汇\n",
    "                return True\n",
    "        \n",
    "        # 检查特定的导航模式\n",
    "        nav_patterns = [\n",
    "            r'^\\s*(home|datasets|models|spaces|docs)\\s*$',\n",
    "            r'^\\s*(download|view|edit|fork|clone)\\s*$'\n",
    "        ]\n",
    "        \n",
    "        return any(re.match(pattern, text_lower) for pattern in nav_patterns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "6fa6ecc5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "\n",
    "def extract_huggingface_dataset_info(df, url_column='url', delay=1, batch_size=None):\n",
    "    \"\"\"\n",
    "    简化的Hugging Face数据集信息批量提取函数\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    df : pd.DataFrame\n",
    "        包含Hugging Face URL的数据框\n",
    "    url_column : str\n",
    "        包含URL的列名，默认为'url'\n",
    "    delay : float\n",
    "        请求间隔时间（秒），默认为1秒\n",
    "    batch_size : int or None\n",
    "        批次大小，None表示处理所有数据\n",
    "    \n",
    "    Returns:\n",
    "    --------\n",
    "    pd.DataFrame\n",
    "        添加了新列的数据框，包含：\n",
    "        - modalities: 模态信息\n",
    "        - formats: 数据格式\n",
    "        - size: 数据集大小\n",
    "        - tags: 标签\n",
    "        - libraries: 相关库\n",
    "        - license: 许可证\n",
    "        - description: 描述\n",
    "        - extraction_status: 提取状态\n",
    "    \"\"\"\n",
    "    \n",
    "    # 创建scraper实例\n",
    "    scraper = HuggingFaceDatasetScraper()\n",
    "    \n",
    "    # 复制数据框避免修改原始数据\n",
    "    result_df = df.copy()\n",
    "    \n",
    "    # 初始化新列\n",
    "    new_columns = ['modalities', 'formats', 'size', 'tags', 'libraries', 'license', 'description', 'extraction_status']\n",
    "    for col in new_columns:\n",
    "        if col not in result_df.columns:\n",
    "            result_df[col] = None\n",
    "    \n",
    "    # 检查URL列是否存在\n",
    "    if url_column not in result_df.columns:\n",
    "        raise ValueError(f\"列 '{url_column}' 不存在于数据框中\")\n",
    "    \n",
    "    # 确定要处理的行\n",
    "    total_rows = len(result_df)\n",
    "    if batch_size is not None:\n",
    "        process_rows = min(batch_size, total_rows)\n",
    "        process_indices = result_df.index[:process_rows]\n",
    "    else:\n",
    "        process_rows = total_rows\n",
    "        process_indices = result_df.index\n",
    "    \n",
    "    print(f\"开始处理 {process_rows} 个Hugging Face数据集URL...\")\n",
    "    \n",
    "    # 统计变量\n",
    "    success_count = 0\n",
    "    failed_count = 0\n",
    "    error_count = 0\n",
    "    \n",
    "    # 使用tqdm显示进度条\n",
    "    for idx in tqdm(process_indices, desc=\"提取数据集信息\"):\n",
    "        try:\n",
    "            url = result_df.loc[idx, url_column]\n",
    "            \n",
    "            # 跳过空URL\n",
    "            if pd.isna(url) or url == '':\n",
    "                result_df.loc[idx, 'extraction_status'] = 'EMPTY_URL'\n",
    "                continue\n",
    "            \n",
    "            # 确保URL是字符串格式\n",
    "            url = str(url).strip()\n",
    "            \n",
    "            # 验证是否为Hugging Face数据集URL\n",
    "            if 'huggingface.co/datasets/' not in url:\n",
    "                result_df.loc[idx, 'extraction_status'] = 'INVALID_URL'\n",
    "                continue\n",
    "            \n",
    "            # 提取信息\n",
    "            result = scraper.extract_dataset_info(url)\n",
    "            \n",
    "            if result[\"extraction_status\"] == \"success\":\n",
    "                metadata = result['metadata']\n",
    "                \n",
    "                # 更新数据框\n",
    "                result_df.loc[idx, 'modalities'] = str(metadata.get('modalities', [])) if metadata.get('modalities') else None\n",
    "                result_df.loc[idx, 'formats'] = str(metadata.get('formats', [])) if metadata.get('formats') else None\n",
    "                result_df.loc[idx, 'size'] = metadata.get('size', '') if metadata.get('size') else None\n",
    "                result_df.loc[idx, 'tags'] = str(metadata.get('tags', [])) if metadata.get('tags') else None\n",
    "                result_df.loc[idx, 'libraries'] = str(metadata.get('libraries', [])) if metadata.get('libraries') else None\n",
    "                result_df.loc[idx, 'license'] = metadata.get('license', '') if metadata.get('license') else None\n",
    "                result_df.loc[idx, 'description'] = str(result.get('description', [])) if result.get('description') else None\n",
    "                result_df.loc[idx, 'extraction_status'] = 'SUCCESS'\n",
    "                \n",
    "                success_count += 1\n",
    "            else:\n",
    "                result_df.loc[idx, 'extraction_status'] = f'FAILED: {result[\"extraction_status\"]}'\n",
    "                failed_count += 1\n",
    "            \n",
    "            # 请求间隔\n",
    "            if delay > 0:\n",
    "                time.sleep(delay)\n",
    "                \n",
    "        except Exception as e:\n",
    "            result_df.loc[idx, 'extraction_status'] = f'ERROR: {str(e)}'\n",
    "            error_count += 1\n",
    "            continue\n",
    "    \n",
    "    # 显示最终统计\n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(\"处理完成！统计结果:\")\n",
    "    print(f\"总处理数量: {process_rows}\")\n",
    "    print(f\"✅ 成功: {success_count} ({success_count/process_rows*100:.1f}%)\")\n",
    "    print(f\"❌ 失败: {failed_count} ({failed_count/process_rows*100:.1f}%)\")\n",
    "    print(f\"⚠️  错误: {error_count} ({error_count/process_rows*100:.1f}%)\")\n",
    "    \n",
    "    # 显示成功示例\n",
    "    successful_rows = result_df[result_df['extraction_status'] == 'SUCCESS']\n",
    "    if len(successful_rows) > 0:\n",
    "        print(f\"\\n成功提取信息的示例 (前3个):\")\n",
    "        for idx, row in successful_rows.head(3).iterrows():\n",
    "            dataset_name = row.get('name', row.get('item_name', 'Unknown'))\n",
    "            print(f\"- {dataset_name}\")\n",
    "            print(f\"  大小: {row['size'] or 'N/A'}\")\n",
    "            print(f\"  许可证: {row['license'] or 'N/A'}\")\n",
    "            print(f\"  标签数量: {len(eval(row['tags'])) if row['tags'] and row['tags'] != 'None' else 0}\")\n",
    "    \n",
    "    return result_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "3748efb9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "开始处理 101 个Hugging Face数据集URL...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "提取数据集信息: 100%|██████████| 101/101 [02:47<00:00,  1.66s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "处理完成！统计结果:\n",
      "总处理数量: 101\n",
      "✅ 成功: 101 (100.0%)\n",
      "❌ 失败: 0 (0.0%)\n",
      "⚠️  错误: 0 (0.0%)\n",
      "\n",
      "成功提取信息的示例 (前3个):\n",
      "- allenai/dolma\n",
      "  大小: n>1T\n",
      "  许可证: odc-by\n",
      "  标签数量: 3\n",
      "- nasa-ibm-ai4science/SDO_training\n",
      "  大小: n>1T\n",
      "  许可证: mit\n",
      "  标签数量: 3\n",
      "- 3DTopia/4DNeX-10M\n",
      "  大小: n>1T\n",
      "  许可证: apache-2.0\n",
      "  标签数量: 4\n",
      "处理完成！\n",
      "原始数据行数: 101\n",
      "处理后数据行数: 101\n",
      "新增列: ['modalities', 'formats', 'size', 'tags', 'libraries', 'license', 'description', 'extraction_status']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# 使用函数处理数据集信息提取\n",
    "# 请根据实际情况修改以下参数\n",
    "\n",
    "# 参数设置\n",
    "input_dataframe = df  # 替换为你的数据框变量名\n",
    "url_column_name = 'url'  # 替换为包含URL的列名\n",
    "request_delay = 1  # 请求间隔时间（秒）\n",
    "process_batch_size = None  # 批次大小，None表示处理全部，也可以设置如10, 50等\n",
    "\n",
    "# 执行提取\n",
    "result_df = extract_huggingface_dataset_info(\n",
    "    df=input_dataframe,\n",
    "    url_column=url_column_name,\n",
    "    delay=request_delay,\n",
    "    batch_size=process_batch_size\n",
    ")\n",
    "\n",
    "# 显示结果\n",
    "print(\"处理完成！\")\n",
    "print(f\"原始数据行数: {len(input_dataframe)}\")\n",
    "print(f\"处理后数据行数: {len(result_df)}\")\n",
    "print(f\"新增列: {[col for col in result_df.columns if col not in input_dataframe.columns]}\")\n",
    "\n",
    "# 保存结果（可选）\n",
    "# result_df.to_excel('dataset_info_extracted.xlsx', index=False)\n",
    "# result_df.to_csv('dataset_info_extracted.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d533a1f2",
   "metadata": {},
   "source": [
    "使用huggingface_hub库进行数据集信息提取：文件格式与大小"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "ea03c6ec",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File sizes for dataset 'isaaccorley/AlphaEarth-EuroSAT':\n",
      "\n",
      "  .gitattributes: 0.00 MB\n",
      "  README.md: 0.00 MB\n",
      "  eurosat-aef-embeddings.npz: 5.44 MB\n",
      "  eurosat-aef.tar.gz: 5710.42 MB\n",
      "\n",
      "Total size: 5715.86 MB\n"
     ]
    }
   ],
   "source": [
    "from huggingface_hub import HfApi\n",
    "def print_dataset_file_sizes(repo_id):\n",
    "    api = HfApi()\n",
    "    dataset_info = api.dataset_info(repo_id=repo_id, files_metadata=True)\n",
    "\n",
    "    total_size_bytes = 0  \n",
    "    print(f\"File sizes for dataset '{repo_id}':\\n\")  \n",
    "    for sibling in dataset_info.siblings:  \n",
    "        filename = sibling.rfilename  \n",
    "        size_in_bytes = sibling.size or 0  \n",
    "        total_size_bytes += size_in_bytes  \n",
    "        size_mb = size_in_bytes / (1024 * 1024)  \n",
    "        print(f\"  {filename}: {size_mb:.2f} MB\")  \n",
    "\n",
    "    total_size_mb = total_size_bytes / (1024 * 1024)  \n",
    "    print(f\"\\nTotal size: {total_size_mb:.2f} MB\")  \n",
    "\n",
    "print_dataset_file_sizes('isaaccorley/AlphaEarth-EuroSAT')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "684774e0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "函数定义完成（已添加2分钟超时机制）\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from collections import defaultdict\n",
    "import os\n",
    "from tqdm import tqdm\n",
    "import time\n",
    "import signal\n",
    "\n",
    "class TimeoutError(Exception):\n",
    "    pass\n",
    "\n",
    "def timeout_handler(signum, frame):\n",
    "    raise TimeoutError(\"操作超时\")\n",
    "\n",
    "def get_dataset_file_info(dataset_id, timeout_seconds=120):\n",
    "    \"\"\"\n",
    "    获取数据集的文件信息，包括格式和总大小（修复版本）\n",
    "    使用dataset_info来获取更详细的文件大小信息\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # 设置超时信号\n",
    "        signal.signal(signal.SIGALRM, timeout_handler)\n",
    "        signal.alarm(timeout_seconds)\n",
    "        \n",
    "        start_time = time.time()\n",
    "        api = HfApi()\n",
    "        \n",
    "        # 使用dataset_info获取详细信息，包括文件元数据\n",
    "        dataset_info = api.dataset_info(repo_id=dataset_id, files_metadata=True)\n",
    "        \n",
    "        # 统计文件格式和大小\n",
    "        formats = set()\n",
    "        total_size = 0\n",
    "        file_count = 0\n",
    "        \n",
    "        # 从siblings中获取文件信息\n",
    "        if hasattr(dataset_info, 'siblings') and dataset_info.siblings:\n",
    "            for sibling in dataset_info.siblings:\n",
    "                # 获取文件扩展名\n",
    "                if hasattr(sibling, 'rfilename') and sibling.rfilename:\n",
    "                    file_path = sibling.rfilename\n",
    "                    if '.' in file_path:\n",
    "                        extension = os.path.splitext(file_path)[1].lower()\n",
    "                        if extension:  # 确保扩展名不为空\n",
    "                            formats.add(extension)\n",
    "                \n",
    "                # 获取文件大小\n",
    "                if hasattr(sibling, 'size') and sibling.size:\n",
    "                    total_size += sibling.size\n",
    "                    file_count += 1\n",
    "        \n",
    "        # 如果没有通过siblings获取到大小，尝试其他方法\n",
    "        if total_size == 0:\n",
    "            try:\n",
    "                # 尝试使用repo_info\n",
    "                repo_info = api.repo_info(repo_id=dataset_id, repo_type=\"dataset\")\n",
    "                if hasattr(repo_info, 'size') and repo_info.size:\n",
    "                    total_size = repo_info.size\n",
    "            except:\n",
    "                pass\n",
    "        \n",
    "        # 取消超时\n",
    "        signal.alarm(0)\n",
    "        \n",
    "        elapsed_time = time.time() - start_time\n",
    "        return {\n",
    "            'formats': list(formats) if formats else [],\n",
    "            'size': total_size if total_size > 0 else None,\n",
    "            'file_count': file_count,\n",
    "            'status': 'success',\n",
    "            'elapsed_time': elapsed_time\n",
    "        }\n",
    "        \n",
    "    except TimeoutError:\n",
    "        signal.alarm(0)  # 取消超时\n",
    "        return {\n",
    "            'formats': [],\n",
    "            'size': None,\n",
    "            'file_count': 0,\n",
    "            'status': f'timeout: 处理超过{timeout_seconds}秒',\n",
    "            'elapsed_time': timeout_seconds\n",
    "        }\n",
    "    except Exception as e:\n",
    "        signal.alarm(0)  # 取消超时\n",
    "        return {\n",
    "            'formats': [],\n",
    "            'size': None,\n",
    "            'file_count': 0,\n",
    "            'status': f'error: {str(e)}',\n",
    "            'elapsed_time': 0\n",
    "        }\n",
    "\n",
    "def format_size(size_bytes):\n",
    "    \"\"\"\n",
    "    将字节大小转换为易读格式\n",
    "    \"\"\"\n",
    "    if size_bytes is None or size_bytes == 0:\n",
    "        return None\n",
    "    \n",
    "    units = ['B', 'KB', 'MB', 'GB', 'TB']\n",
    "    size = float(size_bytes)\n",
    "    unit_index = 0\n",
    "    \n",
    "    while size >= 1024 and unit_index < len(units) - 1:\n",
    "        size /= 1024\n",
    "        unit_index += 1\n",
    "    \n",
    "    return f\"{size:.2f} {units[unit_index]}\"\n",
    "\n",
    "print(\"函数定义完成（已添加2分钟超时机制）\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "69546ba4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "开始处理数据集文件信息...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "处理数据集:   0%|          | 0/101 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "🔄 正在处理: allenai/dolma\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "处理数据集:   1%|          | 1/101 [00:00<00:55,  1.80it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ allenai/dolma: 成功处理 (耗时: 0.6s)\n",
      "\n",
      "🔄 正在处理: nasa-ibm-ai4science/SDO_training\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "处理数据集:   2%|▏         | 2/101 [00:01<00:57,  1.71it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ nasa-ibm-ai4science/SDO_training: 成功处理 (耗时: 0.6s)\n",
      "\n",
      "🔄 正在处理: 3DTopia/4DNeX-10M\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "处理数据集:   3%|▎         | 3/101 [00:02<01:33,  1.05it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ 3DTopia/4DNeX-10M: 成功处理 (耗时: 1.4s)\n",
      "\n",
      "🔄 正在处理: allenai/MADLAD-400\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "处理数据集:   4%|▍         | 4/101 [00:37<22:59, 14.22s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ allenai/MADLAD-400: 成功处理 (耗时: 34.6s)\n",
      "\n",
      "🔄 正在处理: urbanaudiosensing/ASPED\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "处理数据集:   5%|▍         | 5/101 [00:38<15:17,  9.55s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ urbanaudiosensing/ASPED: 成功处理 (耗时: 1.3s)\n",
      "\n",
      "🔄 正在处理: x-humanoid-robomind/RoboMIND\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "处理数据集:   6%|▌         | 6/101 [00:39<10:31,  6.64s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ x-humanoid-robomind/RoboMIND: 成功处理 (耗时: 1.0s)\n",
      "\n",
      "🔄 正在处理: InternRobotics/InternData-N1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "处理数据集:   7%|▋         | 7/101 [00:41<07:52,  5.03s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ InternRobotics/InternData-N1: 成功处理 (耗时: 1.7s)\n",
      "\n",
      "🔄 正在处理: DeliberatorArchiver/asmr-archive-data-01\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "处理数据集:   8%|▊         | 8/101 [01:07<18:28, 11.92s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ DeliberatorArchiver/asmr-archive-data-01: 成功处理 (耗时: 26.7s)\n",
      "\n",
      "🔄 正在处理: DeliberatorArchiver/asmr-archive-data-02\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "处理数据集:   9%|▉         | 9/101 [01:35<25:45, 16.80s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ DeliberatorArchiver/asmr-archive-data-02: 成功处理 (耗时: 27.5s)\n",
      "\n",
      "🔄 正在处理: oscar-corpus/OSCAR-2109\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "处理数据集:  10%|▉         | 10/101 [01:42<21:01, 13.87s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ oscar-corpus/OSCAR-2109: 成功处理 (耗时: 7.3s)\n",
      "\n",
      "🔄 正在处理: poloclub/diffusiondb\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "处理数据集:  11%|█         | 11/101 [01:46<16:20, 10.89s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ poloclub/diffusiondb: 成功处理 (耗时: 4.1s)\n",
      "\n",
      "🔄 正在处理: CarperAI/pilev2-dev\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "处理数据集:  12%|█▏        | 12/101 [01:48<12:07,  8.18s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ CarperAI/pilev2-dev: 成功处理 (耗时: 2.0s)\n",
      "\n",
      "🔄 正在处理: oscar-corpus/OSCAR-2301\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "处理数据集:  13%|█▎        | 13/101 [01:50<08:59,  6.13s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ oscar-corpus/OSCAR-2301: 成功处理 (耗时: 1.4s)\n",
      "\n",
      "🔄 正在处理: oscar-corpus/oscar-2301-hpc\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "处理数据集:  14%|█▍        | 14/101 [01:50<06:26,  4.44s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ oscar-corpus/oscar-2301-hpc: 成功处理 (耗时: 0.5s)\n",
      "\n",
      "🔄 正在处理: ColtonAi/Oi\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "处理数据集:  15%|█▍        | 15/101 [01:51<04:43,  3.30s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ ColtonAi/Oi: 成功处理 (耗时: 0.6s)\n",
      "\n",
      "🔄 正在处理: NOABOL35631y/T\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "处理数据集:  16%|█▌        | 16/101 [01:51<03:30,  2.47s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ NOABOL35631y/T: 成功处理 (耗时: 0.5s)\n",
      "\n",
      "🔄 正在处理: oscar-corpus/colossal-oscar-1.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "处理数据集:  17%|█▋        | 17/101 [01:54<03:24,  2.44s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ oscar-corpus/colossal-oscar-1.0: 成功处理 (耗时: 2.4s)\n",
      "\n",
      "🔄 正在处理: min06/Hscb\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "处理数据集:  18%|█▊        | 18/101 [01:54<02:36,  1.89s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ min06/Hscb: 成功处理 (耗时: 0.6s)\n",
      "\n",
      "🔄 正在处理: CropNet/CropNet\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "处理数据集:  19%|█▉        | 19/101 [01:59<03:39,  2.67s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ CropNet/CropNet: 成功处理 (耗时: 4.5s)\n",
      "\n",
      "🔄 正在处理: DL3DV/DL3DV-Benchmark\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "处理数据集:  20%|█▉        | 20/101 [02:38<18:27, 13.67s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ DL3DV/DL3DV-Benchmark: 成功处理 (耗时: 39.3s)\n",
      "\n",
      "🔄 正在处理: Pytagora/Datalab\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "处理数据集:  21%|██        | 21/101 [02:39<13:06,  9.84s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Pytagora/Datalab: 成功处理 (耗时: 0.9s)\n",
      "\n",
      "🔄 正在处理: DL3DV/DL3DV-ALL-4K\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "处理数据集:  22%|██▏       | 22/101 [02:42<10:18,  7.82s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ DL3DV/DL3DV-ALL-4K: 成功处理 (耗时: 3.1s)\n",
      "\n",
      "🔄 正在处理: shoyimobloqulov/text-to-speech-tts\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "处理数据集:  23%|██▎       | 23/101 [02:43<07:17,  5.61s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ shoyimobloqulov/text-to-speech-tts: 成功处理 (耗时: 0.5s)\n",
      "\n",
      "🔄 正在处理: DL3DV/DL3DV-ALL-960P\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "处理数据集:  24%|██▍       | 24/101 [02:46<06:16,  4.90s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ DL3DV/DL3DV-ALL-960P: 成功处理 (耗时: 3.2s)\n",
      "\n",
      "🔄 正在处理: NikkoIGuess/NikkoDoesRandom_Ai\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "处理数据集:  25%|██▍       | 25/101 [02:46<04:33,  3.60s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ NikkoIGuess/NikkoDoesRandom_Ai: 成功处理 (耗时: 0.6s)\n",
      "\n",
      "🔄 正在处理: DL3DV/DL3DV-ALL-2K\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "处理数据集:  26%|██▌       | 26/101 [02:50<04:31,  3.62s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ DL3DV/DL3DV-ALL-2K: 成功处理 (耗时: 3.7s)\n",
      "\n",
      "🔄 正在处理: DL3DV/DL3DV-ALL-video\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "处理数据集:  27%|██▋       | 27/101 [02:54<04:29,  3.65s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ DL3DV/DL3DV-ALL-video: 成功处理 (耗时: 3.7s)\n",
      "\n",
      "🔄 正在处理: HPLT/hplt_monolingual_v1_2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "处理数据集:  28%|██▊       | 28/101 [02:54<03:17,  2.71s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ HPLT/hplt_monolingual_v1_2: 成功处理 (耗时: 0.5s)\n",
      "\n",
      "🔄 正在处理: DL3DV/DL3DV-ALL-ColmapCache\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "处理数据集:  29%|██▊       | 29/101 [02:58<03:35,  2.99s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ DL3DV/DL3DV-ALL-ColmapCache: 成功处理 (耗时: 3.7s)\n",
      "\n",
      "🔄 正在处理: Pragadishwaran/manual\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "处理数据集:  30%|██▉       | 30/101 [02:59<02:39,  2.25s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Pragadishwaran/manual: 成功处理 (耗时: 0.5s)\n",
      "\n",
      "🔄 正在处理: psoldunov/testset\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "处理数据集:  31%|███       | 31/101 [02:59<02:01,  1.73s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ psoldunov/testset: 成功处理 (耗时: 0.5s)\n",
      "\n",
      "🔄 正在处理: meg/dolma-v1_6-sample\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "处理数据集:  32%|███▏      | 32/101 [03:00<01:35,  1.38s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ meg/dolma-v1_6-sample: 成功处理 (耗时: 0.5s)\n",
      "\n",
      "🔄 正在处理: akhilhsingh/homeo-dataset\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "处理数据集:  33%|███▎      | 33/101 [03:00<01:20,  1.19s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ akhilhsingh/homeo-dataset: 成功处理 (耗时: 0.7s)\n",
      "\n",
      "🔄 正在处理: oscar-corpus/community-oscar\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "处理数据集:  34%|███▎      | 34/101 [03:28<10:15,  9.19s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ oscar-corpus/community-oscar: 成功处理 (耗时: 27.9s)\n",
      "\n",
      "🔄 正在处理: Mohamedfadil369/BrainData\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "处理数据集:  35%|███▍      | 35/101 [03:29<07:13,  6.57s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Mohamedfadil369/BrainData: 成功处理 (耗时: 0.5s)\n",
      "\n",
      "🔄 正在处理: nkandpa2/cccc_all_domains\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "处理数据集:  36%|███▌      | 36/101 [03:29<05:15,  4.85s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ nkandpa2/cccc_all_domains: 成功处理 (耗时: 0.8s)\n",
      "\n",
      "🔄 正在处理: artanekrem/SDFSDF\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "处理数据集:  37%|███▋      | 37/101 [03:30<03:46,  3.55s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ artanekrem/SDFSDF: 成功处理 (耗时: 0.5s)\n",
      "\n",
      "🔄 正在处理: liuqingquan/test2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "处理数据集:  38%|███▊      | 38/101 [03:31<02:51,  2.72s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ liuqingquan/test2: 成功处理 (耗时: 0.8s)\n",
      "\n",
      "🔄 正在处理: Artificial-superintelligence/Athtest\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "处理数据集:  39%|███▊      | 39/101 [03:31<02:07,  2.05s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Artificial-superintelligence/Athtest: 成功处理 (耗时: 0.5s)\n",
      "\n",
      "🔄 正在处理: DataoceanAI/Lip_reading_Speech_Video_Corpus\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "处理数据集:  40%|███▉      | 40/101 [03:32<01:37,  1.60s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ DataoceanAI/Lip_reading_Speech_Video_Corpus: 成功处理 (耗时: 0.5s)\n",
      "\n",
      "🔄 正在处理: shuxunoo/NFT-Net\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "处理数据集:  41%|████      | 41/101 [03:33<01:22,  1.38s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ shuxunoo/NFT-Net: 成功处理 (耗时: 0.9s)\n",
      "\n",
      "🔄 正在处理: BAAI/IndustryCorpus_programming\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "处理数据集:  42%|████▏     | 42/101 [03:33<01:04,  1.09s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ BAAI/IndustryCorpus_programming: 成功处理 (耗时: 0.4s)\n",
      "\n",
      "🔄 正在处理: BAAI/IndustryCorpus_emotion\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "处理数据集:  43%|████▎     | 43/101 [03:34<00:56,  1.03it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ BAAI/IndustryCorpus_emotion: 成功处理 (耗时: 0.7s)\n",
      "\n",
      "🔄 正在处理: BAAI/IndustryCorpus_mathematics\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "处理数据集:  44%|████▎     | 44/101 [03:34<00:46,  1.23it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ BAAI/IndustryCorpus_mathematics: 成功处理 (耗时: 0.5s)\n",
      "\n",
      "🔄 正在处理: BAAI/IndustryCorpus_ai\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "处理数据集:  45%|████▍     | 45/101 [03:35<00:41,  1.33it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ BAAI/IndustryCorpus_ai: 成功处理 (耗时: 0.6s)\n",
      "\n",
      "🔄 正在处理: Yuqi1997/DrivingDojo\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "处理数据集:  46%|████▌     | 46/101 [03:35<00:35,  1.53it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Yuqi1997/DrivingDojo: 成功处理 (耗时: 0.4s)\n",
      "\n",
      "🔄 正在处理: Zyphra/Zyda-2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "处理数据集:  47%|████▋     | 47/101 [04:12<10:18, 11.45s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Zyphra/Zyda-2: 成功处理 (耗时: 36.6s)\n",
      "\n",
      "🔄 正在处理: Maple728/Time-300B\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "处理数据集:  48%|████▊     | 48/101 [04:13<07:16,  8.23s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Maple728/Time-300B: 成功处理 (耗时: 0.7s)\n",
      "\n",
      "🔄 正在处理: DL3DV/DL3DV-Drone\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "处理数据集:  49%|████▊     | 49/101 [04:13<05:07,  5.92s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ DL3DV/DL3DV-Drone: 成功处理 (耗时: 0.5s)\n",
      "\n",
      "🔄 正在处理: Lightarmortech/Bot.com\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "处理数据集:  50%|████▉     | 50/101 [04:14<03:38,  4.28s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Lightarmortech/Bot.com: 成功处理 (耗时: 0.5s)\n",
      "\n",
      "🔄 正在处理: LLM360/TxT360\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "处理数据集:  50%|█████     | 51/101 [04:59<13:55, 16.72s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ LLM360/TxT360: 成功处理 (耗时: 45.7s)\n",
      "\n",
      "🔄 正在处理: avalab/Allo-AVA\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "处理数据集:  51%|█████▏    | 52/101 [05:02<10:15, 12.56s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ avalab/Allo-AVA: 成功处理 (耗时: 2.9s)\n",
      "\n",
      "🔄 正在处理: anchovy/maple728-time_300B\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "处理数据集:  52%|█████▏    | 53/101 [05:03<07:10,  8.98s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ anchovy/maple728-time_300B: 成功处理 (耗时: 0.6s)\n",
      "\n",
      "🔄 正在处理: Khole1234/Chloe\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "处理数据集:  53%|█████▎    | 54/101 [05:03<05:03,  6.45s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Khole1234/Chloe: 成功处理 (耗时: 0.5s)\n",
      "\n",
      "🔄 正在处理: Moonlightsonata/vso\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "处理数据集:  54%|█████▍    | 55/101 [05:04<03:34,  4.66s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Moonlightsonata/vso: 成功处理 (耗时: 0.5s)\n",
      "\n",
      "🔄 正在处理: haggs/test\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "处理数据集:  55%|█████▌    | 56/101 [05:04<02:35,  3.46s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ haggs/test: 成功处理 (耗时: 0.6s)\n",
      "\n",
      "🔄 正在处理: Prabhuram/Medicine-List\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "处理数据集:  56%|█████▋    | 57/101 [05:06<02:03,  2.80s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Prabhuram/Medicine-List: 成功处理 (耗时: 1.3s)\n",
      "\n",
      "🔄 正在处理: dijihax/Dataset\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "处理数据集:  57%|█████▋    | 58/101 [05:06<01:32,  2.15s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ dijihax/Dataset: 成功处理 (耗时: 0.7s)\n",
      "\n",
      "🔄 正在处理: prasenjeet5/DeLTa-Zz-Core\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "处理数据集:  58%|█████▊    | 59/101 [05:07<01:14,  1.76s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ prasenjeet5/DeLTa-Zz-Core: 成功处理 (耗时: 0.8s)\n",
      "\n",
      "🔄 正在处理: coan/bot_claiton\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "处理数据集:  59%|█████▉    | 60/101 [05:08<00:58,  1.42s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ coan/bot_claiton: 成功处理 (耗时: 0.6s)\n",
      "\n",
      "🔄 正在处理: neis-lab/mmcows\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "处理数据集:  60%|██████    | 61/101 [05:09<00:47,  1.19s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ neis-lab/mmcows: 成功处理 (耗时: 0.7s)\n",
      "\n",
      "🔄 正在处理: m-a-p/FineFineWeb-fasttext-seeddata\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "处理数据集:  61%|██████▏   | 62/101 [05:09<00:40,  1.04s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ m-a-p/FineFineWeb-fasttext-seeddata: 成功处理 (耗时: 0.7s)\n",
      "\n",
      "🔄 正在处理: 1989shack/Database\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "处理数据集:  62%|██████▏   | 63/101 [05:10<00:33,  1.13it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ 1989shack/Database: 成功处理 (耗时: 0.5s)\n",
      "\n",
      "🔄 正在处理: sarahgillet/BrainstormingDataset\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "处理数据集:  63%|██████▎   | 64/101 [05:10<00:29,  1.25it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ sarahgillet/BrainstormingDataset: 成功处理 (耗时: 0.6s)\n",
      "\n",
      "🔄 正在处理: BIOMEDICA/biomedica_webdataset_24M\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "处理数据集:  64%|██████▍   | 65/101 [05:11<00:32,  1.11it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ BIOMEDICA/biomedica_webdataset_24M: 成功处理 (耗时: 1.1s)\n",
      "\n",
      "🔄 正在处理: Majidu/Ohr\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "处理数据集:  65%|██████▌   | 66/101 [05:12<00:26,  1.32it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Majidu/Ohr: 成功处理 (耗时: 0.4s)\n",
      "\n",
      "🔄 正在处理: GraspClutter6D/GraspClutter6D\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "处理数据集:  66%|██████▋   | 67/101 [05:12<00:23,  1.45it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ GraspClutter6D/GraspClutter6D: 成功处理 (耗时: 0.5s)\n",
      "\n",
      "🔄 正在处理: Johnnyboystar/Images\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "处理数据集:  67%|██████▋   | 68/101 [05:13<00:20,  1.60it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Johnnyboystar/Images: 成功处理 (耗时: 0.5s)\n",
      "\n",
      "🔄 正在处理: agibot-world/AgiBotDigitalWorld\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "处理数据集:  68%|██████▊   | 69/101 [05:28<02:39,  4.99s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ agibot-world/AgiBotDigitalWorld: 成功处理 (耗时: 15.2s)\n",
      "\n",
      "🔄 正在处理: ZeppelinCorp/Eclipse_Corpuz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "处理数据集:  69%|██████▉   | 70/101 [05:29<01:52,  3.63s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ ZeppelinCorp/Eclipse_Corpuz: 成功处理 (耗时: 0.4s)\n",
      "\n",
      "🔄 正在处理: IntelLabs/BlueLens\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "处理数据集:  70%|███████   | 71/101 [05:31<01:38,  3.28s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ IntelLabs/BlueLens: 成功处理 (耗时: 2.5s)\n",
      "\n",
      "🔄 正在处理: jobs-git/Zyda-2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "处理数据集:  71%|███████▏  | 72/101 [06:04<05:50, 12.09s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ jobs-git/Zyda-2: 成功处理 (耗时: 32.7s)\n",
      "\n",
      "🔄 正在处理: jobs-git/HPLT2.0_cleaned\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "处理数据集:  72%|███████▏  | 73/101 [06:36<08:24, 18.03s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ jobs-git/HPLT2.0_cleaned: 成功处理 (耗时: 31.9s)\n",
      "\n",
      "🔄 正在处理: embed2scale/SSL4EO-S12-v1.1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "处理数据集:  73%|███████▎  | 74/101 [06:40<06:17, 13.97s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ embed2scale/SSL4EO-S12-v1.1: 成功处理 (耗时: 4.5s)\n",
      "\n",
      "🔄 正在处理: torchgeo/CropClimateX\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "处理数据集:  74%|███████▍  | 75/101 [06:47<05:06, 11.78s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ torchgeo/CropClimateX: 成功处理 (耗时: 6.7s)\n",
      "\n",
      "🔄 正在处理: everrer/121212\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "处理数据集:  75%|███████▌  | 76/101 [06:47<03:29,  8.39s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ everrer/121212: 成功处理 (耗时: 0.5s)\n",
      "\n",
      "🔄 正在处理: maxkaufmann/allenai_dolma_test_set\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "处理数据集:  76%|███████▌  | 77/101 [06:48<02:25,  6.06s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ maxkaufmann/allenai_dolma_test_set: 成功处理 (耗时: 0.6s)\n",
      "\n",
      "🔄 正在处理: BIOMEDICA/biomedica_microscopy_subset_webdataset\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "处理数据集:  77%|███████▋  | 78/101 [06:49<01:43,  4.49s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ BIOMEDICA/biomedica_microscopy_subset_webdataset: 成功处理 (耗时: 0.8s)\n",
      "\n",
      "🔄 正在处理: BIOMEDICA/biomedica_dermatology_subset_webdataset\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "处理数据集:  78%|███████▊  | 79/101 [06:49<01:13,  3.35s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ BIOMEDICA/biomedica_dermatology_subset_webdataset: 成功处理 (耗时: 0.7s)\n",
      "\n",
      "🔄 正在处理: BIOMEDICA/biomedica_surgery_subset_webdataset\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "处理数据集:  79%|███████▉  | 80/101 [06:50<00:53,  2.57s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ BIOMEDICA/biomedica_surgery_subset_webdataset: 成功处理 (耗时: 0.7s)\n",
      "\n",
      "🔄 正在处理: trillarmybewm/trillium\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "处理数据集:  80%|████████  | 81/101 [06:51<00:39,  1.95s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ trillarmybewm/trillium: 成功处理 (耗时: 0.5s)\n",
      "\n",
      "🔄 正在处理: DL3DV/DL3DV-GS-960P\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "处理数据集:  81%|████████  | 82/101 [06:55<00:53,  2.80s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ DL3DV/DL3DV-GS-960P: 成功处理 (耗时: 4.8s)\n",
      "\n",
      "🔄 正在处理: criteo/CriteoClickLogs\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "处理数据集:  82%|████████▏ | 83/101 [06:56<00:37,  2.09s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ criteo/CriteoClickLogs: 成功处理 (耗时: 0.4s)\n",
      "\n",
      "🔄 正在处理: jobs-git/diffusiondb\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "处理数据集:  83%|████████▎ | 84/101 [07:03<00:59,  3.50s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ jobs-git/diffusiondb: 成功处理 (耗时: 6.8s)\n",
      "\n",
      "🔄 正在处理: introspector/solfunmeme\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "处理数据集:  84%|████████▍ | 85/101 [07:06<00:53,  3.35s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ introspector/solfunmeme: 成功处理 (耗时: 3.0s)\n",
      "\n",
      "🔄 正在处理: JMaeen25/Mycollection\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "处理数据集:  85%|████████▌ | 86/101 [07:06<00:37,  2.47s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ JMaeen25/Mycollection: 成功处理 (耗时: 0.4s)\n",
      "\n",
      "🔄 正在处理: tugrul93/TaCarla\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "处理数据集:  86%|████████▌ | 87/101 [07:09<00:36,  2.58s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ tugrul93/TaCarla: 成功处理 (耗时: 2.8s)\n",
      "\n",
      "🔄 正在处理: cambrain/DigiFakeAV\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "处理数据集:  87%|████████▋ | 88/101 [07:10<00:28,  2.16s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ cambrain/DigiFakeAV: 成功处理 (耗时: 1.2s)\n",
      "\n",
      "🔄 正在处理: nasa-ibm-ai4science/ar_emergence\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "处理数据集:  88%|████████▊ | 89/101 [07:11<00:20,  1.73s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ nasa-ibm-ai4science/ar_emergence: 成功处理 (耗时: 0.7s)\n",
      "\n",
      "🔄 正在处理: Groovy-123/deep-think\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "处理数据集:  89%|████████▉ | 90/101 [07:11<00:14,  1.35s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Groovy-123/deep-think: 成功处理 (耗时: 0.5s)\n",
      "\n",
      "🔄 正在处理: hiepp2/tvp4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "处理数据集:  90%|█████████ | 91/101 [07:12<00:11,  1.16s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ hiepp2/tvp4: 成功处理 (耗时: 0.7s)\n",
      "\n",
      "🔄 正在处理: Groovy-123/Advance\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "处理数据集:  91%|█████████ | 92/101 [07:13<00:09,  1.06s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Groovy-123/Advance: 成功处理 (耗时: 0.8s)\n",
      "\n",
      "🔄 正在处理: nvidia/PhysicalAI-Autonomous-Vehicle-Cosmos-Drive-Dreams\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "处理数据集:  92%|█████████▏| 93/101 [07:46<01:25, 10.67s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ nvidia/PhysicalAI-Autonomous-Vehicle-Cosmos-Drive-Dreams: 成功处理 (耗时: 33.1s)\n",
      "\n",
      "🔄 正在处理: Prompthumanizer/jain_architecture\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "处理数据集:  93%|█████████▎| 94/101 [07:46<00:53,  7.61s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Prompthumanizer/jain_architecture: 成功处理 (耗时: 0.5s)\n",
      "\n",
      "🔄 正在处理: Mfonkown/Zark-Dataset_1.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "处理数据集:  94%|█████████▍| 95/101 [07:47<00:32,  5.48s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Mfonkown/Zark-Dataset_1.0: 成功处理 (耗时: 0.5s)\n",
      "\n",
      "🔄 正在处理: 1il7tw6n/makevideo\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "处理数据集:  95%|█████████▌| 96/101 [07:47<00:19,  3.98s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ 1il7tw6n/makevideo: 成功处理 (耗时: 0.5s)\n",
      "\n",
      "🔄 正在处理: JQL-AI/fw2_embeddings\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "处理数据集:  96%|█████████▌| 97/101 [07:48<00:12,  3.05s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ JQL-AI/fw2_embeddings: 成功处理 (耗时: 0.9s)\n",
      "\n",
      "🔄 正在处理: JQL-AI/hplt2_embeddings\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "处理数据集:  97%|█████████▋| 98/101 [07:49<00:07,  2.33s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ JQL-AI/hplt2_embeddings: 成功处理 (耗时: 0.7s)\n",
      "\n",
      "🔄 正在处理: PLM-Team/Pretrain-Dataset\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "处理数据集:  98%|█████████▊| 99/101 [07:50<00:04,  2.08s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ PLM-Team/Pretrain-Dataset: 成功处理 (耗时: 1.5s)\n",
      "\n",
      "🔄 正在处理: Groovy-123/QuantumAI\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "处理数据集:  99%|█████████▉| 100/101 [07:51<00:01,  1.63s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Groovy-123/QuantumAI: 成功处理 (耗时: 0.6s)\n",
      "\n",
      "🔄 正在处理: theairlabcmu/tartanair\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "处理数据集: 100%|██████████| 101/101 [07:52<00:00,  4.68s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ theairlabcmu/tartanair: 成功处理 (耗时: 0.8s)\n",
      "\n",
      "============================================================\n",
      "处理完成!\n",
      "✅ 成功处理: 101\n",
      "⏰ 超时跳过: 0\n",
      "❌ 处理失败: 0\n",
      "📊 总计: 101\n",
      "\n",
      "✅ result_df已更新\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# 优化版本：处理result_df，更新formats和size列（支持2分钟超时）\n",
    "print(\"开始处理数据集文件信息...\")\n",
    "\n",
    "# 设置处理数量限制（可以调整）\n",
    "process_count = len(result_df)\n",
    "\n",
    "# 创建副本以避免修改原始数据\n",
    "processed_df = result_df.copy()\n",
    "\n",
    "# 统计处理结果\n",
    "success_count = 0\n",
    "error_count = 0\n",
    "timeout_count = 0\n",
    "errors_log = []\n",
    "timeout_log = []\n",
    "\n",
    "# 只处理前几个数据集\n",
    "for idx in tqdm(range(process_count), desc=\"处理数据集\"):\n",
    "    row = processed_df.iloc[idx]\n",
    "    dataset_id = row['name']\n",
    "    \n",
    "    if pd.isna(dataset_id) or dataset_id == '':\n",
    "        continue\n",
    "    \n",
    "    try:\n",
    "        print(f\"\\n🔄 正在处理: {dataset_id}\")\n",
    "        start_time = time.time()\n",
    "        \n",
    "        # 获取文件信息（2分钟超时）\n",
    "        file_info = get_dataset_file_info(dataset_id, timeout_seconds=120)\n",
    "        \n",
    "        processing_time = time.time() - start_time\n",
    "        \n",
    "        if file_info['status'] == 'success':\n",
    "            # 更新formats列 - 将格式列表转换为字符串\n",
    "            if file_info['formats']:\n",
    "                processed_df.loc[processed_df.index[idx], 'formats'] = ', '.join(sorted(file_info['formats']))\n",
    "            else:\n",
    "                processed_df.loc[processed_df.index[idx], 'formats'] = None\n",
    "            \n",
    "            # 更新size列 - 格式化大小\n",
    "            if file_info['size']:\n",
    "                formatted_size = format_size(file_info['size'])\n",
    "                processed_df.loc[processed_df.index[idx], 'size'] = formatted_size\n",
    "            else:\n",
    "                processed_df.loc[processed_df.index[idx], 'size'] = None\n",
    "                \n",
    "            success_count += 1\n",
    "            print(f\"✅ {dataset_id}: 成功处理 (耗时: {processing_time:.1f}s)\")\n",
    "            \n",
    "        elif 'timeout' in file_info['status']:\n",
    "            timeout_count += 1\n",
    "            timeout_log.append(f\"{dataset_id}: {file_info['status']}\")\n",
    "            processed_df.loc[processed_df.index[idx], 'formats'] = None\n",
    "            processed_df.loc[processed_df.index[idx], 'size'] = None\n",
    "            print(f\"⏰ {dataset_id}: {file_info['status']}\")\n",
    "            \n",
    "        else:\n",
    "            error_count += 1\n",
    "            errors_log.append(f\"{dataset_id}: {file_info['status']}\")\n",
    "            processed_df.loc[processed_df.index[idx], 'formats'] = None\n",
    "            processed_df.loc[processed_df.index[idx], 'size'] = None\n",
    "            print(f\"❌ {dataset_id}: {file_info['status']}\")\n",
    "            \n",
    "    except Exception as e:\n",
    "        error_count += 1\n",
    "        error_msg = str(e)\n",
    "        errors_log.append(f\"{dataset_id}: {error_msg}\")\n",
    "        processed_df.loc[processed_df.index[idx], 'formats'] = None\n",
    "        processed_df.loc[processed_df.index[idx], 'size'] = None\n",
    "        print(f\"❌ {dataset_id}: {error_msg}\")\n",
    "\n",
    "print(f\"\\n{'='*60}\")\n",
    "print(\"处理完成!\")\n",
    "print(f\"✅ 成功处理: {success_count}\")\n",
    "print(f\"⏰ 超时跳过: {timeout_count}\")\n",
    "print(f\"❌ 处理失败: {error_count}\")\n",
    "print(f\"📊 总计: {success_count + timeout_count + error_count}\")\n",
    "\n",
    "if timeout_log:\n",
    "    print(f\"\\n⏰ 超时列表:\")\n",
    "    for timeout in timeout_log:\n",
    "        print(f\"  - {timeout}\")\n",
    "\n",
    "if errors_log:\n",
    "    print(f\"\\n❌ 错误列表:\")\n",
    "    for error in errors_log:\n",
    "        print(f\"  - {error}\")\n",
    "\n",
    "# 更新原始的result_df\n",
    "result_df = processed_df.copy()\n",
    "print(f\"\\n✅ result_df已更新\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de75bfd6",
   "metadata": {},
   "source": [
    "数据清洗：模态"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "e5ad283a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "开始数据清洗...\n",
      "============================================================\n",
      "1. 处理modalities列...\n",
      "modalities清洗结果:\n",
      "  其他: 3\n",
      "  多模态: 1\n"
     ]
    }
   ],
   "source": [
    "import ast\n",
    "import re\n",
    "\n",
    "print(\"开始数据清洗...\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# 1. 清洗modalities列\n",
    "print(\"1. 处理modalities列...\")\n",
    "\n",
    "def clean_modalities(modalities_str):\n",
    "    \"\"\"\n",
    "    清洗并分类modalities\n",
    "    Text、Tabular -> 文本\n",
    "    Image -> 图片\n",
    "    Video -> 视频\n",
    "    多个不同类型 -> 多模态\n",
    "    \"\"\"\n",
    "    if pd.isna(modalities_str) or modalities_str == '' or modalities_str == 'None':\n",
    "        return None\n",
    "    \n",
    "    try:\n",
    "        # 尝试解析字符串为列表\n",
    "        if isinstance(modalities_str, str):\n",
    "            # 去除外层引号和空格\n",
    "            modalities_str = modalities_str.strip().strip(\"'\\\"\")\n",
    "            \n",
    "            # 如果是字符串形式的列表，解析它\n",
    "            if modalities_str.startswith('[') and modalities_str.endswith(']'):\n",
    "                modalities_list = ast.literal_eval(modalities_str)\n",
    "            else:\n",
    "                # 如果不是列表格式，按逗号分割\n",
    "                modalities_list = [item.strip().strip(\"'\\\"\") for item in modalities_str.split(',')]\n",
    "        else:\n",
    "            modalities_list = [modalities_str]\n",
    "            \n",
    "        # 去除空值和None\n",
    "        modalities_list = [item for item in modalities_list if item and item != 'None' and item.strip()]\n",
    "        \n",
    "        if not modalities_list:\n",
    "            return None\n",
    "            \n",
    "        # 转换为小写进行比较\n",
    "        modalities_lower = [item.lower() for item in modalities_list]\n",
    "        \n",
    "        # 分类逻辑\n",
    "        has_text = any(mod in ['text', 'tabular'] for mod in modalities_lower)\n",
    "        has_image = any(mod in ['image'] for mod in modalities_lower)\n",
    "        has_video = any(mod in ['video'] for mod in modalities_lower)\n",
    "        \n",
    "        # 计算不同类型的数量\n",
    "        type_count = sum([has_text, has_image, has_video])\n",
    "        \n",
    "        if type_count > 1:\n",
    "            return '多模态'\n",
    "        elif has_text:\n",
    "            return '文本'\n",
    "        elif has_image:\n",
    "            return '图片'\n",
    "        elif has_video:\n",
    "            return '视频'\n",
    "        else:\n",
    "            # 其他未知类型\n",
    "            return '其他'\n",
    "            \n",
    "    except Exception as e:\n",
    "        print(f\"处理modalities时出错: {modalities_str}, 错误: {e}\")\n",
    "        return None\n",
    "\n",
    "# 应用modalities清洗\n",
    "result_df['modalities'] = result_df['modalities'].apply(clean_modalities)\n",
    "\n",
    "# 统计modalities清洗结果\n",
    "modalities_counts = result_df['modalities'].value_counts()\n",
    "print(\"modalities清洗结果:\")\n",
    "for category, count in modalities_counts.items():\n",
    "    print(f\"  {category}: {count}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4bb0caec",
   "metadata": {},
   "source": [
    "数据清洗：学科"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "4e9df652",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "3. 处理tags列并创建学科分类...\n",
      "应用tags清洗和学科分类...\n",
      "\n",
      "学科分类结果:\n",
      "  生命科学: 12\n",
      "  物质科学: 4\n",
      "  大气海洋: 2\n"
     ]
    }
   ],
   "source": [
    "# 3. 处理tags列并创建学科分类\n",
    "print(\"\\n3. 处理tags列并创建学科分类...\")\n",
    "\n",
    "def classify_subject(tags_str):\n",
    "    \"\"\"\n",
    "    根据tags创建学科分类：\n",
    "    1. 如果包含biology或medical -> '生命科学'\n",
    "    2. 如果包含chemistry -> '物质科学'  \n",
    "    3. 如果包含climate -> '大气海洋'\n",
    "    4. 其他 -> None\n",
    "    \"\"\"\n",
    "    if pd.isna(tags_str) or tags_str == '' or tags_str == 'None':\n",
    "        return None\n",
    "    \n",
    "    try:\n",
    "        # 转换为字符串并转为小写以便比较\n",
    "        tags_str_lower = str(tags_str).lower()\n",
    "        \n",
    "        # 检查是否包含生命科学相关标签\n",
    "        if any(keyword in tags_str_lower for keyword in ['biology', 'medical', 'bio-', 'medicine', 'health', 'genomics', 'protein']):\n",
    "            return '生命科学'\n",
    "        \n",
    "        # 检查是否包含物质科学相关标签\n",
    "        elif any(keyword in tags_str_lower for keyword in ['chemistry', 'chemical', 'molecule', 'compound', 'material']):\n",
    "            return '物质科学'\n",
    "        \n",
    "        # 检查是否包含大气海洋相关标签\n",
    "        elif any(keyword in tags_str_lower for keyword in ['climate', 'weather', 'ocean', 'atmospheric', 'meteorology', 'environmental']):\n",
    "            return '大气海洋'\n",
    "        \n",
    "        # 其他情况返回None\n",
    "        else:\n",
    "            return None\n",
    "            \n",
    "    except Exception as e:\n",
    "        print(f\"处理tags时出错: {tags_str}, 错误: {e}\")\n",
    "        return None\n",
    "\n",
    "def clean_tags(tags_str):\n",
    "    \"\"\"\n",
    "    清洗tags列，去除JSON格式的[]和''，只保留单词\n",
    "    \"\"\"\n",
    "    if pd.isna(tags_str) or tags_str == '' or tags_str == 'None':\n",
    "        return None\n",
    "    \n",
    "    try:\n",
    "        # 转换为字符串\n",
    "        tags_str = str(tags_str).strip()\n",
    "        \n",
    "        # 去除外层引号\n",
    "        tags_str = tags_str.strip(\"'\\\"\")\n",
    "        \n",
    "        # 如果是列表格式，尝试解析\n",
    "        if tags_str.startswith('[') and tags_str.endswith(']'):\n",
    "            try:\n",
    "                tags_list = ast.literal_eval(tags_str)\n",
    "                if isinstance(tags_list, list):\n",
    "                    # 清理列表中的每个标签\n",
    "                    clean_list = []\n",
    "                    for tag in tags_list:\n",
    "                        tag = str(tag).strip().strip(\"'\\\"\")\n",
    "                        if tag and tag.lower() != 'none':\n",
    "                            clean_list.append(tag)\n",
    "                    return ', '.join(clean_list)\n",
    "            except:\n",
    "                # 如果解析失败，直接去除括号和引号\n",
    "                tags_str = tags_str.strip('[]')\n",
    "        \n",
    "        # 去除各种引号\n",
    "        tags_str = re.sub(r\"['\\\"]\", '', tags_str)\n",
    "        \n",
    "        # 分割并清理\n",
    "        if ',' in tags_str:\n",
    "            tags_parts = [part.strip() for part in tags_str.split(',')]\n",
    "        else:\n",
    "            tags_parts = [tags_str.strip()]\n",
    "        \n",
    "        # 过滤并清理标签\n",
    "        valid_tags = []\n",
    "        for tag in tags_parts:\n",
    "            tag = tag.strip()\n",
    "            if tag and tag.lower() not in ['none', 'null', '']:\n",
    "                # 移除特殊字符，但保留连字符和下划线\n",
    "                clean_tag = re.sub(r'[^\\w\\-]', ' ', tag).strip()\n",
    "                clean_tag = re.sub(r'\\s+', '-', clean_tag)  # 将空格替换为连字符\n",
    "                if clean_tag:\n",
    "                    valid_tags.append(clean_tag.lower())\n",
    "        \n",
    "        # 去重并排序\n",
    "        valid_tags = sorted(list(set(valid_tags)))\n",
    "        \n",
    "        if valid_tags:\n",
    "            return ', '.join(valid_tags)\n",
    "        else:\n",
    "            return None\n",
    "            \n",
    "    except Exception as e:\n",
    "        print(f\"处理tags时出错: {tags_str}, 错误: {e}\")\n",
    "        return None\n",
    "\n",
    "# 应用tags清洗和学科分类\n",
    "print(\"应用tags清洗和学科分类...\")\n",
    "result_df['tags_cleaned'] = result_df['tags'].apply(clean_tags)\n",
    "result_df['学科'] = result_df['tags'].apply(classify_subject)\n",
    "\n",
    "# 统计学科分类结果\n",
    "print(\"\\n学科分类结果:\")\n",
    "subject_counts = result_df['学科'].value_counts()\n",
    "for subject, count in subject_counts.items():\n",
    "    print(f\"  {subject}: {count}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "7fbb8b3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "license_df=pd.read_excel('规范协议名称.xlsx')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "472678f2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "协议名称规范化函数已创建\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "from difflib import get_close_matches\n",
    "\n",
    "def normalize_license(license_name, standard_licenses):\n",
    "    \"\"\"\n",
    "    规范化协议名称\n",
    "    \"\"\"\n",
    "    if pd.isna(license_name) or license_name == '':\n",
    "        return license_name\n",
    "    \n",
    "    # 转换为字符串并去除首尾空格\n",
    "    license_str = str(license_name).strip()\n",
    "    \n",
    "    # 如果已经是标准格式，直接返回\n",
    "    if license_str in standard_licenses:\n",
    "        return license_str\n",
    "    \n",
    "    # 处理常见的大小写和格式问题\n",
    "    license_upper = license_str.upper()\n",
    "    \n",
    "    # 创建映射字典处理常见的变体\n",
    "    mapping = {\n",
    "        'APACHE-2.0': 'Apache-2.0',\n",
    "        'APACHE 2.0': 'Apache-2.0',\n",
    "        'APACHE2.0': 'Apache-2.0',\n",
    "        'APACHE': 'Apache-2.0',\n",
    "        'MIT': 'MIT',\n",
    "        'MIT LICENSE': 'MIT',\n",
    "        'CC-BY-4.0': 'CC-BY-4.0',\n",
    "        'CC BY 4.0': 'CC-BY-4.0',\n",
    "        'CC-BY-SA-4.0': 'CC-BY-SA-4.0',\n",
    "        'CC BY SA 4.0': 'CC-BY-SA-4.0',\n",
    "        'CC-BY-NC-SA-4.0': 'CC-BY-NC-SA-4.0',\n",
    "        'CC BY NC SA 4.0': 'CC-BY-NC-SA-4.0',\n",
    "        'CC0-1.0': 'CC0-1.0',\n",
    "        'CC0 1.0': 'CC0-1.0',\n",
    "        'BSD-3-CLAUSE': 'BSD-3-Clause',\n",
    "        'BSD 3 CLAUSE': 'BSD-3-Clause',\n",
    "        'BSD-2-CLAUSE': 'BSD-2-Clause',\n",
    "        'BSD 2 CLAUSE': 'BSD-2-Clause',\n",
    "        'GPL-3.0': 'GPL-3.0',\n",
    "        'GPL V3': 'GPL-3.0',\n",
    "        'LGPL-3.0': 'LGPL-3.0',\n",
    "        'AGPL-3.0': 'AGPL-3.0',\n",
    "        'UNLICENSE': 'Unlicense',\n",
    "        '公开': '公开',\n",
    "        'PUBLIC': '公开',\n",
    "        'OPENRAIL': 'OPENRAIL',\n",
    "        'CUSTOM': 'CUSTOM'\n",
    "    }\n",
    "    \n",
    "    # 先检查精确映射\n",
    "    if license_upper in mapping:\n",
    "        return mapping[license_upper]\n",
    "    \n",
    "    # 使用模糊匹配找到最相近的标准协议\n",
    "    # 首先尝试在标准协议列表中找到最相似的\n",
    "    close_matches = get_close_matches(license_str, standard_licenses, n=1, cutoff=0.8)\n",
    "    if close_matches:\n",
    "        return close_matches[0]\n",
    "    \n",
    "    # 如果没找到匹配，尝试大小写不敏感的匹配\n",
    "    for std_license in standard_licenses:\n",
    "        if license_upper == std_license.upper():\n",
    "            return std_license\n",
    "    \n",
    "    # 如果仍然没找到，返回原值\n",
    "    return license_str\n",
    "\n",
    "# 获取标准协议列表\n",
    "standard_licenses_list = license_df['开源协议'].tolist()\n",
    "\n",
    "print(\"协议名称规范化函数已创建\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "906bc00d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "正在规范化 models_df 的协议名称...\n",
      "Models DataFrame 协议规范化对比:\n",
      "共有 87 条记录发生了变化:\n",
      "                原始协议            规范化协议\n",
      "1                mit              MIT\n",
      "2         apache-2.0       Apache-2.0\n",
      "4          cc-by-4.0        CC-BY-4.0\n",
      "6       cc-by-sa-4.0     CC-BY-SA-4.0\n",
      "7           agpl-3.0         AGPL-3.0\n",
      "9            cc0-1.0          CC0-1.0\n",
      "11              None             None\n",
      "15          openrail         OPENRAIL\n",
      "20           afl-3.0          AFL-3.0\n",
      "40      cc-by-nc-4.0     CC-BY-NC-4.0\n",
      "60   cc-by-nc-sa-4.0  CC-BY-NC-SA-4.0\n",
      "68   CC BY-NC-SA 4.0  CC-BY-NC-SA-4.0\n",
      "80             wtfpl            WTFPL\n",
      "100     bsd-3-clause     BSD-3-Clause\n",
      "\n",
      "规范化后的协议唯一值统计:\n",
      "开源协议\n",
      "Apache-2.0                            23\n",
      "MIT                                   12\n",
      "odc-by                                 9\n",
      "CC0-1.0                                9\n",
      "CC-BY-4.0                              6\n",
      "CC-BY-NC-SA-4.0                        4\n",
      "AGPL-3.0                               3\n",
      "CC-BY-SA-4.0                           2\n",
      "pddl                                   1\n",
      "nexgen                                 1\n",
      "WTFPL                                  1\n",
      "intel-research-development-license     1\n",
      "AFL-3.0                                1\n",
      "cc                                     1\n",
      "CC-BY-NC-4.0                           1\n",
      "OPENRAIL                               1\n",
      "gpl                                    1\n",
      "BSD-3-Clause                           1\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# 对models_df进行协议名称规范化\n",
    "print(\"正在规范化 models_df 的协议名称...\")\n",
    "\n",
    "# 创建备份\n",
    "models_df_backup = result_df.copy()\n",
    "\n",
    "# 应用规范化函数\n",
    "result_df['开源协议'] = result_df['license'].apply(\n",
    "    lambda x: normalize_license(x, standard_licenses_list)\n",
    ")\n",
    "\n",
    "# 显示规范化前后的对比\n",
    "print(\"Models DataFrame 协议规范化对比:\")\n",
    "comparison_models = pd.DataFrame({\n",
    "    '原始协议': result_df['license'],\n",
    "    '规范化协议': result_df['开源协议']\n",
    "})\n",
    "comparison_models = pd.DataFrame({\n",
    "    '原始协议': result_df['license'],\n",
    "    '规范化协议': result_df['开源协议']\n",
    "})\n",
    "\n",
    "# 显示发生变化的记录\n",
    "changed_models = comparison_models[comparison_models['原始协议'] != comparison_models['规范化协议']]\n",
    "if len(changed_models) > 0:\n",
    "    print(f\"共有 {len(changed_models)} 条记录发生了变化:\")\n",
    "    print(changed_models.drop_duplicates().head(20))\n",
    "else:\n",
    "    print(\"没有记录发生变化\")\n",
    "\n",
    "# 显示规范化后的唯一值统计\n",
    "print(f\"\\n规范化后的协议唯一值统计:\")\n",
    "print(result_df['开源协议'].value_counts().head(20))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ebf5ff88",
   "metadata": {},
   "source": [
    "识别企业与机构"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "fc582f80",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "严格识别后的AI公司/机构账号数量: 4\n",
      "识别比例: 4.0%\n",
      "\n",
      "严格识别后的AI公司/机构分布:\n",
      "识别出的AI公司\n",
      "Allen Institute for AI    2\n",
      "TorchGeo                  1\n",
      "NVIDIA                    1\n",
      "Name: count, dtype: int64\n",
      "\n",
      "新的具体映射例子:\n",
      "  allenai -> Allen Institute for AI\n",
      "  torchgeo -> TorchGeo\n",
      "  nvidia -> NVIDIA\n"
     ]
    }
   ],
   "source": [
    "# 重新设计更严格的AI公司识别逻辑\n",
    "def create_strict_ai_company_mapping():\n",
    "    \"\"\"\n",
    "    创建更严格的AI公司映射，只包含确认的知名公司和机构官方账号\n",
    "    \"\"\"\n",
    "    strict_mapping = {\n",
    "        # 大型科技公司（精确匹配官方账号）\n",
    "        'microsoft': 'Microsoft',\n",
    "        'meta': 'Meta',\n",
    "        'facebook': 'Meta',\n",
    "        'google': 'Google',\n",
    "        'googleai': 'Google',\n",
    "        'google-research': 'Google',\n",
    "        'google-deepmind': 'Google DeepMind',\n",
    "        'deepmind': 'Google DeepMind',\n",
    "        'openai': 'OpenAI',\n",
    "        'anthropic': 'Anthropic',\n",
    "        'apple': 'Apple',\n",
    "        'nvidia': 'NVIDIA',\n",
    "        'amazon': 'Amazon',\n",
    "        'aws': 'Amazon',\n",
    "        'ibm': 'IBM',\n",
    "        'ibm-nasa-geospatial': 'IBM',\n",
    "        'ibm-research': 'IBM',\n",
    "        'salesforce': 'Salesforce',\n",
    "        'adobe': 'Adobe',\n",
    "        'intel': 'Intel',\n",
    "        \n",
    "        # AI公司和研究机构（精确匹配）\n",
    "        'huggingface': 'Hugging Face',\n",
    "        'eleutherai': 'EleutherAI',\n",
    "        'stabilityai': 'Stability AI',\n",
    "        'cohere': 'Cohere',\n",
    "        'coherelabs': 'Cohere',\n",
    "        'ai21labs': 'AI21 Labs',\n",
    "        'allenai': 'Allen Institute for AI',\n",
    "        \n",
    "        # 生物技术公司\n",
    "        'basf-ai': 'BASF',\n",
    "        'basf': 'BASF',\n",
    "        \n",
    "        # 研究机构和开源项目（只包含明确的官方账号）\n",
    "        'openclimatefix': 'Open Climate Fix',\n",
    "        'torchgeo': 'TorchGeo',\n",
    "        'imageomics': 'Imageomics',\n",
    "        'biomap-research': 'BioMap Research',\n",
    "        'proteinglm': 'ProteinGLM',\n",
    "        'autobio-bench': 'AutoBio-Bench',\n",
    "        'freedomintelligence': 'FreedomIntelligence',\n",
    "        'hpai-bsc': 'HPAI-BSC',\n",
    "        'scikit-fingerprints': 'Scikit-Fingerprints',\n",
    "        'oxai4science': 'OxAI4Science',\n",
    "        'opendatalab': 'OpenDataLab',\n",
    "        \n",
    "        # 知名大学（只包含明确的官方机构账号，不包含个人用户名）\n",
    "        'stanford-nlp': 'Stanford University',\n",
    "        'stanfordaimlab': 'Stanford University',\n",
    "        'stanfordaimi': 'Stanford University',\n",
    "        'mit-csail': 'MIT',\n",
    "        'fair-forward': 'Meta FAIR',\n",
    "        'epfl-eceo': 'EPFL',\n",
    "        'jingwei-sjtu': 'Shanghai Jiao Tong University',\n",
    "        'aim-harvard': 'Harvard University',\n",
    "        \n",
    "        # 中国知名公司\n",
    "        'alibaba': 'Alibaba',\n",
    "        'tencent': 'Tencent',\n",
    "        'baidu': 'Baidu',\n",
    "        'bytedance': 'ByteDance',\n",
    "        'xiaomi': 'Xiaomi',\n",
    "        'sensetime': 'SenseTime',\n",
    "        'opendatalab': 'OpenDataLab',\n",
    "\n",
    "        # 数据服务公司\n",
    "        'trainingdatapro': 'TrainingDataPro',\n",
    "        'datatonic': 'DataTonic',\n",
    "    }\n",
    "    return strict_mapping\n",
    "\n",
    "def strict_identify_ai_company(author_name):\n",
    "    \"\"\"\n",
    "    更严格的AI公司识别函数，只识别明确的官方账号\n",
    "    \"\"\"\n",
    "    if pd.isna(author_name) or author_name == '':\n",
    "        return None\n",
    "    \n",
    "    author_lower = str(author_name).lower().strip()\n",
    "    strict_mapping = create_strict_ai_company_mapping()\n",
    "    \n",
    "    # 只进行精确匹配，绝不进行模糊匹配或包含匹配\n",
    "    if author_lower in strict_mapping:\n",
    "        return strict_mapping[author_lower]\n",
    "    \n",
    "    return None\n",
    "\n",
    "\n",
    "# 使用新的严格识别逻辑\n",
    "result_df['识别出的AI公司'] = result_df['author'].apply(strict_identify_ai_company)\n",
    "\n",
    "# 显示新的识别结果\n",
    "new_identified = result_df[result_df['识别出的AI公司'].notna()]\n",
    "print(f\"\\n严格识别后的AI公司/机构账号数量: {len(new_identified)}\")\n",
    "print(f\"识别比例: {len(new_identified)/len(result_df)*100:.1f}%\")\n",
    "\n",
    "print(f\"\\n严格识别后的AI公司/机构分布:\")\n",
    "new_company_counts = new_identified['识别出的AI公司'].value_counts()\n",
    "print(new_company_counts)\n",
    "\n",
    "print(f\"\\n新的具体映射例子:\")\n",
    "new_examples = new_identified[['author', '识别出的AI公司']].drop_duplicates()\n",
    "for _, row in new_examples.iterrows():\n",
    "    print(f\"  {row['author']} -> {row['识别出的AI公司']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "b8a28875",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "创建'数据原始发布机构'列完成！\n",
      "数据形状: (101, 19)\n",
      "\n",
      "各数据原始发布机构的分布:\n",
      "数据原始发布机构\n",
      "Huggingface               97\n",
      "Allen Institute for AI     2\n",
      "TorchGeo                   1\n",
      "NVIDIA                     1\n",
      "Name: count, dtype: int64\n",
      "\n",
      "前5行数据预览:\n",
      "                 识别出的AI公司                数据原始发布机构\n",
      "0  Allen Institute for AI  Allen Institute for AI\n",
      "1                    None             Huggingface\n",
      "2                    None             Huggingface\n",
      "3  Allen Institute for AI  Allen Institute for AI\n",
      "4                    None             Huggingface\n"
     ]
    }
   ],
   "source": [
    "# 创建\"数据原始发布机构\"列\n",
    "# 如果\"识别出的AI公司\"列为空或NaN，则标注为\"Huggingface\"，否则使用\"识别出的AI公司\"的值\n",
    "result_df['数据原始发布机构'] = result_df['识别出的AI公司'].fillna('Huggingface')\n",
    "\n",
    "# 处理空字符串的情况\n",
    "result_df['数据原始发布机构'] = result_df['数据原始发布机构'].replace('', 'Huggingface')\n",
    "\n",
    "# 显示结果\n",
    "print(\"创建'数据原始发布机构'列完成！\")\n",
    "print(f\"数据形状: {result_df.shape}\")\n",
    "print(\"\\n各数据原始发布机构的分布:\")\n",
    "print(result_df['数据原始发布机构'].value_counts())\n",
    "print(\"\\n前5行数据预览:\")\n",
    "print(result_df[['识别出的AI公司', '数据原始发布机构']].head())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d0f96ac",
   "metadata": {},
   "source": [
    "# 调用AI翻译英文简介"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "b3fea6c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = \"\"\"请你学习这个关于数据集的介绍，并用简洁的中文对进行总结，说明这个数据集的内容和用途。以JSON格式输出，严格遵循如下格式：```json{\"介绍\":\"你的总结的内容\"}``` \"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "4be531df",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "import json\n",
    "import time\n",
    "import logging\n",
    "from datetime import datetime, timedelta\n",
    "from concurrent.futures import ThreadPoolExecutor\n",
    "from tqdm import tqdm\n",
    "from openai import OpenAI\n",
    "\n",
    "# -------------------------------\n",
    "# JSON解析模块（独立模块）\n",
    "# -------------------------------\n",
    "def default_json_parser(content, idx=None):\n",
    "    \"\"\"\n",
    "    默认的 JSON 解析器：\n",
    "    清理输入内容后尝试解析 JSON，\n",
    "    若成功则返回完整的字典，若失败返回空字典。\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # 去除代码块标记，清理内容\n",
    "        cleaned_content = content.replace('```json\\n', '').replace('```', '').strip()\n",
    "        parsed_result = json.loads(cleaned_content)\n",
    "        return parsed_result\n",
    "    except json.JSONDecodeError:\n",
    "        if idx is not None:\n",
    "            logging.warning(f\"警告: 第 {idx} 行解析 JSON 失败\")\n",
    "        return {}\n",
    "    except Exception as e:\n",
    "        if idx is not None:\n",
    "            logging.error(f\"错误: 第 {idx} 行解析失败 - {str(e)}\")\n",
    "        return {}\n",
    "\n",
    "# -------------------------------\n",
    "# 限流处理器（控制请求频率）\n",
    "# -------------------------------\n",
    "class RateLimitedProcessor:\n",
    "    def __init__(self):\n",
    "        self.request_timestamps = []\n",
    "        self.MAX_RPM = 500\n",
    "        self.window_size = 60  # 60秒窗口\n",
    "\n",
    "    def _clean_old_records(self, current_time):\n",
    "        cutoff_time = current_time - timedelta(seconds=self.window_size)\n",
    "        self.request_timestamps = [ts for ts in self.request_timestamps if ts > cutoff_time]\n",
    "\n",
    "    def can_make_request(self):\n",
    "        \"\"\"检查是否可以发起新请求\"\"\"\n",
    "        current_time = datetime.now()\n",
    "        self._clean_old_records(current_time)\n",
    "        if len(self.request_timestamps) >= self.MAX_RPM:\n",
    "            return False\n",
    "        self.request_timestamps.append(current_time)\n",
    "        return True\n",
    "\n",
    "# -------------------------------\n",
    "# OpenAI文本处理器\n",
    "# -------------------------------\n",
    "class OpenAITextProcessor:\n",
    "    def __init__(self, api_key=None, model=None, base_url=None, json_parser=None):\n",
    "        self.client = OpenAI(api_key=api_key,base_url=base_url)\n",
    "        self.model = model\n",
    "        self.rate_limiter = RateLimitedProcessor()\n",
    "        self.n_workers = 14  # 优化后的线程数\n",
    "        # 如果未提供自定义解析器，则使用默认解析器\n",
    "        self.json_parser = json_parser if json_parser is not None else default_json_parser\n",
    "\n",
    "    def process_batch(self, df, text_column, prompt, batch_size=20, delay=1, json_parser=None):\n",
    "        \"\"\"\n",
    "        批量处理文本，支持灵活的 JSON 解析。\n",
    "        \n",
    "        参数:\n",
    "            df: 包含文本数据的 DataFrame\n",
    "            text_column: 文本所在的列名\n",
    "            prompt: 系统提示，用于 API 调用\n",
    "            batch_size: 每个批次处理的文本条数\n",
    "            delay: 每次请求后的延迟（秒）\n",
    "            json_parser: 可选的自定义 JSON 解析器，若不传入则使用实例内的解析器\n",
    "        \n",
    "        返回:\n",
    "            新的 DataFrame，包含原始数据及 API 返回结果（通过 JSON 解析获得的各字段）\n",
    "        \"\"\"\n",
    "        parser = json_parser if json_parser is not None else self.json_parser\n",
    "        results = []  # 保存每次请求解析后的结果（字典形式）\n",
    "\n",
    "        def process_chunk(chunk_data):\n",
    "            chunk_results = []\n",
    "            for idx, text in chunk_data:\n",
    "                # 限流检测：等待直到可以发送请求\n",
    "                while not self.rate_limiter.can_make_request():\n",
    "                    time.sleep(0.1)\n",
    "                try:\n",
    "                    response = self.client.chat.completions.create(\n",
    "                        model=self.model,\n",
    "                        messages=[\n",
    "                            {\"role\": \"system\", \"content\": prompt},\n",
    "                            {\"role\": \"user\", \"content\": text}\n",
    "                        ],\n",
    "                        temperature=1,\n",
    "                        max_tokens=500\n",
    "                    )\n",
    "                    # 使用解析器处理响应内容，得到字典格式结果\n",
    "                    parsed_result = parser(response.choices[0].message.content, idx)\n",
    "                    chunk_results.append(parsed_result)\n",
    "                    time.sleep(delay)\n",
    "                except Exception as e:\n",
    "                    logging.error(f\"错误: 处理第 {idx} 行时发生异常: {str(e)}\")\n",
    "                    chunk_results.append({})\n",
    "            return chunk_results\n",
    "\n",
    "        # 将数据分成批次，保留行号信息\n",
    "        chunks = [\n",
    "            list(enumerate(df[text_column][i:i+batch_size]))\n",
    "            for i in range(0, len(df), batch_size)\n",
    "        ]\n",
    "\n",
    "        with ThreadPoolExecutor(max_workers=self.n_workers) as executor:\n",
    "            futures = list(tqdm(\n",
    "                executor.map(process_chunk, chunks),\n",
    "                total=len(chunks),\n",
    "                desc=\"Processing batches\"\n",
    "            ))\n",
    "            for chunk_results in futures:\n",
    "                results.extend(chunk_results)\n",
    "\n",
    "        # 将解析结果列表转为 DataFrame，并与原 DataFrame 合并\n",
    "        df_result = df.copy().reset_index(drop=True)\n",
    "        results_df = pd.json_normalize(results)\n",
    "        df_result = pd.concat([df_result, results_df], axis=1)\n",
    "\n",
    "        # 统计处理情况\n",
    "        success_count = sum(1 for r in results if r)\n",
    "        total_count = len(results)\n",
    "        success_rate = (success_count / total_count) * 100 if total_count > 0 else 0\n",
    "        logging.info(f\"处理完成: 总数 {total_count}, 成功 {success_count}, 成功率 {success_rate:.2f}%\")\n",
    "        \n",
    "        return df_result\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df749f16",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing batches:   0%|          | 0/21 [00:00<?, ?it/s]2025-08-26 11:48:02,503 - INFO - HTTP Request: POST https://api.deepseek.com/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-08-26 11:48:02,537 - INFO - HTTP Request: POST https://api.deepseek.com/chat/completions \"HTTP/1.1 422 Unprocessable Entity\"\n",
      "2025-08-26 11:48:02,538 - ERROR - 错误: 处理第 0 行时发生异常: Failed to deserialize the JSON body into the target type: messages[1]: data did not match any variant of untagged enum ChatCompletionRequestContent at line 1 column 299\n",
      "2025-08-26 11:48:02,542 - INFO - HTTP Request: POST https://api.deepseek.com/chat/completions \"HTTP/1.1 422 Unprocessable Entity\"\n",
      "2025-08-26 11:48:02,543 - INFO - HTTP Request: POST https://api.deepseek.com/chat/completions \"HTTP/1.1 422 Unprocessable Entity\"\n",
      "2025-08-26 11:48:02,544 - INFO - HTTP Request: POST https://api.deepseek.com/chat/completions \"HTTP/1.1 422 Unprocessable Entity\"\n",
      "2025-08-26 11:48:02,544 - INFO - HTTP Request: POST https://api.deepseek.com/chat/completions \"HTTP/1.1 422 Unprocessable Entity\"\n",
      "2025-08-26 11:48:02,545 - ERROR - 错误: 处理第 0 行时发生异常: Failed to deserialize the JSON body into the target type: messages[1]: data did not match any variant of untagged enum ChatCompletionRequestContent at line 1 column 299\n",
      "2025-08-26 11:48:02,547 - ERROR - 错误: 处理第 0 行时发生异常: Failed to deserialize the JSON body into the target type: messages[1]: data did not match any variant of untagged enum ChatCompletionRequestContent at line 1 column 299\n",
      "2025-08-26 11:48:02,549 - ERROR - 错误: 处理第 0 行时发生异常: Failed to deserialize the JSON body into the target type: messages[1]: data did not match any variant of untagged enum ChatCompletionRequestContent at line 1 column 299\n",
      "2025-08-26 11:48:02,551 - ERROR - 错误: 处理第 0 行时发生异常: Failed to deserialize the JSON body into the target type: messages[1]: data did not match any variant of untagged enum ChatCompletionRequestContent at line 1 column 299\n",
      "2025-08-26 11:48:02,555 - INFO - HTTP Request: POST https://api.deepseek.com/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-08-26 11:48:02,560 - INFO - HTTP Request: POST https://api.deepseek.com/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-08-26 11:48:02,561 - INFO - HTTP Request: POST https://api.deepseek.com/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-08-26 11:48:02,563 - INFO - HTTP Request: POST https://api.deepseek.com/chat/completions \"HTTP/1.1 422 Unprocessable Entity\"\n",
      "2025-08-26 11:48:02,564 - ERROR - 错误: 处理第 0 行时发生异常: Failed to deserialize the JSON body into the target type: messages[1]: data did not match any variant of untagged enum ChatCompletionRequestContent at line 1 column 299\n",
      "2025-08-26 11:48:02,568 - INFO - HTTP Request: POST https://api.deepseek.com/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-08-26 11:48:02,609 - INFO - HTTP Request: POST https://api.deepseek.com/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-08-26 11:48:02,652 - INFO - HTTP Request: POST https://api.deepseek.com/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-08-26 11:48:02,673 - INFO - HTTP Request: POST https://api.deepseek.com/chat/completions \"HTTP/1.1 422 Unprocessable Entity\"\n",
      "2025-08-26 11:48:02,675 - ERROR - 错误: 处理第 1 行时发生异常: Failed to deserialize the JSON body into the target type: messages[1]: data did not match any variant of untagged enum ChatCompletionRequestContent at line 1 column 299\n",
      "2025-08-26 11:48:02,678 - INFO - HTTP Request: POST https://api.deepseek.com/chat/completions \"HTTP/1.1 422 Unprocessable Entity\"\n",
      "2025-08-26 11:48:02,680 - ERROR - 错误: 处理第 1 行时发生异常: Failed to deserialize the JSON body into the target type: messages[1]: data did not match any variant of untagged enum ChatCompletionRequestContent at line 1 column 299\n",
      "2025-08-26 11:48:02,683 - INFO - HTTP Request: POST https://api.deepseek.com/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-08-26 11:48:02,812 - INFO - HTTP Request: POST https://api.deepseek.com/chat/completions \"HTTP/1.1 422 Unprocessable Entity\"\n",
      "2025-08-26 11:48:02,813 - INFO - HTTP Request: POST https://api.deepseek.com/chat/completions \"HTTP/1.1 422 Unprocessable Entity\"\n",
      "2025-08-26 11:48:02,814 - ERROR - 错误: 处理第 2 行时发生异常: Failed to deserialize the JSON body into the target type: messages[1]: data did not match any variant of untagged enum ChatCompletionRequestContent at line 1 column 299\n",
      "2025-08-26 11:48:02,815 - ERROR - 错误: 处理第 2 行时发生异常: Failed to deserialize the JSON body into the target type: messages[1]: data did not match any variant of untagged enum ChatCompletionRequestContent at line 1 column 299\n",
      "2025-08-26 11:48:02,895 - INFO - HTTP Request: POST https://api.deepseek.com/chat/completions \"HTTP/1.1 422 Unprocessable Entity\"\n",
      "2025-08-26 11:48:02,899 - ERROR - 错误: 处理第 3 行时发生异常: Failed to deserialize the JSON body into the target type: messages[1]: data did not match any variant of untagged enum ChatCompletionRequestContent at line 1 column 299\n",
      "2025-08-26 11:48:02,960 - INFO - HTTP Request: POST https://api.deepseek.com/chat/completions \"HTTP/1.1 422 Unprocessable Entity\"\n",
      "2025-08-26 11:48:02,968 - ERROR - 错误: 处理第 3 行时发生异常: Failed to deserialize the JSON body into the target type: messages[1]: data did not match any variant of untagged enum ChatCompletionRequestContent at line 1 column 299\n",
      "2025-08-26 11:48:02,977 - INFO - HTTP Request: POST https://api.deepseek.com/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-08-26 11:48:03,060 - INFO - HTTP Request: POST https://api.deepseek.com/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-08-26 11:48:03,069 - INFO - HTTP Request: POST https://api.deepseek.com/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-08-26 11:48:03,074 - INFO - HTTP Request: POST https://api.deepseek.com/chat/completions \"HTTP/1.1 422 Unprocessable Entity\"\n",
      "2025-08-26 11:48:03,076 - ERROR - 错误: 处理第 4 行时发生异常: Failed to deserialize the JSON body into the target type: messages[1]: data did not match any variant of untagged enum ChatCompletionRequestContent at line 1 column 299\n",
      "2025-08-26 11:48:03,287 - INFO - HTTP Request: POST https://api.deepseek.com/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-08-26 11:48:03,414 - INFO - HTTP Request: POST https://api.deepseek.com/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-08-26 11:48:04,113 - INFO - HTTP Request: POST https://api.deepseek.com/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-08-26 11:48:08,268 - INFO - HTTP Request: POST https://api.deepseek.com/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-08-26 11:48:08,722 - INFO - HTTP Request: POST https://api.deepseek.com/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-08-26 11:48:09,047 - INFO - HTTP Request: POST https://api.deepseek.com/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-08-26 11:48:09,048 - INFO - HTTP Request: POST https://api.deepseek.com/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-08-26 11:48:09,167 - INFO - HTTP Request: POST https://api.deepseek.com/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-08-26 11:48:09,353 - INFO - HTTP Request: POST https://api.deepseek.com/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-08-26 11:48:09,889 - INFO - HTTP Request: POST https://api.deepseek.com/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-08-26 11:48:10,315 - INFO - HTTP Request: POST https://api.deepseek.com/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-08-26 11:48:10,411 - INFO - HTTP Request: POST https://api.deepseek.com/chat/completions \"HTTP/1.1 422 Unprocessable Entity\"\n",
      "2025-08-26 11:48:10,412 - ERROR - 错误: 处理第 1 行时发生异常: Failed to deserialize the JSON body into the target type: messages[1]: data did not match any variant of untagged enum ChatCompletionRequestContent at line 1 column 299\n",
      "2025-08-26 11:48:10,465 - INFO - HTTP Request: POST https://api.deepseek.com/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-08-26 11:48:10,816 - INFO - HTTP Request: POST https://api.deepseek.com/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-08-26 11:48:10,970 - INFO - HTTP Request: POST https://api.deepseek.com/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-08-26 11:48:11,151 - INFO - HTTP Request: POST https://api.deepseek.com/chat/completions \"HTTP/1.1 422 Unprocessable Entity\"\n",
      "2025-08-26 11:48:11,151 - ERROR - 错误: 处理第 2 行时发生异常: Failed to deserialize the JSON body into the target type: messages[1]: data did not match any variant of untagged enum ChatCompletionRequestContent at line 1 column 299\n",
      "2025-08-26 11:48:11,181 - INFO - HTTP Request: POST https://api.deepseek.com/chat/completions \"HTTP/1.1 422 Unprocessable Entity\"\n",
      "2025-08-26 11:48:11,182 - ERROR - 错误: 处理第 1 行时发生异常: Failed to deserialize the JSON body into the target type: messages[1]: data did not match any variant of untagged enum ChatCompletionRequestContent at line 1 column 299\n",
      "2025-08-26 11:48:11,207 - INFO - HTTP Request: POST https://api.deepseek.com/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-08-26 11:48:11,228 - INFO - HTTP Request: POST https://api.deepseek.com/chat/completions \"HTTP/1.1 422 Unprocessable Entity\"\n",
      "2025-08-26 11:48:11,229 - ERROR - 错误: 处理第 2 行时发生异常: Failed to deserialize the JSON body into the target type: messages[1]: data did not match any variant of untagged enum ChatCompletionRequestContent at line 1 column 299\n",
      "2025-08-26 11:48:11,266 - INFO - HTTP Request: POST https://api.deepseek.com/chat/completions \"HTTP/1.1 422 Unprocessable Entity\"\n",
      "2025-08-26 11:48:11,267 - ERROR - 错误: 处理第 3 行时发生异常: Failed to deserialize the JSON body into the target type: messages[1]: data did not match any variant of untagged enum ChatCompletionRequestContent at line 1 column 299\n",
      "2025-08-26 11:48:11,392 - INFO - HTTP Request: POST https://api.deepseek.com/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-08-26 11:48:11,402 - INFO - HTTP Request: POST https://api.deepseek.com/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-08-26 11:48:15,148 - INFO - HTTP Request: POST https://api.deepseek.com/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-08-26 11:48:15,362 - INFO - HTTP Request: POST https://api.deepseek.com/chat/completions \"HTTP/1.1 422 Unprocessable Entity\"\n",
      "2025-08-26 11:48:15,366 - ERROR - 错误: 处理第 2 行时发生异常: Failed to deserialize the JSON body into the target type: messages[1]: data did not match any variant of untagged enum ChatCompletionRequestContent at line 1 column 299\n",
      "2025-08-26 11:48:15,418 - INFO - HTTP Request: POST https://api.deepseek.com/chat/completions \"HTTP/1.1 422 Unprocessable Entity\"\n",
      "2025-08-26 11:48:15,420 - ERROR - 错误: 处理第 3 行时发生异常: Failed to deserialize the JSON body into the target type: messages[1]: data did not match any variant of untagged enum ChatCompletionRequestContent at line 1 column 299\n",
      "2025-08-26 11:48:15,487 - INFO - HTTP Request: POST https://api.deepseek.com/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-08-26 11:48:15,628 - INFO - HTTP Request: POST https://api.deepseek.com/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-08-26 11:48:16,018 - INFO - HTTP Request: POST https://api.deepseek.com/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-08-26 11:48:16,447 - INFO - HTTP Request: POST https://api.deepseek.com/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-08-26 11:48:16,558 - INFO - HTTP Request: POST https://api.deepseek.com/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-08-26 11:48:16,691 - INFO - HTTP Request: POST https://api.deepseek.com/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-08-26 11:48:17,314 - INFO - HTTP Request: POST https://api.deepseek.com/chat/completions \"HTTP/1.1 422 Unprocessable Entity\"\n",
      "2025-08-26 11:48:17,314 - ERROR - 错误: 处理第 3 行时发生异常: Failed to deserialize the JSON body into the target type: messages[1]: data did not match any variant of untagged enum ChatCompletionRequestContent at line 1 column 299\n",
      "2025-08-26 11:48:17,354 - INFO - HTTP Request: POST https://api.deepseek.com/chat/completions \"HTTP/1.1 422 Unprocessable Entity\"\n",
      "2025-08-26 11:48:17,355 - ERROR - 错误: 处理第 4 行时发生异常: Failed to deserialize the JSON body into the target type: messages[1]: data did not match any variant of untagged enum ChatCompletionRequestContent at line 1 column 299\n",
      "2025-08-26 11:48:17,470 - INFO - HTTP Request: POST https://api.deepseek.com/chat/completions \"HTTP/1.1 422 Unprocessable Entity\"\n",
      "2025-08-26 11:48:17,471 - ERROR - 错误: 处理第 0 行时发生异常: Failed to deserialize the JSON body into the target type: messages[1]: data did not match any variant of untagged enum ChatCompletionRequestContent at line 1 column 299\n",
      "2025-08-26 11:48:17,601 - INFO - HTTP Request: POST https://api.deepseek.com/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-08-26 11:48:17,637 - INFO - HTTP Request: POST https://api.deepseek.com/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-08-26 11:48:18,655 - INFO - HTTP Request: POST https://api.deepseek.com/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-08-26 11:48:19,146 - INFO - HTTP Request: POST https://api.deepseek.com/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-08-26 11:48:19,265 - INFO - HTTP Request: POST https://api.deepseek.com/chat/completions \"HTTP/1.1 422 Unprocessable Entity\"\n",
      "2025-08-26 11:48:19,266 - ERROR - 错误: 处理第 0 行时发生异常: Failed to deserialize the JSON body into the target type: messages[1]: data did not match any variant of untagged enum ChatCompletionRequestContent at line 1 column 299\n",
      "2025-08-26 11:48:19,814 - INFO - HTTP Request: POST https://api.deepseek.com/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-08-26 11:48:19,901 - INFO - HTTP Request: POST https://api.deepseek.com/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-08-26 11:48:21,093 - INFO - HTTP Request: POST https://api.deepseek.com/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-08-26 11:48:21,499 - INFO - HTTP Request: POST https://api.deepseek.com/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-08-26 11:48:22,429 - INFO - HTTP Request: POST https://api.deepseek.com/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-08-26 11:48:23,449 - INFO - HTTP Request: POST https://api.deepseek.com/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-08-26 11:48:23,970 - INFO - HTTP Request: POST https://api.deepseek.com/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-08-26 11:48:24,568 - INFO - HTTP Request: POST https://api.deepseek.com/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-08-26 11:48:24,899 - INFO - HTTP Request: POST https://api.deepseek.com/chat/completions \"HTTP/1.1 422 Unprocessable Entity\"\n",
      "2025-08-26 11:48:24,901 - ERROR - 错误: 处理第 4 行时发生异常: Failed to deserialize the JSON body into the target type: messages[1]: data did not match any variant of untagged enum ChatCompletionRequestContent at line 1 column 299\n",
      "2025-08-26 11:48:24,998 - INFO - HTTP Request: POST https://api.deepseek.com/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-08-26 11:48:25,025 - INFO - HTTP Request: POST https://api.deepseek.com/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-08-26 11:48:25,145 - INFO - HTTP Request: POST https://api.deepseek.com/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-08-26 11:48:25,743 - INFO - HTTP Request: POST https://api.deepseek.com/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-08-26 11:48:26,442 - INFO - HTTP Request: POST https://api.deepseek.com/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-08-26 11:48:26,993 - INFO - HTTP Request: POST https://api.deepseek.com/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-08-26 11:48:27,744 - INFO - HTTP Request: POST https://api.deepseek.com/chat/completions \"HTTP/1.1 422 Unprocessable Entity\"\n",
      "2025-08-26 11:48:27,745 - ERROR - 错误: 处理第 0 行时发生异常: Failed to deserialize the JSON body into the target type: messages[1]: data did not match any variant of untagged enum ChatCompletionRequestContent at line 1 column 299\n",
      "2025-08-26 11:48:27,839 - INFO - HTTP Request: POST https://api.deepseek.com/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-08-26 11:48:28,202 - INFO - HTTP Request: POST https://api.deepseek.com/chat/completions \"HTTP/1.1 422 Unprocessable Entity\"\n",
      "2025-08-26 11:48:28,203 - ERROR - 错误: 处理第 3 行时发生异常: Failed to deserialize the JSON body into the target type: messages[1]: data did not match any variant of untagged enum ChatCompletionRequestContent at line 1 column 299\n",
      "2025-08-26 11:48:28,275 - INFO - HTTP Request: POST https://api.deepseek.com/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-08-26 11:48:28,585 - INFO - HTTP Request: POST https://api.deepseek.com/chat/completions \"HTTP/1.1 422 Unprocessable Entity\"\n",
      "2025-08-26 11:48:28,586 - ERROR - 错误: 处理第 4 行时发生异常: Failed to deserialize the JSON body into the target type: messages[1]: data did not match any variant of untagged enum ChatCompletionRequestContent at line 1 column 299\n",
      "2025-08-26 11:48:30,314 - INFO - HTTP Request: POST https://api.deepseek.com/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-08-26 11:48:31,361 - INFO - HTTP Request: POST https://api.deepseek.com/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-08-26 11:48:31,675 - INFO - HTTP Request: POST https://api.deepseek.com/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-08-26 11:48:33,177 - INFO - HTTP Request: POST https://api.deepseek.com/chat/completions \"HTTP/1.1 422 Unprocessable Entity\"\n",
      "2025-08-26 11:48:33,184 - ERROR - 错误: 处理第 1 行时发生异常: Failed to deserialize the JSON body into the target type: messages[1]: data did not match any variant of untagged enum ChatCompletionRequestContent at line 1 column 299\n",
      "2025-08-26 11:48:33,192 - INFO - HTTP Request: POST https://api.deepseek.com/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-08-26 11:48:33,255 - INFO - HTTP Request: POST https://api.deepseek.com/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-08-26 11:48:33,625 - INFO - HTTP Request: POST https://api.deepseek.com/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-08-26 11:48:34,411 - INFO - HTTP Request: POST https://api.deepseek.com/chat/completions \"HTTP/1.1 422 Unprocessable Entity\"\n",
      "2025-08-26 11:48:34,422 - ERROR - 错误: 处理第 4 行时发生异常: Failed to deserialize the JSON body into the target type: messages[1]: data did not match any variant of untagged enum ChatCompletionRequestContent at line 1 column 299\n",
      "2025-08-26 11:48:35,361 - INFO - HTTP Request: POST https://api.deepseek.com/chat/completions \"HTTP/1.1 422 \"\n",
      "2025-08-26 11:48:35,363 - ERROR - 错误: 处理第 3 行时发生异常: Error code: 422 - {'error_msg': 'Invalid Parameters. Multiple 422 errors detected. Please wait for 1 minute before trying again.'}\n",
      "2025-08-26 11:48:35,908 - INFO - HTTP Request: POST https://api.deepseek.com/chat/completions \"HTTP/1.1 422 \"\n",
      "2025-08-26 11:48:35,909 - ERROR - 错误: 处理第 4 行时发生异常: Error code: 422 - {'error_msg': 'Invalid Parameters. Multiple 422 errors detected. Please wait for 1 minute before trying again.'}\n",
      "2025-08-26 11:48:36,844 - INFO - HTTP Request: POST https://api.deepseek.com/chat/completions \"HTTP/1.1 422 \"\n",
      "2025-08-26 11:48:36,845 - ERROR - 错误: 处理第 4 行时发生异常: Error code: 422 - {'error_msg': 'Invalid Parameters. Multiple 422 errors detected. Please wait for 1 minute before trying again.'}\n",
      "Processing batches:   5%|▍         | 1/21 [00:37<12:25, 37.26s/it]2025-08-26 11:48:39,446 - INFO - HTTP Request: POST https://api.deepseek.com/chat/completions \"HTTP/1.1 422 \"\n",
      "2025-08-26 11:48:39,456 - ERROR - 错误: 处理第 4 行时发生异常: Error code: 422 - {'error_msg': 'Invalid Parameters. Multiple 422 errors detected. Please wait for 1 minute before trying again.'}\n",
      "2025-08-26 11:48:39,579 - INFO - HTTP Request: POST https://api.deepseek.com/chat/completions \"HTTP/1.1 422 \"\n",
      "2025-08-26 11:48:39,582 - ERROR - 错误: 处理第 4 行时发生异常: Error code: 422 - {'error_msg': 'Invalid Parameters. Multiple 422 errors detected. Please wait for 1 minute before trying again.'}\n",
      "Processing batches:  29%|██▊       | 6/21 [00:38<01:12,  4.84s/it]2025-08-26 11:48:41,021 - INFO - HTTP Request: POST https://api.deepseek.com/chat/completions \"HTTP/1.1 422 \"\n",
      "2025-08-26 11:48:41,021 - ERROR - 错误: 处理第 2 行时发生异常: Error code: 422 - {'error_msg': 'Invalid Parameters. Multiple 422 errors detected. Please wait for 1 minute before trying again.'}\n",
      "2025-08-26 11:48:41,511 - INFO - HTTP Request: POST https://api.deepseek.com/chat/completions \"HTTP/1.1 422 \"\n",
      "2025-08-26 11:48:41,519 - ERROR - 错误: 处理第 3 行时发生异常: Error code: 422 - {'error_msg': 'Invalid Parameters. Multiple 422 errors detected. Please wait for 1 minute before trying again.'}\n",
      "2025-08-26 11:48:41,581 - INFO - HTTP Request: POST https://api.deepseek.com/chat/completions \"HTTP/1.1 422 \"\n",
      "2025-08-26 11:48:41,587 - ERROR - 错误: 处理第 3 行时发生异常: Error code: 422 - {'error_msg': 'Invalid Parameters. Multiple 422 errors detected. Please wait for 1 minute before trying again.'}\n",
      "2025-08-26 11:48:42,184 - INFO - HTTP Request: POST https://api.deepseek.com/chat/completions \"HTTP/1.1 422 \"\n",
      "2025-08-26 11:48:42,185 - INFO - HTTP Request: POST https://api.deepseek.com/chat/completions \"HTTP/1.1 422 \"\n",
      "2025-08-26 11:48:42,188 - ERROR - 错误: 处理第 4 行时发生异常: Error code: 422 - {'error_msg': 'Invalid Parameters. Multiple 422 errors detected. Please wait for 1 minute before trying again.'}\n",
      "Processing batches:  90%|█████████ | 19/21 [00:40<00:02,  1.22s/it]2025-08-26 11:48:42,190 - ERROR - 错误: 处理第 4 行时发生异常: Error code: 422 - {'error_msg': 'Invalid Parameters. Multiple 422 errors detected. Please wait for 1 minute before trying again.'}\n",
      "Processing batches: 100%|██████████| 21/21 [00:40<00:00,  1.92s/it]\n",
      "2025-08-26 11:48:42,194 - INFO - 处理完成: 总数 101, 成功 61, 成功率 60.40%\n"
     ]
    }
   ],
   "source": [
    "processor = OpenAITextProcessor(api_key=\"此处填入你的deepseek api key\", base_url=\"https://api.deepseek.com\",model=\"deepseek-chat\")\n",
    "df_result = processor.process_batch(\n",
    "    df=result_df,\n",
    "    text_column=\"description\",\n",
    "    prompt=prompt,\n",
    "    batch_size=5,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "df749f16",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_result.to_csv(\"备份_中间流程结果/dataset_translated.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f62dd7b",
   "metadata": {},
   "source": [
    "# 打标签"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "5502835d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== 标签分配结果 ===\n",
      "\n",
      "ID: 1\n",
      "标题: MammalNet哺乳动物行为识别数据集\n",
      "标签: 大气海洋/趋势模式识别/数值天气预报/观测数据\n",
      "置信度: 0.025\n",
      "--------------------------------------------------\n",
      "ID: 2\n",
      "标题: 湍流流动DNS数据库\n",
      "标签: 工程技术/流体动力学分析/流体力学/仿真数据\n",
      "置信度: 0.006\n",
      "--------------------------------------------------\n",
      "ID: 3\n",
      "标题: 分子机器学习基准MoleculeNet\n",
      "标签: 大气海洋/环境质量评估/数值天气预报/观测数据\n",
      "置信度: 0.062\n",
      "--------------------------------------------------\n",
      "ID: 4\n",
      "标题: 超高清月面地图\n",
      "标签: 大气海洋/天气数值预报/遥感应用/观测数据\n",
      "置信度: 0.012\n",
      "--------------------------------------------------\n",
      "ID: 5\n",
      "标题: 全球海洋再分析数据GLORYS12V1\n",
      "标签: 大气海洋/环境质量评估/数值天气预报/再分析数据\n",
      "置信度: 0.136\n",
      "--------------------------------------------------\n",
      "\n",
      "=== 标签统计信息 ===\n",
      "{\n",
      "  \"总数据量\": 5,\n",
      "  \"学科分布\": {\n",
      "    \"大气海洋\": 4,\n",
      "    \"工程技术\": 1\n",
      "  },\n",
      "  \"置信度分布\": {\n",
      "    \"高(>0.3)\": 0,\n",
      "    \"中(0.1-0.3)\": 1,\n",
      "    \"低(<0.1)\": 4\n",
      "  },\n",
      "  \"功能导向分布\": {\n",
      "    \"趋势模式识别\": 1,\n",
      "    \"流体动力学分析\": 1,\n",
      "    \"环境质量评估\": 2,\n",
      "    \"天气数值预报\": 1\n",
      "  },\n",
      "  \"应用领域分布\": {\n",
      "    \"数值天气预报\": 3,\n",
      "    \"流体力学\": 1,\n",
      "    \"遥感应用\": 1\n",
      "  },\n",
      "  \"数据类型分布\": {\n",
      "    \"观测数据\": 3,\n",
      "    \"仿真数据\": 1,\n",
      "    \"再分析数据\": 1\n",
      "  }\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "import json\n",
    "from typing import Dict, List, Set, Tuple\n",
    "from collections import defaultdict\n",
    "\n",
    "class ScienceCorpusTagger:\n",
    "    \"\"\"\n",
    "    科学语料数据自动标签分配系统（三层级整合版）\n",
    "    输出格式：学科/功能导向/应用领域/数据类型\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.tag_hierarchy = self._init_tag_hierarchy()\n",
    "        self.keyword_mapping = self._init_keyword_mapping()\n",
    "        \n",
    "    def _init_tag_hierarchy(self) -> Dict:\n",
    "        \"\"\"初始化三层标签层次结构\"\"\"\n",
    "        return {\n",
    "            \"生命科学\": {\n",
    "                \"功能导向\": [\n",
    "                    \"疾病诊断分类\", \"治疗预测建模\", \"基因功能注释\", \"药物筛选发现\", \n",
    "                    \"生物标志物识别\", \"病理图像分析\", \"分子结构预测\", \"进化关联分析\",\n",
    "                    \"临床预后评估\", \"健康状态监测\"\n",
    "                ],\n",
    "                \"应用领域\": [\n",
    "                    \"医学影像\", \"基因组学\", \"蛋白质组学\", \"神经科学\", \"临床医学\",\n",
    "                    \"药物学\", \"免疫学\", \"病理学\", \"生物信息学\", \"分子生物学\"\n",
    "                ],\n",
    "                \"数据类型\": [\n",
    "                    \"序列数据\", \"影像数据\", \"信号数据\", \"结构数据\", \"表型数据\",\n",
    "                    \"光谱数据\", \"时间序列数据\"\n",
    "                ]\n",
    "            },\n",
    "            \"工程技术\": {\n",
    "                \"功能导向\": [\n",
    "                    \"系统设计优化\", \"性能预测评估\", \"故障诊断监测\", \"过程控制仿真\",\n",
    "                    \"参数调优验证\", \"流体动力学分析\", \"热传导建模\", \"结构力学计算\",\n",
    "                    \"自动化控制\", \"实验数据处理\"\n",
    "                ],\n",
    "                \"应用领域\": [\n",
    "                    \"流体力学\", \"热传递工程\", \"结构工程\", \"自动化工程\", \"计算工程\",\n",
    "                    \"机械工程\", \"化工工程\", \"航空航天\", \"材料工程\", \"控制工程\"\n",
    "                ],\n",
    "                \"数据类型\": [\n",
    "                    \"仿真数据\", \"实验数据\", \"传感器数据\", \"时序数据\", \"几何数据\",\n",
    "                    \"网格数据\", \"图像数据\"\n",
    "                ]\n",
    "            },\n",
    "            \"物质科学\": {\n",
    "                \"功能导向\": [\n",
    "                    \"分子性质预测\", \"化学反应预测\", \"材料结构设计\", \"催化活性筛选\",\n",
    "                    \"药物分子生成\", \"合成路径规划\", \"晶体结构优化\", \"光谱特征分析\",\n",
    "                    \"热力学性质计算\", \"毒性安全评估\"\n",
    "                ],\n",
    "                \"应用领域\": [\n",
    "                    \"药物化学\", \"催化科学\", \"材料科学\", \"有机化学\", \"无机化学\",\n",
    "                    \"物理化学\", \"计算化学\", \"高分子科学\", \"纳米材料\", \"能源材料\"\n",
    "                ],\n",
    "                \"数据类型\": [\n",
    "                    \"量子化学数据\", \"分子描述符数据\", \"反应数据\", \"光谱数据\",\n",
    "                    \"热力学数据\", \"动力学数据\", \"结构数据\"\n",
    "                ]\n",
    "            },\n",
    "            \"空间信息\": {\n",
    "                \"功能导向\": [\n",
    "                    \"天体目标识别\", \"轨道参数确定\", \"光度测量分析\", \"光谱特征提取\",\n",
    "                    \"时间序列分析\", \"位置导航定位\", \"空间环境监测\", \"图像处理识别\",\n",
    "                    \"射电信号分析\", \"变源监测预警\"\n",
    "                ],\n",
    "                \"应用领域\": [\n",
    "                    \"天体物理学\", \"行星科学\", \"恒星物理学\", \"星系天文学\", \"空间技术\",\n",
    "                    \"射电天文学\", \"光学天文学\", \"高能天体物理\", \"太阳物理学\", \"宇宙学\"\n",
    "                ],\n",
    "                \"数据类型\": [\n",
    "                    \"光学数据\", \"射电数据\", \"光谱数据\", \"测距数据\", \"图像数据\",\n",
    "                    \"时序数据\", \"位置数据\"\n",
    "                ]\n",
    "            },\n",
    "            \"大气海洋\": {\n",
    "                \"功能导向\": [\n",
    "                    \"天气数值预报\", \"气候变化检测\", \"极端事件预警\", \"环境质量评估\",\n",
    "                    \"数据同化分析\", \"海洋动力学建模\", \"大气成分监测\", \"趋势模式识别\",\n",
    "                    \"灾害风险评估\", \"生态环境预测\"\n",
    "                ],\n",
    "                \"应用领域\": [\n",
    "                    \"数值天气预报\", \"气候变化研究\", \"海洋学\", \"大气化学\", \"环境科学\",\n",
    "                    \"气象学\", \"水文学\", \"极地科学\", \"遥感应用\", \"生态气象学\"\n",
    "                ],\n",
    "                \"数据类型\": [\n",
    "                    \"观测数据\", \"再分析数据\", \"卫星数据\", \"模式数据\", \"代理数据\",\n",
    "                    \"时序数据\", \"网格数据\"\n",
    "                ]\n",
    "            },\n",
    "            \"其他\": {\n",
    "                \"功能导向\": [\n",
    "                    \"知识图谱构建\", \"信息检索排序\", \"文本挖掘分析\", \"多语言处理\",\n",
    "                    \"社会网络分析\", \"统计建模预测\", \"可视化呈现\", \"情感态度分析\",\n",
    "                    \"档案数字化管理\", \"跨学科数据融合\"\n",
    "                ],\n",
    "                \"应用领域\": [\n",
    "                    \"数字人文\", \"计算社会科学\", \"语言学\", \"历史学\", \"文化研究\",\n",
    "                    \"政治学\", \"经济学\", \"人口学\", \"教育学\", \"科学计量学\"\n",
    "                ],\n",
    "                \"数据类型\": [\n",
    "                    \"文本数据\", \"调查数据\", \"档案数据\", \"多媒体数据\", \"网络数据\",\n",
    "                    \"语音数据\", \"图像数据\"\n",
    "                ]\n",
    "            }\n",
    "        }\n",
    "    \n",
    "    def _init_keyword_mapping(self) -> Dict:\n",
    "        \"\"\"初始化完善的关键词映射体系\"\"\"\n",
    "        return {\n",
    "            \"生命科学\": {\n",
    "                \"功能导向\": {\n",
    "                    \"疾病诊断分类\": [\n",
    "                        # 核心词汇\n",
    "                        \"诊断\", \"疾病\", \"分类\", \"识别\", \"检测\", \"病理\", \"临床\", \"诊疗\",\n",
    "                        # 技术方法\n",
    "                        \"分类器\", \"模式识别\", \"特征提取\", \"机器学习\", \"深度学习\", \"神经网络\",\n",
    "                        \"支持向量机\", \"随机森林\", \"卷积神经网络\", \"决策树\", \"朴素贝叶斯\",\n",
    "                        # 应用场景\n",
    "                        \"早期诊断\", \"鉴别诊断\", \"辅助诊断\", \"自动诊断\", \"智能诊断\",\n",
    "                        \"影像诊断\", \"实验室诊断\", \"临床决策\", \"病理分析\",\n",
    "                        # 疾病类型\n",
    "                        \"癌症\", \"肿瘤\", \"糖尿病\", \"心血管\", \"神经系统\", \"感染性疾病\",\n",
    "                        \"遗传病\", \"罕见病\", \"慢性病\", \"急性病\",\n",
    "                        # 英文词汇\n",
    "                        \"diagnosis\", \"disease\", \"classification\", \"detection\", \"pathology\",\n",
    "                        \"clinical\", \"diagnostic\", \"screening\", \"biomarker\", \"phenotype\"\n",
    "                    ],\n",
    "                    \"治疗预测建模\": [\n",
    "                        # 核心词汇\n",
    "                        \"治疗\", \"预测\", \"建模\", \"预后\", \"疗效\", \"康复\", \"预测模型\",\n",
    "                        # 治疗方法\n",
    "                        \"药物治疗\", \"手术治疗\", \"放疗\", \"化疗\", \"免疫治疗\", \"基因治疗\",\n",
    "                        \"靶向治疗\", \"个性化治疗\", \"精准医疗\", \"联合治疗\",\n",
    "                        # 预测指标\n",
    "                        \"生存率\", \"复发率\", \"治愈率\", \"缓解率\", \"副作用\", \"耐药性\",\n",
    "                        \"治疗反应\", \"预后评估\", \"风险评估\", \"生存分析\",\n",
    "                        # 技术方法\n",
    "                        \"回归分析\", \"生存模型\", \"Cox回归\", \"随机效应\", \"贝叶斯模型\",\n",
    "                        \"时间序列\", \"纵向数据\", \"队列研究\", \"临床试验\",\n",
    "                        # 英文词汇\n",
    "                        \"treatment\", \"therapy\", \"prognosis\", \"efficacy\", \"outcome\",\n",
    "                        \"survival\", \"response\", \"toxicity\", \"adverse\", \"recovery\"\n",
    "                    ],\n",
    "                    \"基因功能注释\": [\n",
    "                        # 核心词汇\n",
    "                        \"基因\", \"功能\", \"注释\", \"转录\", \"表达\", \"调控\", \"基因组\",\n",
    "                        # 分子机制\n",
    "                        \"转录调控\", \"转录因子\", \"启动子\", \"增强子\", \"表观遗传\",\n",
    "                        \"甲基化\", \"组蛋白修饰\", \"染色质\", \"非编码RNA\", \"microRNA\",\n",
    "                        \"lncRNA\", \"siRNA\", \"piRNA\", \"circRNA\",\n",
    "                        # 功能类别\n",
    "                        \"信号通路\", \"代谢通路\", \"细胞周期\", \"细胞凋亡\", \"细胞分化\",\n",
    "                        \"免疫应答\", \"DNA修复\", \"蛋白质合成\", \"能量代谢\",\n",
    "                        # 技术方法\n",
    "                        \"RNA-seq\", \"ChIP-seq\", \"ATAC-seq\", \"单细胞测序\", \"全基因组\",\n",
    "                        \"功能富集\", \"通路分析\", \"共表达网络\", \"基因本体\",\n",
    "                        # 英文词汇\n",
    "                        \"gene\", \"annotation\", \"transcription\", \"expression\", \"regulation\",\n",
    "                        \"functional\", \"pathway\", \"ontology\", \"enrichment\", \"network\"\n",
    "                    ],\n",
    "                    \"药物筛选发现\": [\n",
    "                        # 核心词汇\n",
    "                        \"药物\", \"筛选\", \"发现\", \"化合物\", \"活性\", \"毒性\", \"药效\",\n",
    "                        # 药物类型\n",
    "                        \"小分子\", \"大分子\", \"生物药\", \"抗体\", \"疫苗\", \"基因药物\",\n",
    "                        \"纳米药物\", \"天然产物\", \"合成化合物\", \"先导化合物\",\n",
    "                        # 筛选方法\n",
    "                        \"高通量筛选\", \"虚拟筛选\", \"分子对接\", \"结构筛选\", \"表型筛选\",\n",
    "                        \"细胞筛选\", \"生化筛选\", \"靶点筛选\", \"片段筛选\",\n",
    "                        # 评价指标\n",
    "                        \"IC50\", \"EC50\", \"ADMET\", \"溶解度\", \"渗透性\", \"代谢稳定性\",\n",
    "                        \"细胞毒性\", \"选择性\", \"特异性\", \"亲和力\",\n",
    "                        # 英文词汇\n",
    "                        \"drug\", \"compound\", \"screening\", \"discovery\", \"activity\",\n",
    "                        \"toxicity\", \"pharmacology\", \"medicinal\", \"therapeutic\", \"binding\"\n",
    "                    ],\n",
    "                    \"生物标志物识别\": [\n",
    "                        # 核心词汇\n",
    "                        \"标志物\", \"生物标记\", \"分子标记\", \"诊断标记\", \"预后标记\",\n",
    "                        # 标志物类型\n",
    "                        \"蛋白质标志物\", \"基因标志物\", \"代谢标志物\", \"影像标志物\",\n",
    "                        \"液体活检\", \"循环肿瘤细胞\", \"循环DNA\", \"外泌体\", \"miRNA\",\n",
    "                        # 应用场景\n",
    "                        \"早期发现\", \"风险预测\", \"治疗监测\", \"药物反应\", \"疾病分型\",\n",
    "                        \"预后判断\", \"复发监测\", \"伴随诊断\", \"个体化医疗\",\n",
    "                        # 技术方法\n",
    "                        \"质谱\", \"蛋白组学\", \"代谢组学\", \"基因组学\", \"多组学\",\n",
    "                        \"机器学习\", \"统计分析\", \"验证研究\", \"队列分析\",\n",
    "                        # 英文词汇\n",
    "                        \"biomarker\", \"marker\", \"indicator\", \"signature\", \"panel\",\n",
    "                        \"proteomics\", \"metabolomics\", \"genomics\", \"omics\", \"validation\"\n",
    "                    ],\n",
    "                    \"病理图像分析\": [\n",
    "                        # 核心词汇\n",
    "                        \"病理\", \"切片\", \"组织\", \"细胞\", \"形态学\", \"病理学\",\n",
    "                        # 图像类型\n",
    "                        \"HE染色\", \"免疫组化\", \"荧光染色\", \"特殊染色\", \"显微镜\",\n",
    "                        \"数字病理\", \"全切片图像\", \"细胞病理\", \"组织病理\",\n",
    "                        # 分析内容\n",
    "                        \"细胞计数\", \"形态分析\", \"结构分析\", \"染色定量\", \"空间分析\",\n",
    "                        \"核型分析\", \"细胞分割\", \"组织分割\", \"病变检测\",\n",
    "                        # 技术方法\n",
    "                        \"图像处理\", \"计算机视觉\", \"深度学习\", \"卷积网络\", \"分割算法\",\n",
    "                        \"特征提取\", \"模式识别\", \"自动分析\", \"人工智能\",\n",
    "                        # 英文词汇\n",
    "                        \"pathology\", \"histology\", \"cytology\", \"microscopy\", \"morphology\",\n",
    "                        \"digital\", \"image\", \"segmentation\", \"quantification\", \"automated\"\n",
    "                    ],\n",
    "                    \"分子结构预测\": [\n",
    "                        # 核心词汇\n",
    "                        \"结构\", \"预测\", \"折叠\", \"构象\", \"三维\", \"分子结构\",\n",
    "                        # 结构类型\n",
    "                        \"蛋白质结构\", \"DNA结构\", \"RNA结构\", \"复合物结构\", \"膜蛋白\",\n",
    "                        \"二级结构\", \"三级结构\", \"四级结构\", \"动态结构\", \"柔性结构\",\n",
    "                        # 预测方法\n",
    "                        \"同源建模\", \"从头预测\", \"穿线法\", \"分子动力学\", \"蒙特卡洛\",\n",
    "                        \"神经网络\", \"深度学习\", \"AlphaFold\", \"结构比对\", \"能量最小化\",\n",
    "                        # 应用领域\n",
    "                        \"药物设计\", \"功能预测\", \"相互作用\", \"结构域\", \"活性位点\",\n",
    "                        \"分子对接\", \"虚拟筛选\", \"结构生物学\", \"计算生物学\",\n",
    "                        # 英文词汇\n",
    "                        \"structure\", \"folding\", \"conformation\", \"prediction\", \"modeling\",\n",
    "                        \"homology\", \"template\", \"dynamics\", \"simulation\", \"computational\"\n",
    "                    ],\n",
    "                    \"进化关联分析\": [\n",
    "                        # 核心词汇\n",
    "                        \"进化\", \"系统发育\", \"比较\", \"同源\", \"保守\", \"进化树\",\n",
    "                        # 进化概念\n",
    "                        \"自然选择\", \"遗传漂变\", \"基因流\", \"物种形成\", \"适应性进化\",\n",
    "                        \"中性进化\", \"分子进化\", \"共同祖先\", \"分歧时间\", \"进化速率\",\n",
    "                        # 分析方法\n",
    "                        \"系统发育分析\", \"比较基因组\", \"同源性分析\", \"保守性分析\",\n",
    "                        \"正选择检验\", \"分子钟\", \"贝叶斯分析\", \"最大似然\", \"邻接法\",\n",
    "                        # 应用对象\n",
    "                        \"物种进化\", \"基因进化\", \"蛋白质进化\", \"代谢通路进化\",\n",
    "                        \"病毒进化\", \"细菌进化\", \"真核进化\", \"功能进化\",\n",
    "                        # 英文词汇\n",
    "                        \"evolution\", \"phylogeny\", \"comparative\", \"homology\", \"conservation\",\n",
    "                        \"selection\", \"divergence\", \"ancestral\", \"molecular\", \"adaptive\"\n",
    "                    ],\n",
    "                    \"临床预后评估\": [\n",
    "                        # 核心词汇\n",
    "                        \"临床\", \"预后\", \"评估\", \"生存\", \"风险\", \"预后因子\",\n",
    "                        # 评估指标\n",
    "                        \"生存率\", \"无病生存\", \"总生存\", \"复发风险\", \"死亡率\",\n",
    "                        \"生活质量\", \"功能状态\", \"并发症\", \"治疗反应\", \"缓解率\",\n",
    "                        # 预后因子\n",
    "                        \"临床分期\", \"病理分级\", \"分子分型\", \"基因表达\", \"蛋白标志物\",\n",
    "                        \"年龄\", \"性别\", \"合并症\", \"治疗史\", \"家族史\",\n",
    "                        # 分析方法\n",
    "                        \"生存分析\", \"Cox回归\", \"Kaplan-Meier\", \"风险评分\", \"预测模型\",\n",
    "                        \"机器学习\", \"深度学习\", \"多变量分析\", \"时间依赖\", \"竞争风险\",\n",
    "                        # 英文词汇\n",
    "                        \"prognosis\", \"survival\", \"outcome\", \"risk\", \"prognostic\",\n",
    "                        \"clinical\", \"factor\", \"prediction\", \"assessment\", \"stratification\"\n",
    "                    ],\n",
    "                    \"健康状态监测\": [\n",
    "                        # 核心词汇\n",
    "                        \"健康\", \"监测\", \"体征\", \"生理\", \"状态\", \"健康监护\",\n",
    "                        # 监测参数\n",
    "                        \"心率\", \"血压\", \"体温\", \"呼吸\", \"血氧\", \"血糖\",\n",
    "                        \"心电图\", \"脑电图\", \"睡眠\", \"运动\", \"压力\", \"情绪\",\n",
    "                        # 监测技术\n",
    "                        \"可穿戴设备\", \"传感器\", \"物联网\", \"远程监测\", \"实时监测\",\n",
    "                        \"连续监测\", \"移动健康\", \"数字健康\", \"智能设备\", \"生物传感\",\n",
    "                        # 数据分析\n",
    "                        \"异常检测\", \"趋势分析\", \"预警系统\", \"健康评估\", \"风险预测\",\n",
    "                        \"个性化监测\", \"数据融合\", \"信号处理\", \"模式识别\",\n",
    "                        # 英文词汇\n",
    "                        \"health\", \"monitoring\", \"vital\", \"physiological\", \"wearable\",\n",
    "                        \"sensor\", \"remote\", \"continuous\", \"digital\", \"wellness\"\n",
    "                    ]\n",
    "                },\n",
    "                \"应用领域\": {\n",
    "                    \"医学影像\": [\n",
    "                        # 影像技术\n",
    "                        \"CT\", \"MRI\", \"X射线\", \"超声\", \"影像\", \"扫描\", \"PET\", \"SPECT\",\n",
    "                        \"DSA\", \"OCT\", \"内镜\", \"病理影像\", \"分子影像\", \"功能影像\",\n",
    "                        # 解剖部位\n",
    "                        \"头颅\", \"胸部\", \"腹部\", \"心脏\", \"肺部\", \"肝脏\", \"肾脏\",\n",
    "                        \"骨骼\", \"关节\", \"血管\", \"神经\", \"眼科\", \"妇科\", \"儿科\",\n",
    "                        # 影像诊断\n",
    "                        \"影像诊断\", \"影像分析\", \"三维重建\", \"图像配准\", \"图像融合\",\n",
    "                        \"对比度增强\", \"影像组学\", \"定量影像\", \"计算机辅助诊断\",\n",
    "                        # 英文词汇\n",
    "                        \"imaging\", \"radiology\", \"radiography\", \"tomography\", \"ultrasound\",\n",
    "                        \"magnetic\", \"resonance\", \"computed\", \"positron\", \"emission\"\n",
    "                    ],\n",
    "                    \"基因组学\": [\n",
    "                        # 核心概念\n",
    "                        \"基因组\", \"DNA\", \"测序\", \"变异\", \"SNP\", \"CNV\", \"INDEL\",\n",
    "                        \"基因型\", \"表型\", \"遗传\", \"突变\", \"多态性\", \"单倍型\",\n",
    "                        # 测序技术\n",
    "                        \"二代测序\", \"三代测序\", \"全基因组\", \"全外显子\", \"靶向测序\",\n",
    "                        \"单细胞测序\", \"宏基因组\", \"转录组\", \"表观基因组\", \"染色质\",\n",
    "                        # 分析方法\n",
    "                        \"序列比对\", \"变异检测\", \"基因注释\", \"功能预测\", \"通路分析\",\n",
    "                        \"全基因组关联\", \"连锁分析\", \"家系分析\", \"群体遗传学\",\n",
    "                        # 英文词汇\n",
    "                        \"genomics\", \"sequencing\", \"variant\", \"mutation\", \"polymorphism\",\n",
    "                        \"genotype\", \"phenotype\", \"genome\", \"exome\", \"transcriptome\"\n",
    "                    ],\n",
    "                    \"蛋白质组学\": [\n",
    "                        # 核心概念\n",
    "                        \"蛋白质\", \"蛋白\", \"酶\", \"多肽\", \"蛋白质组\", \"翻译后修饰\",\n",
    "                        \"蛋白质相互作用\", \"蛋白质复合物\", \"蛋白质网络\", \"酶活性\",\n",
    "                        # 技术方法\n",
    "                        \"质谱\", \"二维电泳\", \"Western blot\", \"免疫印迹\", \"ELISA\",\n",
    "                        \"色谱\", \"蛋白质纯化\", \"蛋白质鉴定\", \"定量蛋白质组学\",\n",
    "                        # 功能分析\n",
    "                        \"功能域\", \"蛋白质折叠\", \"结构预测\", \"活性位点\", \"底物特异性\",\n",
    "                        \"酶动力学\", \"蛋白质稳定性\", \"降解\", \"修饰\", \"磷酸化\",\n",
    "                        # 英文词汇\n",
    "                        \"proteomics\", \"protein\", \"enzyme\", \"peptide\", \"mass\",\n",
    "                        \"spectrometry\", \"interaction\", \"modification\", \"folding\", \"activity\"\n",
    "                    ],\n",
    "                    \"神经科学\": [\n",
    "                        # 核心概念\n",
    "                        \"神经\", \"大脑\", \"脑电\", \"EEG\", \"fMRI\", \"认知\", \"神经元\",\n",
    "                        \"突触\", \"神经网络\", \"神经递质\", \"神经回路\", \"神经可塑性\",\n",
    "                        # 脑区功能\n",
    "                        \"前额叶\", \"海马\", \"杏仁核\", \"小脑\", \"脑干\", \"丘脑\",\n",
    "                        \"皮层\", \"白质\", \"灰质\", \"神经纤维\", \"神经束\", \"脑区连接\",\n",
    "                        # 认知功能\n",
    "                        \"记忆\", \"学习\", \"注意\", \"语言\", \"运动\", \"感觉\", \"情绪\",\n",
    "                        \"执行功能\", \"工作记忆\", \"决策\", \"意识\", \"睡眠\", \"觉醒\",\n",
    "                        # 技术方法\n",
    "                        \"脑成像\", \"电生理\", \"光遗传学\", \"钙成像\", \"单细胞记录\",\n",
    "                        \"脑电图\", \"脑磁图\", \"近红外\", \"深部脑刺激\", \"经颅刺激\",\n",
    "                        # 英文词汇\n",
    "                        \"neuroscience\", \"brain\", \"neural\", \"cognitive\", \"synaptic\",\n",
    "                        \"neuronal\", \"cortical\", \"hippocampus\", \"amygdala\", \"plasticity\"\n",
    "                    ],\n",
    "                    \"临床医学\": [\n",
    "                        # 医疗实践\n",
    "                        \"临床\", \"患者\", \"医院\", \"诊疗\", \"医学\", \"治疗\", \"护理\",\n",
    "                        \"急诊\", \"ICU\", \"手术\", \"门诊\", \"住院\", \"康复\", \"预防\",\n",
    "                        # 临床科室\n",
    "                        \"内科\", \"外科\", \"儿科\", \"妇产科\", \"神经科\", \"心内科\",\n",
    "                        \"肿瘤科\", \"感染科\", \"急诊科\", \"麻醉科\", \"放射科\", \"检验科\",\n",
    "                        # 临床研究\n",
    "                        \"临床试验\", \"随机对照\", \"队列研究\", \"病例对照\", \"横断面\",\n",
    "                        \"临床指南\", \"循证医学\", \"临床路径\", \"质量控制\", \"安全性\",\n",
    "                        # 英文词汇\n",
    "                        \"clinical\", \"patient\", \"hospital\", \"treatment\", \"therapy\",\n",
    "                        \"medical\", \"healthcare\", \"diagnosis\", \"medicine\", \"care\"\n",
    "                    ],\n",
    "                    \"药物学\": [\n",
    "                        # 药物概念\n",
    "                        \"药物\", \"药理\", \"药代\", \"药效\", \"药动学\", \"药效学\",\n",
    "                        \"药物相互作用\", \"不良反应\", \"毒理学\", \"药物安全性\",\n",
    "                        # 药物类型\n",
    "                        \"处方药\", \"非处方药\", \"生物药\", \"化学药\", \"中药\", \"疫苗\",\n",
    "                        \"抗生素\", \"抗病毒\", \"抗肿瘤\", \"心血管药\", \"神经药物\",\n",
    "                        # 药物研发\n",
    "                        \"药物发现\", \"临床前\", \"临床试验\", \"药物审批\", \"上市后监测\",\n",
    "                        \"制剂工艺\", \"质量控制\", \"药物递送\", \"靶向给药\", \"个体化用药\",\n",
    "                        # 英文词汇\n",
    "                        \"pharmacology\", \"drug\", \"pharmaceutical\", \"medication\", \"therapeutic\",\n",
    "                        \"pharmacokinetics\", \"pharmacodynamics\", \"toxicology\", \"clinical\", \"trial\"\n",
    "                    ],\n",
    "                    \"免疫学\": [\n",
    "                        # 免疫细胞\n",
    "                        \"免疫\", \"抗体\", \"T细胞\", \"B细胞\", \"疫苗\", \"NK细胞\",\n",
    "                        \"巨噬细胞\", \"树突细胞\", \"中性粒细胞\", \"嗜酸性粒细胞\",\n",
    "                        # 免疫机制\n",
    "                        \"先天免疫\", \"获得性免疫\", \"细胞免疫\", \"体液免疫\", \"免疫应答\",\n",
    "                        \"免疫耐受\", \"自身免疫\", \"过敏反应\", \"炎症反应\", \"补体系统\",\n",
    "                        # 免疫分子\n",
    "                        \"细胞因子\", \"白介素\", \"干扰素\", \"肿瘤坏死因子\", \"趋化因子\",\n",
    "                        \"HLA\", \"MHC\", \"TCR\", \"BCR\", \"免疫球蛋白\", \"IgG\", \"IgM\",\n",
    "                        # 英文词汇\n",
    "                        \"immunology\", \"immune\", \"antibody\", \"lymphocyte\", \"vaccination\",\n",
    "                        \"immunotherapy\", \"autoimmune\", \"inflammation\", \"cytokine\", \"antigen\"\n",
    "                    ],\n",
    "                    \"病理学\": [\n",
    "                        # 病理概念\n",
    "                        \"病理\", \"肿瘤\", \"癌症\", \"组织病理\", \"细胞病理\", \"分子病理\",\n",
    "                        \"病理诊断\", \"病理分级\", \"病理分期\", \"恶性\", \"良性\", \"转移\",\n",
    "                        # 病理技术\n",
    "                        \"组织切片\", \"石蜡切片\", \"冰冻切片\", \"HE染色\", \"免疫组化\",\n",
    "                        \"原位杂交\", \"分子诊断\", \"基因检测\", \"突变分析\", \"融合基因\",\n",
    "                        # 肿瘤类型\n",
    "                        \"上皮肿瘤\", \"间质肿瘤\", \"血液肿瘤\", \"神经肿瘤\", \"软组织肿瘤\",\n",
    "                        \"骨肿瘤\", \"皮肤肿瘤\", \"乳腺癌\", \"肺癌\", \"胃癌\", \"肝癌\",\n",
    "                        # 英文词汇\n",
    "                        \"pathology\", \"tumor\", \"cancer\", \"carcinoma\", \"sarcoma\",\n",
    "                        \"malignant\", \"benign\", \"metastasis\", \"histology\", \"cytology\"\n",
    "                    ],\n",
    "                    \"生物信息学\": [\n",
    "                        # 核心概念\n",
    "                        \"生物信息\", \"计算生物\", \"数据库\", \"算法\", \"序列分析\",\n",
    "                        \"结构分析\", \"系统生物学\", \"网络生物学\", \"进化分析\",\n",
    "                        # 数据类型\n",
    "                        \"基因序列\", \"蛋白序列\", \"结构数据\", \"表达数据\", \"变异数据\",\n",
    "                        \"相互作用数据\", \"通路数据\", \"表型数据\", \"临床数据\",\n",
    "                        # 分析方法\n",
    "                        \"序列比对\", \"同源性搜索\", \"基因预测\", \"功能注释\", \"通路分析\",\n",
    "                        \"网络分析\", \"机器学习\", \"深度学习\", \"统计分析\", \"可视化\",\n",
    "                        # 英文词汇\n",
    "                        \"bioinformatics\", \"computational\", \"biology\", \"database\", \"algorithm\",\n",
    "                        \"sequence\", \"alignment\", \"annotation\", \"phylogenetics\", \"genomics\"\n",
    "                    ],\n",
    "                    \"分子生物学\": [\n",
    "                        # 核心概念\n",
    "                        \"分子\", \"细胞\", \"RNA\", \"转录\", \"翻译\", \"基因表达\",\n",
    "                        \"基因调控\", \"信号转导\", \"细胞周期\", \"细胞分裂\", \"细胞凋亡\",\n",
    "                        # 分子技术\n",
    "                        \"PCR\", \"RT-PCR\", \"qPCR\", \"Western blot\", \"Northern blot\",\n",
    "                        \"Southern blot\", \"免疫荧光\", \"流式细胞术\", \"CRISPR\", \"基因敲除\",\n",
    "                        # 细胞过程\n",
    "                        \"DNA复制\", \"DNA修复\", \"RNA剪接\", \"蛋白质合成\", \"蛋白质修饰\",\n",
    "                        \"细胞代谢\", \"能量代谢\", \"信号传递\", \"基因重组\", \"表观遗传\",\n",
    "                        # 英文词汇\n",
    "                        \"molecular\", \"biology\", \"cellular\", \"transcription\", \"translation\",\n",
    "                        \"replication\", \"expression\", \"regulation\", \"signaling\", \"metabolism\"\n",
    "                    ]\n",
    "                },\n",
    "                \"数据类型\": [\n",
    "                    # 基础数据类型\n",
    "                    \"序列数据\", \"影像数据\", \"信号数据\", \"结构数据\", \"表型数据\",\n",
    "                    \"光谱数据\", \"时间序列数据\", \"空间数据\", \"网络数据\", \"图数据\",\n",
    "                    # 生物数据类型\n",
    "                    \"基因组数据\", \"转录组数据\", \"蛋白质组数据\", \"代谢组数据\",\n",
    "                    \"表观基因组数据\", \"微生物组数据\", \"单细胞数据\", \"多组学数据\",\n",
    "                    # 临床数据类型\n",
    "                    \"临床数据\", \"电子病历\", \"影像数据\", \"检验数据\", \"药物数据\",\n",
    "                    \"流行病学数据\", \"队列数据\", \"登记数据\", \"监测数据\",\n",
    "                    # 实验数据类型\n",
    "                    \"实验数据\", \"测量数据\", \"观察数据\", \"调查数据\", \"传感器数据\",\n",
    "                    \"生理数据\", \"行为数据\", \"环境数据\", \"地理数据\"\n",
    "                ]\n",
    "            },\n",
    "            \n",
    "            \"工程技术\": {\n",
    "                \"功能导向\": {\n",
    "                    \"系统设计优化\": [\n",
    "                        # 核心概念\n",
    "                        \"设计\", \"优化\", \"系统\", \"架构\", \"方案\", \"设计方法\", \"系统工程\",\n",
    "                        # 优化方法\n",
    "                        \"遗传算法\", \"粒子群\", \"模拟退火\", \"多目标优化\", \"约束优化\",\n",
    "                        \"全局优化\", \"局部优化\", \"启发式算法\", \"智能优化\", \"进化算法\",\n",
    "                        # 设计理念\n",
    "                        \"模块化设计\", \"集成设计\", \"并行设计\", \"协同设计\", \"绿色设计\",\n",
    "                        \"可靠性设计\", \"鲁棒设计\", \"创新设计\", \"逆向设计\", \"正向设计\",\n",
    "                        # 系统类型\n",
    "                        \"机械系统\", \"电气系统\", \"控制系统\", \"信息系统\", \"网络系统\",\n",
    "                        \"分布式系统\", \"嵌入式系统\", \"复杂系统\", \"智能系统\",\n",
    "                        # 英文词汇\n",
    "                        \"design\", \"optimization\", \"system\", \"architecture\", \"engineering\",\n",
    "                        \"algorithm\", \"genetic\", \"particle\", \"swarm\", \"robust\", \"modular\"\n",
    "                    ],\n",
    "                    \"性能预测评估\": [\n",
    "                        # 核心概念\n",
    "                        \"性能\", \"预测\", \"评估\", \"效率\", \"指标\", \"性能分析\", \"效率评价\",\n",
    "                        # 性能指标\n",
    "                        \"吞吐量\", \"响应时间\", \"可靠性\", \"可用性\", \"稳定性\", \"精度\",\n",
    "                        \"功耗\", \"速度\", \"承载能力\", \"处理能力\", \"传输速率\",\n",
    "                        # 预测方法\n",
    "                        \"数学建模\", \"仿真预测\", \"回归分析\", \"机器学习\", \"深度学习\",\n",
    "                        \"时间序列\", \"统计预测\", \"经验模型\", \"物理模型\", \"混合模型\",\n",
    "                        # 评估技术\n",
    "                        \"基准测试\", \"压力测试\", \"负载测试\", \"性能监控\", \"性能调优\",\n",
    "                        \"瓶颈分析\", \"容量规划\", \"性能建模\", \"风险评估\",\n",
    "                        # 英文词汇\n",
    "                        \"performance\", \"prediction\", \"evaluation\", \"efficiency\", \"throughput\",\n",
    "                        \"reliability\", \"modeling\", \"simulation\", \"benchmark\", \"monitoring\"\n",
    "                    ],\n",
    "                    \"故障诊断监测\": [\n",
    "                        # 核心概念\n",
    "                        \"故障\", \"诊断\", \"监测\", \"检测\", \"异常\", \"故障分析\", \"健康监测\",\n",
    "                        # 故障类型\n",
    "                        \"硬件故障\", \"软件故障\", \"系统故障\", \"网络故障\", \"传感器故障\",\n",
    "                        \"执行器故障\", \"间歇性故障\", \"永久性故障\", \"渐进性故障\",\n",
    "                        # 诊断方法\n",
    "                        \"模式识别\", \"信号处理\", \"频谱分析\", \"小波分析\", \"神经网络\",\n",
    "                        \"专家系统\", \"决策树\", \"支持向量机\", \"深度学习\", \"集成学习\",\n",
    "                        # 监测技术\n",
    "                        \"在线监测\", \"离线监测\", \"实时监测\", \"连续监测\", \"周期监测\",\n",
    "                        \"振动监测\", \"温度监测\", \"压力监测\", \"声学监测\", \"光学监测\",\n",
    "                        # 英文词汇\n",
    "                        \"fault\", \"diagnosis\", \"monitoring\", \"detection\", \"anomaly\",\n",
    "                        \"failure\", \"condition\", \"health\", \"vibration\", \"acoustic\"\n",
    "                    ],\n",
    "                    \"过程控制仿真\": [\n",
    "                        # 核心概念\n",
    "                        \"控制\", \"仿真\", \"过程\", \"调节\", \"稳定\", \"过程控制\", \"自动控制\",\n",
    "                        # 控制理论\n",
    "                        \"PID控制\", \"自适应控制\", \"鲁棒控制\", \"预测控制\", \"智能控制\",\n",
    "                        \"模糊控制\", \"神经控制\", \"滑模控制\", \"最优控制\", \"非线性控制\",\n",
    "                        # 仿真技术\n",
    "                        \"数值仿真\", \"离散仿真\", \"连续仿真\", \"混合仿真\", \"实时仿真\",\n",
    "                        \"蒙特卡洛\", \"分子动力学\", \"有限元\", \"数字孪生\", \"虚拟现实\",\n",
    "                        # 过程类型\n",
    "                        \"化工过程\", \"制造过程\", \"生产过程\", \"工艺过程\", \"传输过程\",\n",
    "                        \"反应过程\", \"分离过程\", \"传热过程\", \"传质过程\", \"流动过程\",\n",
    "                        # 英文词汇\n",
    "                        \"control\", \"simulation\", \"process\", \"regulation\", \"stability\",\n",
    "                        \"adaptive\", \"robust\", \"predictive\", \"fuzzy\", \"optimal\"\n",
    "                    ],\n",
    "                    \"参数调优验证\": [\n",
    "                        # 核心概念\n",
    "                        \"参数\", \"调优\", \"验证\", \"校准\", \"标定\", \"参数优化\", \"性能调优\",\n",
    "                        # 调优方法\n",
    "                        \"网格搜索\", \"随机搜索\", \"贝叶斯优化\", \"进化算法\", \"梯度下降\",\n",
    "                        \"启发式搜索\", \"多目标优化\", \"约束优化\", \"全局搜索\", \"局部搜索\",\n",
    "                        # 验证技术\n",
    "                        \"交叉验证\", \"留一验证\", \"自助法\", \"A/B测试\", \"统计检验\",\n",
    "                        \"假设检验\", \"置信区间\", \"显著性检验\", \"效应量\", \"功效分析\",\n",
    "                        # 调优对象\n",
    "                        \"控制参数\", \"算法参数\", \"模型参数\", \"系统参数\", \"工艺参数\",\n",
    "                        \"运行参数\", \"配置参数\", \"超参数\", \"结构参数\",\n",
    "                        # 英文词汇\n",
    "                        \"parameter\", \"tuning\", \"validation\", \"calibration\", \"optimization\",\n",
    "                        \"grid\", \"search\", \"bayesian\", \"cross\", \"bootstrap\"\n",
    "                    ],\n",
    "                    \"流体动力学分析\": [\n",
    "                        # 核心概念\n",
    "                        \"流体\", \"湍流\", \"流动\", \"动力学\", \"CFD\", \"流体力学\", \"流场分析\",\n",
    "                        # 流动类型\n",
    "                        \"层流\", \"湍流\", \"过渡流\", \"可压缩流\", \"不可压缩流\", \"粘性流\",\n",
    "                        \"无粘流\", \"定常流\", \"非定常流\", \"多相流\", \"两相流\",\n",
    "                        # 分析方法\n",
    "                        \"有限差分\", \"有限元\", \"有限体积\", \"谱方法\", \"格子玻尔兹曼\",\n",
    "                        \"大涡模拟\", \"直接数值模拟\", \"雷诺平均\", \"湍流模型\",\n",
    "                        # 应用领域\n",
    "                        \"空气动力学\", \"水动力学\", \"传热传质\", \"燃烧\", \"多相流\",\n",
    "                        \"生物流体\", \"地球物理流体\", \"天体物理流体\", \"微流体\",\n",
    "                        # 英文词汇\n",
    "                        \"fluid\", \"dynamics\", \"turbulence\", \"flow\", \"computational\",\n",
    "                        \"viscous\", \"compressible\", \"simulation\", \"reynolds\", \"navier\"\n",
    "                    ],\n",
    "                    \"热传导建模\": [\n",
    "                        # 核心概念\n",
    "                        \"传热\", \"热传导\", \"温度\", \"热量\", \"散热\", \"热分析\", \"传热学\",\n",
    "                        # 传热方式\n",
    "                        \"导热\", \"对流\", \"辐射\", \"相变传热\", \"沸腾\", \"凝结\",\n",
    "                        \"自然对流\", \"强制对流\", \"混合对流\", \"层流传热\", \"湍流传热\",\n",
    "                        # 建模方法\n",
    "                        \"有限差分\", \"有限元\", \"边界元\", \"热网络\", \"集总参数\",\n",
    "                        \"分布参数\", \"数值传热\", \"解析解\", \"近似解\", \"格林函数\",\n",
    "                        # 应用场景\n",
    "                        \"电子散热\", \"建筑节能\", \"工业炉窑\", \"换热器\", \"制冷系统\",\n",
    "                        \"航空航天\", \"核反应堆\", \"太阳能\", \"地热\", \"生物传热\",\n",
    "                        # 英文词汇\n",
    "                        \"heat\", \"transfer\", \"conduction\", \"convection\", \"radiation\",\n",
    "                        \"thermal\", \"temperature\", \"cooling\", \"heating\", \"exchange\"\n",
    "                    ],\n",
    "                    \"结构力学计算\": [\n",
    "                        # 核心概念\n",
    "                        \"结构\", \"力学\", \"应力\", \"变形\", \"强度\", \"结构分析\", \"固体力学\",\n",
    "                        # 力学概念\n",
    "                        \"弹性力学\", \"塑性力学\", \"断裂力学\", \"疲劳\", \"蠕变\", \"松弛\",\n",
    "                        \"屈曲\", \"振动\", \"动力学\", \"静力学\", \"非线性\", \"大变形\",\n",
    "                        # 计算方法\n",
    "                        \"有限元\", \"边界元\", \"差分法\", \"虚功原理\", \"变分法\",\n",
    "                        \"矩阵法\", \"传递矩阵\", \"子结构\", \"超单元\", \"多尺度\",\n",
    "                        # 结构类型\n",
    "                        \"梁结构\", \"板结构\", \"壳结构\", \"框架\", \"桁架\", \"拱结构\",\n",
    "                        \"悬索\", \"薄膜\", \"复合材料\", \"智能结构\", \"生物结构\",\n",
    "                        # 英文词汇\n",
    "                        \"structural\", \"mechanics\", \"stress\", \"strain\", \"deformation\",\n",
    "                        \"elastic\", \"plastic\", \"finite\", \"element\", \"buckling\"\n",
    "                    ],\n",
    "                    \"自动化控制\": [\n",
    "                        # 核心概念\n",
    "                        \"自动化\", \"控制\", \"PID\", \"反馈\", \"调节\", \"控制系统\", \"自动控制\",\n",
    "                        # 控制类型\n",
    "                        \"开环控制\", \"闭环控制\", \"前馈控制\", \"反馈控制\", \"复合控制\",\n",
    "                        \"集中控制\", \"分散控制\", \"分布式控制\", \"层次控制\", \"智能控制\",\n",
    "                        # 控制算法\n",
    "                        \"PID\", \"模糊控制\", \"神经网络控制\", \"自适应控制\", \"预测控制\",\n",
    "                        \"滑模控制\", \"鲁棒控制\", \"最优控制\", \"线性二次\", \"卡尔曼滤波\",\n",
    "                        # 应用领域\n",
    "                        \"工业自动化\", \"过程控制\", \"运动控制\", \"机器人控制\", \"电力系统\",\n",
    "                        \"交通控制\", \"楼宇自动化\", \"家庭自动化\", \"农业自动化\",\n",
    "                        # 英文词汇\n",
    "                        \"automation\", \"control\", \"feedback\", \"regulation\", \"servo\",\n",
    "                        \"pid\", \"fuzzy\", \"adaptive\", \"predictive\", \"robust\"\n",
    "                    ],\n",
    "                    \"实验数据处理\": [\n",
    "                        # 核心概念\n",
    "                        \"实验\", \"数据处理\", \"测量\", \"采集\", \"分析\", \"实验设计\", \"数据分析\",\n",
    "                        # 数据采集\n",
    "                        \"传感器\", \"数据采集\", \"信号调理\", \"A/D转换\", \"实时采集\",\n",
    "                        \"多通道\", \"高速采集\", \"同步采集\", \"远程采集\", \"无线采集\",\n",
    "                        # 处理方法\n",
    "                        \"滤波\", \"去噪\", \"平滑\", \"插值\", \"拟合\", \"变换\", \"压缩\",\n",
    "                        \"特征提取\", \"模式识别\", \"异常检测\", \"趋势分析\", \"相关分析\",\n",
    "                        # 统计分析\n",
    "                        \"描述统计\", \"推断统计\", \"方差分析\", \"回归分析\", \"聚类分析\",\n",
    "                        \"主成分分析\", \"因子分析\", \"判别分析\", \"时间序列\", \"可靠性分析\",\n",
    "                        # 英文词汇\n",
    "                        \"experimental\", \"data\", \"processing\", \"measurement\", \"acquisition\",\n",
    "                        \"sensor\", \"filtering\", \"analysis\", \"statistical\", \"regression\"\n",
    "                    ]\n",
    "                },\n",
    "                \"应用领域\": {\n",
    "                    \"流体力学\": [\n",
    "                        \"流体\", \"湍流\", \"流动\", \"粘性\", \"雷诺\", \"层流\", \"压缩性\",\n",
    "                        \"伯努利\", \"纳维斯托克斯\", \"边界层\", \"分离\", \"涡\", \"射流\",\n",
    "                        \"尾流\", \"激波\", \"声学\", \"空化\", \"多相\", \"非牛顿流体\",\n",
    "                        \"fluid\", \"turbulent\", \"viscous\", \"reynolds\", \"laminar\",\n",
    "                        \"compressible\", \"boundary\", \"layer\", \"vortex\", \"shock\"\n",
    "                    ],\n",
    "                    \"热传递工程\": [\n",
    "                        \"传热\", \"热交换\", \"热传导\", \"对流\", \"辐射\", \"换热器\",\n",
    "                        \"散热\", \"冷却\", \"加热\", \"保温\", \"绝热\", \"相变\", \"沸腾\",\n",
    "                        \"凝结\", \"热泵\", \"制冷\", \"空调\", \"供暖\", \"节能\", \"余热\",\n",
    "                        \"heat\", \"transfer\", \"conduction\", \"convection\", \"radiation\",\n",
    "                        \"thermal\", \"cooling\", \"heating\", \"exchanger\", \"refrigeration\"\n",
    "                    ],\n",
    "                    \"结构工程\": [\n",
    "                        \"结构\", \"建筑\", \"桥梁\", \"力学\", \"材料\", \"混凝土\", \"钢结构\",\n",
    "                        \"地震\", \"抗震\", \"风荷载\", \"基础\", \"框架\", \"剪力墙\", \"预应力\",\n",
    "                        \"加固\", \"检测\", \"监测\", \"安全\", \"耐久性\", \"施工\", \"设计\",\n",
    "                        \"structural\", \"construction\", \"concrete\", \"steel\", \"seismic\",\n",
    "                        \"foundation\", \"building\", \"bridge\", \"earthquake\", \"safety\"\n",
    "                    ],\n",
    "                    \"自动化工程\": [\n",
    "                        \"自动化\", \"控制\", \"机器人\", \"传感器\", \"执行器\", \"PLC\",\n",
    "                        \"SCADA\", \"DCS\", \"现场总线\", \"工业以太网\", \"人机界面\",\n",
    "                        \"组态\", \"编程\", \"调试\", \"维护\", \"故障诊断\", \"系统集成\",\n",
    "                        \"automation\", \"control\", \"robot\", \"sensor\", \"actuator\",\n",
    "                        \"programmable\", \"logic\", \"controller\", \"scada\", \"fieldbus\"\n",
    "                    ],\n",
    "                    \"计算工程\": [\n",
    "                        \"计算\", \"数值\", \"仿真\", \"模拟\", \"求解\", \"有限元\", \"有限差分\",\n",
    "                        \"边界元\", \"网格\", \"离散\", \"迭代\", \"收敛\", \"精度\", \"误差\",\n",
    "                        \"并行计算\", \"高性能\", \"算法\", \"软件\", \"建模\", \"验证\",\n",
    "                        \"computational\", \"numerical\", \"simulation\", \"finite\", \"element\",\n",
    "                        \"mesh\", \"solver\", \"algorithm\", \"parallel\", \"verification\"\n",
    "                    ],\n",
    "                    \"机械工程\": [\n",
    "                        \"机械\", \"机器\", \"零件\", \"制造\", \"加工\", \"设计\", \"材料\",\n",
    "                        \"强度\", \"刚度\", \"疲劳\", \"磨损\", \"润滑\", \"振动\", \"噪声\",\n",
    "                        \"精度\", \"公差\", \"装配\", \"维修\", \"可靠性\", \"寿命\", \"优化\",\n",
    "                        \"mechanical\", \"machine\", \"manufacturing\", \"machining\", \"design\",\n",
    "                        \"material\", \"strength\", \"fatigue\", \"vibration\", \"precision\"\n",
    "                    ],\n",
    "                    \"化工工程\": [\n",
    "                        \"化工\", \"反应器\", \"分离\", \"精馏\", \"催化\", \"传质\", \"传热\",\n",
    "                        \"流体\", \"工艺\", \"设备\", \"管道\", \"泵\", \"压缩机\", \"换热器\",\n",
    "                        \"塔器\", \"反应\", \"动力学\", \"热力学\", \"相平衡\", \"安全\",\n",
    "                        \"chemical\", \"reactor\", \"separation\", \"distillation\", \"catalyst\",\n",
    "                        \"mass\", \"transfer\", \"process\", \"equipment\", \"thermodynamics\"\n",
    "                    ],\n",
    "                    \"航空航天\": [\n",
    "                        \"航空\", \"航天\", \"飞行\", \"火箭\", \"卫星\", \"飞机\", \"发动机\",\n",
    "                        \"推进\", \"空气动力学\", \"轨道\", \"制导\", \"导航\", \"控制\",\n",
    "                        \"材料\", \"结构\", \"系统\", \"可靠性\", \"安全\", \"测试\", \"验证\",\n",
    "                        \"aerospace\", \"aircraft\", \"rocket\", \"satellite\", \"propulsion\",\n",
    "                        \"aerodynamics\", \"orbit\", \"guidance\", \"navigation\", \"flight\"\n",
    "                    ],\n",
    "                    \"材料工程\": [\n",
    "                        \"材料\", \"金属\", \"复合材料\", \"陶瓷\", \"聚合物\", \"性能\",\n",
    "                        \"结构\", \"相变\", \"热处理\", \"表面\", \"涂层\", \"腐蚀\", \"防护\",\n",
    "                        \"制备\", \"合成\", \"表征\", \"测试\", \"分析\", \"设计\", \"应用\",\n",
    "                        \"material\", \"metal\", \"composite\", \"ceramic\", \"polymer\",\n",
    "                        \"properties\", \"structure\", \"surface\", \"corrosion\", \"synthesis\"\n",
    "                    ],\n",
    "                    \"控制工程\": [\n",
    "                        \"控制\", \"系统\", \"反馈\", \"稳定性\", \"调节\", \"控制器\", \"传递函数\",\n",
    "                        \"状态空间\", \"频域\", \"时域\", \"根轨迹\", \"奈奎斯特\", \"伯德图\",\n",
    "                        \"最优\", \"鲁棒\", \"自适应\", \"预测\", \"模糊\", \"神经\", \"智能\",\n",
    "                        \"control\", \"system\", \"feedback\", \"stability\", \"controller\",\n",
    "                        \"transfer\", \"function\", \"optimal\", \"robust\", \"adaptive\"\n",
    "                    ]\n",
    "                },\n",
    "                \"数据类型\": [\n",
    "                    \"仿真数据\", \"实验数据\", \"传感器数据\", \"时序数据\", \"几何数据\",\n",
    "                    \"网格数据\", \"图像数据\", \"信号数据\", \"测量数据\", \"监测数据\",\n",
    "                    \"控制数据\", \"运行数据\", \"故障数据\", \"性能数据\", \"设计数据\",\n",
    "                    \"工艺数据\", \"制造数据\", \"测试数据\", \"校准数据\", \"参数数据\"\n",
    "                ]\n",
    "\n",
    "            },\n",
    "\n",
    "        \"物质科学\": {\n",
    "            \"功能导向\": {\n",
    "                \"分子性质预测\": [\n",
    "                    # 核心词汇\n",
    "                    \"性质\", \"预测\", \"分子性质\", \"物化性质\", \"理化性质\", \"性质预测\",\n",
    "                    # QSAR相关\n",
    "                    \"QSAR\", \"QSPR\", \"构效关系\", \"结构活性\", \"定量构效\", \"机器学习\",\n",
    "                    # 物理化学性质\n",
    "                    \"溶解度\", \"沸点\", \"熔点\", \"密度\", \"粘度\", \"表面张力\", \"蒸汽压\",\n",
    "                    \"分配系数\", \"辛醇水分配\", \"logP\", \"pKa\", \"极性\", \"疏水性\",\n",
    "                    # 分子描述符\n",
    "                    \"分子描述符\", \"指纹\", \"拓扑指数\", \"电子性质\", \"几何性质\", \"立体性质\",\n",
    "                    \"原子贡献\", \"基团贡献\", \"片段\", \"药效团\", \"原子坐标\",\n",
    "                    # 计算方法\n",
    "                    \"密度泛函\", \"DFT\", \"半经验\", \"分子动力学\", \"蒙特卡洛\", \"量子化学\",\n",
    "                    \"神经网络\", \"支持向量机\", \"随机森林\", \"深度学习\", \"集成学习\",\n",
    "                    # 英文词汇\n",
    "                    \"property\", \"prediction\", \"QSAR\", \"descriptor\", \"molecular\",\n",
    "                    \"solubility\", \"LogP\", \"fingerprint\", \"machine\", \"learning\"\n",
    "                ],\n",
    "                \"化学反应预测\": [\n",
    "                    # 核心词汇\n",
    "                    \"反应\", \"预测\", \"机理\", \"反应机理\", \"反应路径\", \"反应预测\",\n",
    "                    # 反应类型\n",
    "                    \"有机反应\", \"无机反应\", \"催化反应\", \"酶反应\", \"光化学反应\",\n",
    "                    \"电化学反应\", \"聚合反应\", \"偶联反应\", \"环化反应\", \"开环反应\",\n",
    "                    # 反应特征\n",
    "                    \"产物\", \"收率\", \"选择性\", \"立体选择性\", \"区域选择性\", \"化学选择性\",\n",
    "                    \"反应条件\", \"温度\", \"压力\", \"溶剂\", \"pH\", \"反应时间\",\n",
    "                    # 动力学热力学\n",
    "                    \"反应速率\", \"活化能\", \"反应焓\", \"反应熵\", \"吉布斯自由能\",\n",
    "                    \"平衡常数\", \"速率常数\", \"阿伦尼乌斯\", \"过渡态\", \"中间体\",\n",
    "                    # 计算方法\n",
    "                    \"反应数据库\", \"反应模板\", \"图神经网络\", \"变换器\", \"注意机制\",\n",
    "                    \"逆合成\", \"正向合成\", \"多步反应\", \"反应规划\", \"合成树\",\n",
    "                    # 英文词汇\n",
    "                    \"reaction\", \"mechanism\", \"yield\", \"selectivity\", \"catalysis\",\n",
    "                    \"synthesis\", \"kinetics\", \"thermodynamics\", \"activation\", \"energy\"\n",
    "                ],\n",
    "                \"材料结构设计\": [\n",
    "                    # 核心词汇\n",
    "                    \"材料\", \"设计\", \"结构\", \"结构设计\", \"材料设计\", \"材料工程\",\n",
    "                    # 材料类型\n",
    "                    \"晶体\", \"非晶\", \"多晶\", \"单晶\", \"纳米材料\", \"二维材料\",\n",
    "                    \"复合材料\", \"金属材料\", \"陶瓷材料\", \"聚合物材料\", \"功能材料\",\n",
    "                    # 结构特征\n",
    "                    \"晶格\", \"对称\", \"空间群\", \"点群\", \"缺陷\", \"位错\", \"界面\",\n",
    "                    \"孔隙\", \"多孔\", \"介孔\", \"微孔\", \"表面\", \"形貌\", \"织构\",\n",
    "                    # 性能关系\n",
    "                    \"结构性能\", \"构效关系\", \"性能优化\", \"多目标优化\", \"设计准则\",\n",
    "                    \"性能预测\", \"力学性能\", \"电学性能\", \"磁学性能\", \"光学性能\",\n",
    "                    # 设计方法\n",
    "                    \"计算材料学\", \"高通量计算\", \"材料基因组\", \"机器学习\", \"人工智能\",\n",
    "                    \"遗传算法\", \"进化算法\", \"拓扑优化\", \"逆向设计\", \"数据驱动\",\n",
    "                    # 英文词汇\n",
    "                    \"material\", \"design\", \"structure\", \"crystal\", \"lattice\",\n",
    "                    \"property\", \"optimization\", \"computational\", \"high-throughput\", \"genome\"\n",
    "                ],\n",
    "                \"催化活性筛选\": [\n",
    "                    # 核心词汇\n",
    "                    \"催化\", \"活性\", \"筛选\", \"催化剂\", \"催化活性\", \"活性筛选\",\n",
    "                    # 催化剂类型\n",
    "                    \"均相催化\", \"多相催化\", \"酶催化\", \"光催化\", \"电催化\",\n",
    "                    \"单原子催化\", \"纳米催化\", \"负载催化\", \"分子催化\", \"仿生催化\",\n",
    "                    # 活性指标\n",
    "                    \"选择性\", \"转化率\", \"活性\", \"稳定性\", \"TOF\", \"TON\",\n",
    "                    \"催化效率\", \"反应活性\", \"立体选择性\", \"化学选择性\", \"区域选择性\",\n",
    "                    # 反应类型\n",
    "                    \"氧化反应\", \"还原反应\", \"氢化反应\", \"脱氢反应\", \"异构化\",\n",
    "                    \"聚合反应\", \"裂解反应\", \"重整反应\", \"烷基化\", \"酰化反应\",\n",
    "                    # 催化机理\n",
    "                    \"活性中心\", \"反应位点\", \"吸附\", \"活化\", \"脱附\", \"毒化\",\n",
    "                    \"载体效应\", \"尺寸效应\", \"电子效应\", \"配体效应\", \"协同效应\",\n",
    "                    # 筛选方法\n",
    "                    \"高通量筛选\", \"虚拟筛选\", \"组合化学\", \"并行合成\", \"自动化\",\n",
    "                    \"机器学习\", \"数据挖掘\", \"活性预测\", \"描述符\", \"特征工程\",\n",
    "                    # 英文词汇\n",
    "                    \"catalysis\", \"catalyst\", \"activity\", \"selectivity\", \"screening\",\n",
    "                    \"conversion\", \"turnover\", \"heterogeneous\", \"homogeneous\", \"enzyme\"\n",
    "                ],\n",
    "                \"药物分子生成\": [\n",
    "                    # 核心词汇\n",
    "                    \"药物\", \"分子\", \"生成\", \"药物设计\", \"分子设计\", \"药物发现\",\n",
    "                    # 药物类型\n",
    "                    \"小分子\", \"大分子\", \"天然产物\", \"合成化合物\", \"先导化合物\",\n",
    "                    \"候选药物\", \"原创药\", \"仿制药\", \"生物药\", \"抗体药物\",\n",
    "                    # 设计方法\n",
    "                    \"基于结构\", \"基于配体\", \"分子对接\", \"药效团\", \"虚拟筛选\",\n",
    "                    \"从头设计\", \"片段组装\", \"支架跳跃\", \"分子优化\", \"先导优化\",\n",
    "                    # 生成算法\n",
    "                    \"深度学习\", \"生成对抗网络\", \"变分自编码器\", \"强化学习\",\n",
    "                    \"图神经网络\", \"序列模型\", \"SMILES\", \"分子图\", \"原子贡献\",\n",
    "                    # 药物性质\n",
    "                    \"ADMET\", \"药代动力学\", \"药效学\", \"毒性\", \"安全性\",\n",
    "                    \"生物利用度\", \"血脑屏障\", \"代谢稳定性\", \"溶解度\", \"渗透性\",\n",
    "                    # 靶点相关\n",
    "                    \"靶点\", \"结合亲和力\", \"选择性\", \"抑制剂\", \"激动剂\",\n",
    "                    \"拮抗剂\", \"变构调节\", \"多靶点\", \"靶点验证\", \"脱靶效应\",\n",
    "                    # 英文词汇\n",
    "                    \"drug\", \"molecule\", \"generation\", \"design\", \"discovery\",\n",
    "                    \"lead\", \"compound\", \"ADMET\", \"target\", \"binding\", \"affinity\"\n",
    "                ],\n",
    "                \"合成路径规划\": [\n",
    "                    # 核心词汇\n",
    "                    \"合成\", \"路径\", \"路线\", \"合成路径\", \"合成路线\", \"路径规划\",\n",
    "                    # 合成策略\n",
    "                    \"逆合成\", \"正向合成\", \"汇聚合成\", \"线性合成\", \"多组分反应\",\n",
    "                    \"一锅反应\", \"串联反应\", \"多米诺反应\", \"级联反应\", \"协同合成\",\n",
    "                    # 规划方法\n",
    "                    \"步骤\", \"策略\", \"合成策略\", \"反应序列\", \"合成树\", \"反应网络\",\n",
    "                    \"路径搜索\", \"最短路径\", \"最优路径\", \"成本分析\", \"风险评估\",\n",
    "                    # 起始原料\n",
    "                    \"起始原料\", \"商业可得\", \"建构单元\", \"合成砌块\", \"手性池\",\n",
    "                    \"天然产物\", \"廉价原料\", \"可再生原料\", \"绿色原料\", \"工业原料\",\n",
    "                    # 反应条件\n",
    "                    \"反应条件\", \"试剂\", \"催化剂\", \"溶剂\", \"温度\", \"时间\",\n",
    "                    \"收率\", \"纯度\", \"工艺\", \"放大\", \"工业化\", \"经济性\",\n",
    "                    # 计算工具\n",
    "                    \"反应数据库\", \"合成软件\", \"人工智能\", \"机器学习\", \"专家系统\",\n",
    "                    \"算法\", \"启发式\", \"蒙特卡洛\", \"树搜索\", \"神经网络\",\n",
    "                    # 英文词汇\n",
    "                    \"synthesis\", \"route\", \"pathway\", \"retrosynthesis\", \"planning\",\n",
    "                    \"strategy\", \"reaction\", \"sequence\", \"tree\", \"search\", \"algorithm\"\n",
    "                ],\n",
    "                \"晶体结构优化\": [\n",
    "                    # 核心词汇\n",
    "                    \"晶体\", \"优化\", \"结构优化\", \"晶体结构\", \"晶格优化\", \"结构弛豫\",\n",
    "                    # 晶体学基础\n",
    "                    \"晶格\", \"晶胞\", \"原胞\", \"布拉维格子\", \"对称\", \"空间群\",\n",
    "                    \"点群\", \"晶系\", \"晶族\", \"米勒指数\", \"倒格子\", \"布里渊区\",\n",
    "                    # 结构参数\n",
    "                    \"晶格参数\", \"晶格常数\", \"键长\", \"键角\", \"配位数\", \"原子坐标\",\n",
    "                    \"占据度\", \"热参数\", \"位移参数\", \"结构因子\", \"衍射强度\",\n",
    "                    # 相变与多晶型\n",
    "                    \"相\", \"相变\", \"多晶型\", \"同质多晶\", \"构象多晶\", \"溶剂化合物\",\n",
    "                    \"水合物\", \"共晶\", \"固溶体\", \"超分子\", \"主客体化合物\",\n",
    "                    # 能量与稳定性\n",
    "                    \"晶格能\", \"内聚能\", \"形成能\", \"表面能\", \"缺陷形成能\",\n",
    "                    \"热力学稳定性\", \"动力学稳定性\", \"亚稳态\", \"能量地貌\", \"全局最小\",\n",
    "                    # 优化方法\n",
    "                    \"结构预测\", \"晶体结构预测\", \"从头预测\", \"全局优化\", \"局部优化\",\n",
    "                    \"遗传算法\", \"模拟退火\", \"粒子群\", \"差分进化\", \"盆地跳跃\",\n",
    "                    # 英文词汇\n",
    "                    \"crystal\", \"optimization\", \"lattice\", \"symmetry\", \"space\",\n",
    "                    \"group\", \"polymorphism\", \"prediction\", \"energy\", \"minimization\"\n",
    "                ],\n",
    "                \"光谱特征分析\": [\n",
    "                    # 核心词汇\n",
    "                    \"光谱\", \"特征\", \"分析\", \"光谱分析\", \"光谱特征\", \"谱学分析\",\n",
    "                    # 光谱类型\n",
    "                    \"红外光谱\", \"拉曼光谱\", \"紫外可见\", \"核磁共振\", \"质谱\",\n",
    "                    \"X射线\", \"电子能谱\", \"荧光光谱\", \"发光光谱\", \"圆二色谱\",\n",
    "                    # 光谱特征\n",
    "                    \"峰\", \"谱峰\", \"吸收\", \"发射\", \"振动\", \"转动\", \"电子跃迁\",\n",
    "                    \"化学位移\", \"耦合常数\", \"峰强度\", \"峰面积\", \"峰位\", \"峰形\",\n",
    "                    # 振动光谱\n",
    "                    \"伸缩振动\", \"弯曲振动\", \"面内振动\", \"面外振动\", \"骨架振动\",\n",
    "                    \"特征频率\", \"指纹区\", \"泛频\", \"组频\", \"费米共振\", \"倍频\",\n",
    "                    # NMR相关\n",
    "                    \"1H NMR\", \"13C NMR\", \"二维NMR\", \"多维NMR\", \"化学交换\",\n",
    "                    \"NOE\", \"COSY\", \"HSQC\", \"HMBC\", \"弛豫时间\", \"自旋耦合\",\n",
    "                    # 质谱相关\n",
    "                    \"分子离子峰\", \"基峰\", \"碎片离子\", \"同位素峰\", \"准分子离子\",\n",
    "                    \"离子化\", \"电喷雾\", \"MALDI\", \"质量精度\", \"分辨率\", \"碰撞解离\",\n",
    "                    # 分析应用\n",
    "                    \"结构鉴定\", \"定性分析\", \"定量分析\", \"纯度分析\", \"构型确定\",\n",
    "                    \"构象分析\", \"相互作用\", \"动力学\", \"反应监测\", \"过程分析\",\n",
    "                    # 英文词汇\n",
    "                    \"spectroscopy\", \"spectrum\", \"infrared\", \"raman\", \"NMR\",\n",
    "                    \"mass\", \"peak\", \"vibration\", \"chemical\", \"shift\", \"analysis\"\n",
    "                ],\n",
    "                \"热力学性质计算\": [\n",
    "                    # 核心词汇\n",
    "                    \"热力学\", \"性质\", \"计算\", \"热力学性质\", \"热力学计算\", \"热化学\",\n",
    "                    # 基本性质\n",
    "                    \"焓\", \"熵\", \"自由能\", \"内能\", \"热容\", \"吉布斯自由能\",\n",
    "                    \"亥姆霍兹自由能\", \"化学势\", \"活度\", \"活度系数\", \"逸度\",\n",
    "                    # 反应热力学\n",
    "                    \"形成焓\", \"燃烧焓\", \"升华焓\", \"熔化焓\", \"汽化焓\", \"溶解焓\",\n",
    "                    \"稀释焓\", \"混合焓\", \"反应焓\", \"活化焓\", \"结合焓\", \"解离焓\",\n",
    "                    # 相平衡\n",
    "                    \"平衡\", \"相平衡\", \"化学平衡\", \"平衡常数\", \"分配系数\",\n",
    "                    \"溶解度\", \"饱和蒸汽压\", \"沸点\", \"熔点\", \"临界点\", \"三相点\",\n",
    "                    # 溶液热力学\n",
    "                    \"溶液\", \"理想溶液\", \"正规溶液\", \"活度模型\", \"状态方程\",\n",
    "                    \"混合规则\", \"过量性质\", \"部分摩尔性质\", \"表观摩尔性质\",\n",
    "                    # 统计热力学\n",
    "                    \"配分函数\", \"玻尔兹曼分布\", \"分子配分函数\", \"振动配分函数\",\n",
    "                    \"转动配分函数\", \"平动配分函数\", \"电子配分函数\", \"统计权重\",\n",
    "                    # 计算方法\n",
    "                    \"量子化学\", \"密度泛函\", \"从头计算\", \"半经验\", \"分子动力学\",\n",
    "                    \"蒙特卡洛\", \"群贡献法\", \"Benson方法\", \"Joback方法\", \"UNIFAC\",\n",
    "                    # 英文词汇\n",
    "                    \"thermodynamics\", \"enthalpy\", \"entropy\", \"gibbs\", \"free\",\n",
    "                    \"energy\", \"equilibrium\", \"partition\", \"function\", \"calculation\"\n",
    "                ],\n",
    "                \"毒性安全评估\": [\n",
    "                    # 核心词汇\n",
    "                    \"毒性\", \"安全\", \"评估\", \"毒理学\", \"安全评估\", \"风险评估\",\n",
    "                    # 毒性类型\n",
    "                    \"急性毒性\", \"慢性毒性\", \"亚急性毒性\", \"亚慢性毒性\", \"遗传毒性\",\n",
    "                    \"致癌性\", \"致畸性\", \"致突变性\", \"生殖毒性\", \"发育毒性\",\n",
    "                    # ADMET性质\n",
    "                    \"ADMET\", \"吸收\", \"分布\", \"代谢\", \"排泄\", \"毒性\",\n",
    "                    \"生物利用度\", \"血浆蛋白结合\", \"血脑屏障\", \"肝毒性\", \"肾毒性\",\n",
    "                    # 毒性终点\n",
    "                    \"LD50\", \"LC50\", \"NOEL\", \"NOAEL\", \"LOEL\", \"LOAEL\",\n",
    "                    \"基准剂量\", \"安全剂量\", \"每日允许摄入量\", \"参考剂量\", \"阈值\",\n",
    "                    # 评估方法\n",
    "                    \"副作用\", \"不良反应\", \"药物相互作用\", \"禁忌症\", \"警告\",\n",
    "                    \"体外试验\", \"体内试验\", \"细胞毒性\", \"器官毒性\", \"系统毒性\",\n",
    "                    # 替代方法\n",
    "                    \"QSAR\", \"Read-across\", \"体外替代\", \"计算毒理学\", \"系统毒理学\",\n",
    "                    \"毒性预测\", \"机器学习\", \"人工智能\", \"专家系统\", \"决策树\",\n",
    "                    # 法规标准\n",
    "                    \"GHS\", \"REACH\", \"OECD\", \"ICH\", \"FDA\", \"EMA\",\n",
    "                    \"毒理学指导原则\", \"安全性评价\", \"风险管理\", \"风险控制\", \"监管\",\n",
    "                    # 英文词汇\n",
    "                    \"toxicity\", \"safety\", \"assessment\", \"ADMET\", \"adverse\",\n",
    "                    \"effect\", \"risk\", \"evaluation\", \"regulatory\", \"guidelines\"\n",
    "                ]\n",
    "            },\n",
    "            \"应用领域\": {\n",
    "                \"药物化学\": [\n",
    "                    # 核心概念\n",
    "                    \"药物\", \"制药\", \"医药\", \"药化\", \"药物化学\", \"医药化学\",\n",
    "                    # 药物类型\n",
    "                    \"活性\", \"药效\", \"药理\", \"药物活性\", \"生物活性\", \"药效学\",\n",
    "                    \"小分子药物\", \"大分子药物\", \"生物药\", \"化学药\", \"天然药物\",\n",
    "                    # 先导化合物\n",
    "                    \"先导\", \"先导化合物\", \"候选药物\", \"活性化合物\", \"药物前体\",\n",
    "                    \"原型药物\", \"母体化合物\", \"活性分子\", \"药物分子\", \"化合物库\",\n",
    "                    # 药物发现\n",
    "                    \"药物发现\", \"药物设计\", \"药物开发\", \"新药研发\", \"创新药\",\n",
    "                    \"靶点发现\", \"靶点验证\", \"高通量筛选\", \"虚拟筛选\", \"表型筛选\",\n",
    "                    # 药物优化\n",
    "                    \"先导优化\", \"结构优化\", \"活性优化\", \"选择性优化\", \"ADMET优化\",\n",
    "                    \"构效关系\", \"SAR\", \"QSAR\", \"分子修饰\", \"结构改造\",\n",
    "                    # 药理作用\n",
    "                    \"受体\", \"酶抑制剂\", \"激动剂\", \"拮抗剂\", \"变构调节\",\n",
    "                    \"信号通路\", \"作用机制\", \"药理机制\", \"分子机制\", \"靶点结合\",\n",
    "                    # 英文词汇\n",
    "                    \"medicinal\", \"chemistry\", \"pharmaceutical\", \"drug\", \"discovery\",\n",
    "                    \"lead\", \"compound\", \"bioactive\", \"pharmacology\", \"receptor\"\n",
    "                ],\n",
    "                \"催化科学\": [\n",
    "                    # 催化基础\n",
    "                    \"催化\", \"催化剂\", \"催化反应\", \"催化科学\", \"催化化学\",\n",
    "                    # 催化类型\n",
    "                    \"反应\", \"机理\", \"催化机理\", \"反应机理\", \"催化循环\",\n",
    "                    \"均相催化\", \"多相催化\", \"酶催化\", \"仿生催化\", \"光催化\",\n",
    "                    # 催化性能\n",
    "                    \"活性\", \"催化活性\", \"反应活性\", \"催化效率\", \"催化性能\",\n",
    "                    \"选择性\", \"立体选择性\", \"化学选择性\", \"区域选择性\", \"尺寸选择性\",\n",
    "                    # 催化剂设计\n",
    "                    \"催化剂设计\", \"活性中心\", \"反应位点\", \"载体\", \"负载\",\n",
    "                    \"修饰\", \"改性\", \"掺杂\", \"合金\", \"单原子催化剂\",\n",
    "                    # 催化应用\n",
    "                    \"石油化工\", \"精细化工\", \"环境催化\", \"能源催化\", \"绿色催化\",\n",
    "                    \"工业催化\", \"有机合成\", \"聚合催化\", \"电催化\", \"光电催化\",\n",
    "                    # 表征分析\n",
    "                    \"表征\", \"结构表征\", \"性能评价\", \"原位表征\", \"催化剂失活\",\n",
    "                    \"再生\", \"稳定性\", \"寿命\", \"中毒\", \"烧结\", \"积碳\",\n",
    "                    # 英文词汇\n",
    "                    \"catalysis\", \"catalyst\", \"catalytic\", \"reaction\", \"mechanism\",\n",
    "                    \"selectivity\", \"activity\", \"heterogeneous\", \"homogeneous\", \"enzyme\"\n",
    "                ],\n",
    "                \"材料科学\": [\n",
    "                    # 材料基础\n",
    "                    \"材料\", \"材料科学\", \"材料工程\", \"材料物理\", \"材料化学\",\n",
    "                    # 材料分类\n",
    "                    \"金属\", \"金属材料\", \"合金\", \"钢铁\", \"有色金属\",\n",
    "                    \"陶瓷\", \"陶瓷材料\", \"氧化物陶瓷\", \"非氧化物陶瓷\", \"功能陶瓷\",\n",
    "                    \"聚合物\", \"高分子\", \"塑料\", \"橡胶\", \"纤维\", \"涂料\",\n",
    "                    \"复合\", \"复合材料\", \"纤维增强\", \"颗粒增强\", \"层状复合\",\n",
    "                    # 纳米材料\n",
    "                    \"纳米\", \"纳米材料\", \"纳米粒子\", \"纳米结构\", \"纳米技术\",\n",
    "                    \"量子点\", \"纳米管\", \"纳米线\", \"纳米片\", \"二维材料\",\n",
    "                    # 功能材料\n",
    "                    \"功能材料\", \"智能材料\", \"形状记忆\", \"超导材料\", \"磁性材料\",\n",
    "                    \"压电材料\", \"铁电材料\", \"光电材料\", \"热电材料\", \"储能材料\",\n",
    "                    # 材料性能\n",
    "                    \"力学性能\", \"电学性能\", \"磁学性能\", \"光学性能\", \"热学性能\",\n",
    "                    \"强度\", \"韧性\", \"硬度\", \"弹性\", \"塑性\", \"导电性\", \"绝缘性\",\n",
    "                    # 英文词汇\n",
    "                    \"material\", \"science\", \"metal\", \"ceramic\", \"polymer\",\n",
    "                    \"composite\", \"nanomaterial\", \"functional\", \"properties\", \"structure\"\n",
    "                ],\n",
    "                \"有机化学\": [\n",
    "                    # 有机基础\n",
    "                    \"有机\", \"有机化学\", \"有机合成\", \"有机分子\", \"有机反应\",\n",
    "                    # 碳化学\n",
    "                    \"碳\", \"碳化合物\", \"烃\", \"脂肪烃\", \"芳香烃\", \"杂环化合物\",\n",
    "                    \"碳链\", \"碳骨架\", \"碳环\", \"苯环\", \"杂环\", \"稠环\",\n",
    "                    # 官能团\n",
    "                    \"官能团\", \"烷基\", \"烯基\", \"炔基\", \"苯基\", \"羟基\",\n",
    "                    \"羰基\", \"羧基\", \"氨基\", \"硝基\", \"卤代\", \"醚键\",\n",
    "                    # 有机反应\n",
    "                    \"反应\", \"加成反应\", \"消除反应\", \"取代反应\", \"重排反应\",\n",
    "                    \"氧化反应\", \"还原反应\", \"偶联反应\", \"环化反应\", \"开环反应\",\n",
    "                    # 立体化学\n",
    "                    \"立体化学\", \"手性\", \"对映体\", \"非对映体\", \"构型\", \"构象\",\n",
    "                    \"顺反异构\", \"光学活性\", \"旋光性\", \"绝对构型\", \"相对构型\",\n",
    "                    # 合成策略\n",
    "                    \"合成\", \"全合成\", \"不对称合成\", \"立体选择性合成\", \"区域选择性\",\n",
    "                    \"保护基\", \"脱保护\", \"官能团转化\", \"碳碳键形成\", \"杂原子引入\",\n",
    "                    # 英文词汇\n",
    "                    \"organic\", \"chemistry\", \"carbon\", \"functional\", \"group\",\n",
    "                    \"reaction\", \"synthesis\", \"stereochemistry\", \"chiral\", \"aromatic\"\n",
    "                ],\n",
    "                \"无机化学\": [\n",
    "                    # 无机基础\n",
    "                    \"无机\", \"无机化学\", \"无机物\", \"无机材料\", \"无机合成\",\n",
    "                    # 金属化学\n",
    "                    \"金属\", \"过渡金属\", \"主族金属\", \"稀土金属\", \"贵金属\",\n",
    "                    \"金属离子\", \"金属原子\", \"金属簇\", \"金属键\", \"金属性\",\n",
    "                    # 配位化学\n",
    "                    \"配合物\", \"配位化合物\", \"配体\", \"螯合物\", \"络合物\",\n",
    "                    \"配位数\", \"配位几何\", \"晶体场\", \"配体场\", \"电子结构\",\n",
    "                    # 离子化合物\n",
    "                    \"离子\", \"阳离子\", \"阴离子\", \"离子键\", \"离子晶体\",\n",
    "                    \"盐\", \"酸\", \"碱\", \"氧化物\", \"氢氧化物\", \"硫化物\",\n",
    "                    # 固体化学\n",
    "                    \"晶体\", \"晶体结构\", \"晶格\", \"点缺陷\", \"线缺陷\", \"面缺陷\",\n",
    "                    \"固溶体\", \"相变\", \"超导\", \"磁性\", \"铁电性\", \"压电性\",\n",
    "                    # 材料应用\n",
    "                    \"陶瓷\", \"玻璃\", \"水泥\", \"耐火材料\", \"半导体\", \"超导体\",\n",
    "                    \"磁性材料\", \"发光材料\", \"催化材料\", \"储氢材料\", \"电池材料\",\n",
    "                    # 英文词汇\n",
    "                    \"inorganic\", \"chemistry\", \"metal\", \"complex\", \"coordination\",\n",
    "                    \"ion\", \"crystal\", \"solid\", \"state\", \"material\", \"ceramic\"\n",
    "                ],\n",
    "                \"物理化学\": [\n",
    "                    # 基础概念\n",
    "                    \"物理化学\", \"物化\", \"理论化学\", \"化学物理\", \"分子物理\",\n",
    "                    # 热力学\n",
    "                    \"热力学\", \"热化学\", \"化学热力学\", \"统计热力学\", \"热力学函数\",\n",
    "                    \"状态函数\", \"过程函数\", \"热力学定律\", \"可逆过程\", \"不可逆过程\",\n",
    "                    # 动力学\n",
    "                    \"动力学\", \"反应动力学\", \"化学动力学\", \"反应速率\", \"速率方程\",\n",
    "                    \"反应级数\", \"速率常数\", \"活化能\", \"阿伦尼乌斯方程\", \"反应机理\",\n",
    "                    # 量子化学\n",
    "                    \"量子\", \"量子化学\", \"量子力学\", \"分子轨道\", \"原子轨道\",\n",
    "                    \"薛定谔方程\", \"波函数\", \"哈密顿算符\", \"本征值\", \"本征函数\",\n",
    "                    # 电化学\n",
    "                    \"电化学\", \"电极\", \"电解\", \"腐蚀\", \"电池\", \"燃料电池\",\n",
    "                    \"电极电位\", \"能斯特方程\", \"法拉第定律\", \"电导\", \"电迁移\",\n",
    "                    # 表面化学\n",
    "                    \"表面\", \"界面\", \"吸附\", \"脱附\", \"表面张力\", \"润湿\",\n",
    "                    \"胶体\", \"乳液\", \"泡沫\", \"凝胶\", \"自组装\", \"LB膜\",\n",
    "                    # 英文词汇\n",
    "                    \"physical\", \"chemistry\", \"thermodynamics\", \"kinetics\", \"quantum\",\n",
    "                    \"electrochemistry\", \"surface\", \"interface\", \"adsorption\", \"colloid\"\n",
    "                ],\n",
    "                \"计算化学\": [\n",
    "                    # 基础概念\n",
    "                    \"计算\", \"计算化学\", \"理论化学\", \"计算模拟\", \"分子模拟\",\n",
    "                    # 量子化学方法\n",
    "                    \"量子\", \"量子化学\", \"从头计算\", \"ab initio\", \"密度泛函\",\n",
    "                    \"DFT\", \"Hartree-Fock\", \"HF\", \"MP2\", \"CCSD\", \"CI\",\n",
    "                    # 分子动力学\n",
    "                    \"分子动力学\", \"MD\", \"蒙特卡洛\", \"MC\", \"分子力学\", \"力场\",\n",
    "                    \"经典动力学\", \"量子动力学\", \"Car-Parrinello\", \"BOMD\", \"AIMD\",\n",
    "                    # 软件工具\n",
    "                    \"Gaussian\", \"VASP\", \"CASTEP\", \"Materials Studio\", \"LAMMPS\",\n",
    "                    \"GROMACS\", \"AMBER\", \"CHARMM\", \"NAMD\", \"Quantum Espresso\",\n",
    "                    # 计算内容\n",
    "                    \"几何优化\", \"频率计算\", \"能量计算\", \"轨道分析\", \"电荷分析\",\n",
    "                    \"振动分析\", \"光谱计算\", \"反应路径\", \"过渡态\", \"IRC\",\n",
    "                    # 应用领域\n",
    "                    \"药物设计\", \"材料设计\", \"催化机理\", \"反应机理\", \"性质预测\",\n",
    "                    \"构效关系\", \"分子识别\", \"超分子\", \"生物大分子\", \"蛋白质折叠\",\n",
    "                    # 英文词汇\n",
    "                    \"computational\", \"chemistry\", \"quantum\", \"DFT\", \"molecular\",\n",
    "                    \"dynamics\", \"simulation\", \"calculation\", \"optimization\", \"theory\"\n",
    "                ],\n",
    "                \"高分子科学\": [\n",
    "                    # 高分子基础\n",
    "                    \"高分子\", \"聚合物\", \"大分子\", \"高分子化学\", \"高分子物理\",\n",
    "                    # 聚合物类型\n",
    "                    \"塑料\", \"合成树脂\", \"热塑性\", \"热固性\", \"弹性体\",\n",
    "                    \"橡胶\", \"天然橡胶\", \"合成橡胶\", \"硅橡胶\", \"丁苯橡胶\",\n",
    "                    \"纤维\", \"合成纤维\", \"尼龙\", \"聚酯\", \"丙纶\", \"腈纶\",\n",
    "                    # 聚合反应\n",
    "                    \"聚合\", \"聚合反应\", \"加聚\", \"缩聚\", \"开环聚合\", \"配位聚合\",\n",
    "                    \"自由基聚合\", \"离子聚合\", \"活性聚合\", \"可控聚合\", \"ATRP\", \"RAFT\",\n",
    "                    # 分子结构\n",
    "                    \"分子量\", \"分子量分布\", \"重均分子量\", \"数均分子量\", \"多分散性\",\n",
    "                    \"构型\", \"立构规整性\", \"等规\", \"间规\", \"无规\", \"序列分布\",\n",
    "                    # 物理性质\n",
    "                    \"玻璃化转变\", \"熔点\", \"结晶\", \"结晶度\", \"取向\", \"缠结\",\n",
    "                    \"粘弹性\", \"蠕变\", \"应力松弛\", \"动态力学\", \"流变\", \"加工性能\",\n",
    "                    # 改性与应用\n",
    "                    \"改性\", \"共混\", \"复合\", \"增强\", \"增韧\", \"阻燃\",\n",
    "                    \"功能高分子\", \"智能材料\", \"生物材料\", \"医用材料\", \"包装材料\",\n",
    "                    # 英文词汇\n",
    "                    \"polymer\", \"macromolecule\", \"plastic\", \"rubber\", \"fiber\",\n",
    "                    \"polymerization\", \"molecular\", \"weight\", \"crystallization\", \"glass\"\n",
    "                ],\n",
    "                \"纳米材料\": [\n",
    "                    # 纳米基础\n",
    "                    \"纳米\", \"纳米材料\", \"纳米科学\", \"纳米技术\", \"纳米结构\",\n",
    "                    # 维度分类\n",
    "                    \"纳米粒子\", \"零维\", \"一维\", \"二维\", \"三维\", \"量子点\",\n",
    "                    \"纳米线\", \"纳米管\", \"纳米棒\", \"纳米片\", \"纳米带\", \"纳米环\",\n",
    "                    # 二维材料\n",
    "                    \"纳米材料\", \"石墨烯\", \"氧化石墨烯\", \"MXene\", \"过渡金属硫化物\",\n",
    "                    \"黑磷\", \"硼烯\", \"氮化硼\", \"单层\", \"双层\", \"多层\", \"范德华层状\",\n",
    "                    # 制备方法\n",
    "                    \"自上而下\", \"自下而上\", \"物理制备\", \"化学制备\", \"生物制备\",\n",
    "                    \"溅射\", \"蒸发\", \"CVD\", \"ALD\", \"溶胶凝胶\", \"水热\", \"微波\",\n",
    "                    # 表征手段\n",
    "                    \"扫描电镜\", \"透射电镜\", \"原子力显微镜\", \"X射线衍射\", \"拉曼光谱\",\n",
    "                    \"X射线光电子能谱\", \"比表面积\", \"孔径分布\", \"粒径分析\", \"Zeta电位\",\n",
    "                    # 特殊效应\n",
    "                    \"表面\", \"界面\", \"尺寸效应\", \"量子效应\", \"表面等离激元\",\n",
    "                    \"超疏水\", \"超亲水\", \"光催化\", \"电催化\", \"传感\", \"药物载体\",\n",
    "                    # 英文词汇\n",
    "                    \"nanomaterial\", \"nanoparticle\", \"nanotechnology\", \"graphene\", \"quantum\",\n",
    "                    \"dot\", \"nanotube\", \"nanowire\", \"surface\", \"interface\", \"size\", \"effect\"\n",
    "                ],\n",
    "                \"能源材料\": [\n",
    "                    # 能源基础\n",
    "                    \"能源\", \"新能源\", \"可再生能源\", \"清洁能源\", \"能源材料\",\n",
    "                    # 电池材料\n",
    "                    \"电池\", \"锂电池\", \"钠电池\", \"锂离子电池\", \"固态电池\",\n",
    "                    \"正极材料\", \"负极材料\", \"电解质\", \"隔膜\", \"集流体\", \"粘结剂\",\n",
    "                    # 太阳能材料\n",
    "                    \"太阳能\", \"光伏\", \"太阳能电池\", \"硅电池\", \"薄膜电池\",\n",
    "                    \"钙钛矿电池\", \"有机光伏\", \"染料敏化\", \"光电转换\", \"能量转换效率\",\n",
    "                    # 燃料电池\n",
    "                    \"燃料电池\", \"质子交换膜\", \"固体氧化物\", \"催化剂\", \"载体\",\n",
    "                    \"膜电极\", \"双极板\", \"气体扩散层\", \"电解质膜\", \"离子导体\",\n",
    "                    # 储能材料\n",
    "                    \"储能\", \"超级电容器\", \"储氢\", \"储氢材料\", \"相变材料\",\n",
    "                    \"热电材料\", \"压电材料\", \"摩擦电\", \"能量收集\", \"能量存储\",\n",
    "                    # 功能特性\n",
    "                    \"能量密度\", \"功率密度\", \"循环稳定性\", \"倍率性能\", \"库伦效率\",\n",
    "                    \"充放电\", \"电化学性能\", \"离子传导\", \"电子传导\", \"界面阻抗\",\n",
    "                    # 英文词汇\n",
    "                    \"energy\", \"material\", \"battery\", \"solar\", \"cell\", \"fuel\",\n",
    "                    \"storage\", \"electrode\", \"electrolyte\", \"conversion\", \"efficiency\"\n",
    "                ]\n",
    "            },\n",
    "            \"数据类型\": [\n",
    "                # 化学数据类型\n",
    "                \"量子化学数据\", \"分子描述符数据\", \"反应数据\", \"光谱数据\",\n",
    "                \"热力学数据\", \"动力学数据\", \"结构数据\", \"电化学数据\",\n",
    "                # 材料数据类型\n",
    "                \"晶体结构数据\", \"物性数据\", \"力学性能数据\", \"电学性能数据\",\n",
    "                \"磁学性能数据\", \"光学性能数据\", \"热学性能数据\", \"表面数据\",\n",
    "                # 表征数据类型\n",
    "                \"X射线衍射数据\", \"电子显微镜数据\", \"核磁共振数据\", \"红外光谱数据\",\n",
    "                \"拉曼光谱数据\", \"质谱数据\", \"热分析数据\", \"元素分析数据\",\n",
    "                # 实验数据类型\n",
    "                \"合成条件数据\", \"催化性能数据\", \"反应条件数据\", \"分离纯化数据\",\n",
    "                \"稳定性数据\", \"毒性数据\", \"安全性数据\", \"环境数据\"\n",
    "            ]\n",
    "        },\n",
    "        \n",
    "        \"空间信息\": {\n",
    "            \"功能导向\": {\n",
    "                \"天体目标识别\": [\n",
    "                    # 核心概念\n",
    "                    \"识别\", \"分类\", \"天体\", \"星体\", \"目标\", \"天体识别\", \"目标检测\",\n",
    "                    # 天体类型\n",
    "                    \"恒星\", \"行星\", \"小行星\", \"彗星\", \"流星\", \"星系\", \"星云\",\n",
    "                    \"类星体\", \"脉冲星\", \"中子星\", \"白矮星\", \"红巨星\", \"超新星\",\n",
    "                    # 观测技术\n",
    "                    \"望远镜\", \"CCD\", \"CMOS\", \"光学观测\", \"射电观测\", \"红外观测\",\n",
    "                    \"紫外观测\", \"X射线观测\", \"γ射线观测\", \"多波段观测\", \"全天观测\",\n",
    "                    # 图像处理\n",
    "                    \"图像识别\", \"模式识别\", \"特征提取\", \"目标分割\", \"背景扣除\",\n",
    "                    \"噪声滤除\", \"图像增强\", \"形态学操作\", \"边缘检测\", \"轮廓提取\",\n",
    "                    # 机器学习\n",
    "                    \"机器学习\", \"深度学习\", \"神经网络\", \"卷积网络\", \"支持向量机\",\n",
    "                    \"随机森林\", \"聚类分析\", \"分类算法\", \"目标检测算法\", \"语义分割\",\n",
    "                    # 英文词汇\n",
    "                    \"identification\", \"classification\", \"celestial\", \"object\", \"detection\",\n",
    "                    \"star\", \"galaxy\", \"nebula\", \"telescope\", \"astronomical\", \"survey\"\n",
    "                ],\n",
    "                \"轨道参数确定\": [\n",
    "                    # 核心概念\n",
    "                    \"轨道\", \"参数\", \"确定\", \"轨道参数\", \"轨道要素\", \"开普勒要素\",\n",
    "                    # 轨道要素\n",
    "                    \"半长轴\", \"偏心率\", \"倾角\", \"升交点\", \"近点角\", \"真近点角\",\n",
    "                    \"平近点角\", \"偏近点角\", \"轨道周期\", \"轨道速度\", \"角速度\",\n",
    "                    # 轨道类型\n",
    "                    \"椭圆\", \"椭圆轨道\", \"圆轨道\", \"抛物线轨道\", \"双曲线轨道\",\n",
    "                    \"地心轨道\", \"日心轨道\", \"月球轨道\", \"行星轨道\", \"小行星轨道\",\n",
    "                    # 计算方法\n",
    "                    \"计算\", \"数值计算\", \"分析解\", \"数值积分\", \"摄动理论\",\n",
    "                    \"最小二乘\", \"卡尔曼滤波\", \"统计轨道确定\", \"初轨确定\", \"精密定轨\",\n",
    "                    # 观测数据\n",
    "                    \"位置观测\", \"速度观测\", \"测距\", \"测角\", \"径向速度\",\n",
    "                    \"视差\", \"自行\", \"光行时\", \"相对论修正\", \"大气折射\",\n",
    "                    # 应用领域\n",
    "                    \"天体力学\", \"航天动力学\", \"空间探测\", \"卫星导航\", \"空间监视\",\n",
    "                    \"近地天体\", \"空间碎片\", \"交会对接\", \"轨道转移\", \"轨道机动\",\n",
    "                    # 英文词汇\n",
    "                    \"orbital\", \"parameter\", \"determination\", \"kepler\", \"elements\",\n",
    "                    \"ellipse\", \"eccentricity\", \"inclination\", \"calculation\", \"trajectory\"\n",
    "                ],\n",
    "                \"光度测量分析\": [\n",
    "                    # 核心概念\n",
    "                    \"光度\", \"测量\", \"分析\", \"光度学\", \"测光\", \"光度测量\",\n",
    "                    # 光度系统\n",
    "                    \"亮度\", \"星等\", \"视星等\", \"绝对星等\", \"色指数\", \"颜色\",\n",
    "                    \"UBV系统\", \"约翰逊系统\", \"斯特伦姆格林系统\", \"2MASS\", \"SDSS\",\n",
    "                    # 光度量\n",
    "                    \"流量\", \"光度\", \"发光度\", \"光通量\", \"辐射流量\", \"表面亮度\",\n",
    "                    \"中心亮度\", \"积分星等\", \"孔径测光\", \"轮廓测光\", \"表面测光\",\n",
    "                    # 测量技术\n",
    "                    \"CCD测光\", \"光电测光\", \"照相测光\", \"红外测光\", \"空间测光\",\n",
    "                    \"地基测光\", \"多色测光\", \"宽带测光\", \"窄带测光\", \"中带测光\",\n",
    "                    # 数据处理\n",
    "                    \"定标\", \"流量定标\", \"大气消光\", \"仪器响应\", \"系统误差\",\n",
    "                    \"统计误差\", \"标准星\", \"次级标准\", \"测光精度\", \"测光标准系统\",\n",
    "                    # 变星观测\n",
    "                    \"变星\", \"光变\", \"光变曲线\", \"周期\", \"振幅\", \"相位\",\n",
    "                    \"脉动变星\", \"食变星\", \"爆发变星\", \"新星\", \"超新星\", \"激变变星\",\n",
    "                    # 英文词汇\n",
    "                    \"photometry\", \"magnitude\", \"brightness\", \"flux\", \"luminosity\",\n",
    "                    \"color\", \"index\", \"measurement\", \"calibration\", \"photometric\"\n",
    "                ],\n",
    "                \"光谱特征提取\": [\n",
    "                    # 核心概念\n",
    "                    \"光谱\", \"特征\", \"提取\", \"光谱学\", \"光谱分析\", \"谱线分析\",\n",
    "                    # 光谱类型\n",
    "                    \"谱线\", \"吸收谱\", \"发射谱\", \"连续谱\", \"线谱\", \"带谱\",\n",
    "                    \"恒星光谱\", \"星系光谱\", \"行星光谱\", \"太阳光谱\", \"星际介质光谱\",\n",
    "                    # 光谱特征\n",
    "                    \"红移\", \"蓝移\", \"多普勒效应\", \"谱线轮廓\", \"等值宽度\", \"线强\",\n",
    "                    \"中心深度\", \"半高全宽\", \"不对称性\", \"翼部\", \"核心\", \"吸收线\",\n",
    "                    # 光谱仪器\n",
    "                    \"分析\", \"光谱仪\", \"色散\", \"分辨率\", \"光谱分辨率\", \"波长定标\",\n",
    "                    \"流量定标\", \"响应函数\", \"狭缝\", \"光纤\", \"积分视场\", \"回声光栅\",\n",
    "                    # 数据处理\n",
    "                    \"光谱提取\", \"背景扣除\", \"宇宙线去除\", \"波长定标\", \"流量定标\",\n",
    "                    \"大气谱线\", \"视向速度\", \"径向速度\", \"光谱型\", \"光谱分类\",\n",
    "                    # 物理信息\n",
    "                    \"温度\", \"表面重力\", \"金属丰度\", \"化学丰度\", \"元素丰度\",\n",
    "                    \"电离度\", \"激发温度\", \"湍动速度\", \"磁场\", \"压力致宽\",\n",
    "                    # 英文词汇\n",
    "                    \"spectroscopy\", \"spectrum\", \"spectral\", \"line\", \"feature\",\n",
    "                    \"redshift\", \"doppler\", \"wavelength\", \"resolution\", \"analysis\"\n",
    "                ],\n",
    "                \"时间序列分析\": [\n",
    "                    # 核心概念\n",
    "                    \"时间序列\", \"时序\", \"时变\", \"时域分析\", \"变化\", \"演化\",\n",
    "                    # 变化类型\n",
    "                    \"周期\", \"周期性\", \"准周期\", \"非周期\", \"趋势\", \"长期变化\",\n",
    "                    \"短期变化\", \"突发变化\", \"渐变\", \"阶跃变化\", \"随机变化\",\n",
    "                    # 分析方法\n",
    "                    \"傅里叶分析\", \"功率谱\", \"周期图\", \"自相关\", \"交叉相关\",\n",
    "                    \"小波分析\", \"希尔伯特变换\", \"经验模态分解\", \"主成分分析\",\n",
    "                    # 统计方法\n",
    "                    \"统计分析\", \"概率分布\", \"方差分析\", \"回归分析\", \"时间序列建模\",\n",
    "                    \"ARIMA\", \"状态空间模型\", \"卡尔曼滤波\", \"贝叶斯分析\",\n",
    "                    # 天体应用\n",
    "                    \"变星\", \"脉冲星\", \"活动星系核\", \"γ射线暴\", \"超新星\",\n",
    "                    \"太阳活动\", \"黑洞\", \"吸积盘\", \"喷流\", \"耀斑\", \"爆发\",\n",
    "                    # 信号处理\n",
    "                    \"噪声\", \"信噪比\", \"滤波\", \"去趋势\", \"插值\", \"重采样\",\n",
    "                    \"数据质量\", \"异常值\", \"缺失数据\", \"采样率\", \"奈奎斯特频率\",\n",
    "                    # 英文词汇\n",
    "                    \"time\", \"series\", \"temporal\", \"variability\", \"periodic\",\n",
    "                    \"trend\", \"fourier\", \"power\", \"spectrum\", \"correlation\"\n",
    "                ],\n",
    "                \"位置导航定位\": [\n",
    "                    # 核心概念\n",
    "                    \"位置\", \"导航\", \"定位\", \"坐标\", \"定位系统\", \"导航系统\",\n",
    "                    # 全球导航系统\n",
    "                    \"GPS\", \"GNSS\", \"北斗\", \"GLONASS\", \"Galileo\", \"QZSS\",\n",
    "                    \"卫星导航\", \"伪距\", \"载波相位\", \"差分GPS\", \"RTK\", \"PPP\",\n",
    "                    # 坐标系统\n",
    "                    \"坐标\", \"天球坐标\", \"地心坐标\", \"大地坐标\", \"投影坐标\",\n",
    "                    \"赤道坐标\", \"银道坐标\", \"黄道坐标\", \"地平坐标\", \"时角坐标\",\n",
    "                    # 参考系\n",
    "                    \"参考系\", \"参考框架\", \"国际天球参考系\", \"国际地球参考系\",\n",
    "                    \"WGS84\", \"ITRF\", \"J2000\", \"岁差\", \"章动\", \"极移\",\n",
    "                    # 测量技术\n",
    "                    \"VLBI\", \"激光测距\", \"卫星测距\", \"多普勒测量\", \"干涉测量\",\n",
    "                    \"天体测量\", \"位置天文学\", \"视差\", \"自行\", \"三角视差\",\n",
    "                    # 精度评估\n",
    "                    \"精度\", \"准确度\", \"误差\", \"系统误差\", \"随机误差\", \"不确定度\",\n",
    "                    \"协方差\", \"误差椭圆\", \"GDOP\", \"PDOP\", \"置信区间\",\n",
    "                    # 英文词汇\n",
    "                    \"position\", \"navigation\", \"positioning\", \"coordinate\", \"GPS\",\n",
    "                    \"GNSS\", \"satellite\", \"accuracy\", \"precision\", \"reference\"\n",
    "                ],\n",
    "                \"空间环境监测\": [\n",
    "                    # 核心概念\n",
    "                    \"空间\", \"环境\", \"监测\", \"空间环境\", \"空间天气\", \"日地关系\",\n",
    "                    # 监测对象\n",
    "                    \"辐射\", \"粒子辐射\", \"电磁辐射\", \"宇宙线\", \"太阳粒子\",\n",
    "                    \"范艾伦带\", \"辐射带\", \"等离子体\", \"电离层\", \"磁层\",\n",
    "                    # 磁场环境\n",
    "                    \"磁场\", \"地磁场\", \"行星际磁场\", \"磁暴\", \"磁亚暴\",\n",
    "                    \"磁重联\", \"磁层顶\", \"激波\", \"等离子体片\", \"环电流\",\n",
    "                    # 太阳活动\n",
    "                    \"太阳风\", \"日冕物质抛射\", \"太阳耀斑\", \"质子事件\",\n",
    "                    \"太阳周期\", \"太阳黑子\", \"日冕洞\", \"太阳射电爆发\",\n",
    "                    # 监测手段\n",
    "                    \"卫星监测\", \"地面监测\", \"雷达监测\", \"光学监测\",\n",
    "                    \"粒子探测器\", \"磁强计\", \"等离子体分析仪\", \"射电望远镜\",\n",
    "                    # 空间效应\n",
    "                    \"单粒子效应\", \"总剂量效应\", \"位移损伤\", \"表面充电\",\n",
    "                    \"深层充电\", \"大气阻尼\", \"轨道衰减\", \"通信中断\",\n",
    "                    # 英文词汇\n",
    "                    \"space\", \"environment\", \"monitoring\", \"radiation\", \"magnetic\",\n",
    "                    \"field\", \"solar\", \"wind\", \"plasma\", \"ionosphere\", \"magnetosphere\"\n",
    "                ],\n",
    "                \"图像处理识别\": [\n",
    "                    # 核心概念\n",
    "                    \"图像\", \"处理\", \"识别\", \"图像处理\", \"图像识别\", \"计算机视觉\",\n",
    "                    # 预处理\n",
    "                    \"增强\", \"滤波\", \"去噪\", \"锐化\", \"平滑\", \"对比度增强\",\n",
    "                    \"直方图均衡\", \"伽马校正\", \"几何校正\", \"辐射校正\", \"大气校正\",\n",
    "                    # 特征提取\n",
    "                    \"特征提取\", \"边缘检测\", \"角点检测\", \"纹理分析\", \"形状分析\",\n",
    "                    \"SIFT\", \"SURF\", \"ORB\", \"HOG\", \"LBP\", \"Gabor滤波器\",\n",
    "                    # 图像分割\n",
    "                    \"分割\", \"阈值分割\", \"区域生长\", \"边缘分割\", \"聚类分割\",\n",
    "                    \"水平集\", \"活动轮廓\", \"图割\", \"语义分割\", \"实例分割\",\n",
    "                    # 模式识别\n",
    "                    \"模式识别\", \"分类\", \"聚类\", \"支持向量机\", \"神经网络\",\n",
    "                    \"深度学习\", \"卷积神经网络\", \"循环神经网络\", \"注意机制\",\n",
    "                    # 目标检测\n",
    "                    \"目标检测\", \"目标跟踪\", \"YOLO\", \"R-CNN\", \"SSD\",\n",
    "                    \"候选区域\", \"非极大值抑制\", \"边界框\", \"锚点\", \"特征金字塔\",\n",
    "                    # 应用场景\n",
    "                    \"天体识别\", \"表面特征\", \"撞击坑\", \"地形分析\", \"变化检测\",\n",
    "                    \"运动目标\", \"遥感图像\", \"多光谱\", \"高光谱\", \"合成孔径雷达\",\n",
    "                    # 英文词汇\n",
    "                    \"image\", \"processing\", \"recognition\", \"enhancement\", \"filtering\",\n",
    "                    \"segmentation\", \"feature\", \"detection\", \"classification\", \"CNN\"\n",
    "                ],\n",
    "                \"射电信号分析\": [\n",
    "                    # 核心概念\n",
    "                    \"射电\", \"信号\", \"分析\", \"射电天文\", \"射电望远镜\", \"无线电\",\n",
    "                    # 射电源\n",
    "                    \"频谱\", \"射电源\", \"脉冲星\", \"类星体\", \"射电星系\",\n",
    "                    \"超新星遗迹\", \"HII区\", \"分子云\", \"同步辐射\", \"自由自由辐射\",\n",
    "                    # 观测技术\n",
    "                    \"干涉\", \"干涉测量\", \"综合孔径\", \"VLBI\", \"阵列\", \"相关器\",\n",
    "                    \"波束形成\", \"校准\", \"成像\", \"频率\", \"带宽\", \"时间分辨率\",\n",
    "                    # 信号处理\n",
    "                    \"数字信号处理\", \"快速傅里叶变换\", \"滤波\", \"去色散\",\n",
    "                    \"相位校准\", \"幅度校准\", \"RFI抑制\", \"基线减除\", \"去卷积\",\n",
    "                    # 脉冲星\n",
    "                    \"脉冲\", \"脉冲星\", \"脉冲轮廓\", \"色散\", \"色散测量\", \"周期\",\n",
    "                    \"周期导数\", \"计时\", \"脉冲到达时间\", \"残差\", \"引力波\",\n",
    "                    # 数据分析\n",
    "                    \"时频分析\", \"动态频谱\", \"功率谱\", \"自相关\", \"交叉相关\",\n",
    "                    \"相干分析\", \"偏振分析\", \"法拉第旋转\", \"同步辐射\", \"热辐射\",\n",
    "                    # 英文词汇\n",
    "                    \"radio\", \"signal\", \"analysis\", \"pulsar\", \"interferometry\",\n",
    "                    \"spectrum\", \"correlation\", \"calibration\", \"imaging\", \"dispersion\"\n",
    "                ],\n",
    "                \"变源监测预警\": [\n",
    "                    # 核心概念\n",
    "                    \"变源\", \"监测\", \"预警\", \"变源监测\", \"瞬变源\", \"爆发源\",\n",
    "                    # 变源类型\n",
    "                    \"爆发\", \"γ射线暴\", \"超新星\", \"新星\", \"矮新星\", \"耀斑星\",\n",
    "                    \"激变变星\", \"X射线暴\", \"软γ重复暴\", \"快速射电暴\", \"引力波源\",\n",
    "                    # 监测系统\n",
    "                    \"闪烁\", \"光变\", \"全天监测\", \"巡天\", \"实时监测\", \"多信使\",\n",
    "                    \"早期预警\", \"快速跟进\", \"后随观测\", \"多波段\", \"多望远镜\",\n",
    "                    # 探测方法\n",
    "                    \"阈值检测\", \"异常检测\", \"模式识别\", \"机器学习\", \"神经网络\",\n",
    "                    \"统计检验\", \"假阳性控制\", \"灵敏度\", \"完备性\", \"污染率\",\n",
    "                    # 数据处理\n",
    "                    \"实时处理\", \"流水线\", \"自动化\", \"事件识别\", \"分类\",\n",
    "                    \"参数估计\", \"光变曲线\", \"能谱分析\", \"时间分析\", \"空间定位\",\n",
    "                    # 预警系统\n",
    "                    \"快速通知\", \"警报\", \"GCN\", \"ATel\", \"TNS\", \"VOEvent\",\n",
    "                    \"坐标分发\", \"多信使协调\", \"国际合作\", \"数据共享\", \"开放获取\",\n",
    "                    # 英文词汇\n",
    "                    \"transient\", \"monitoring\", \"alert\", \"burst\", \"flare\",\n",
    "                    \"detection\", \"real-time\", \"follow-up\", \"multi-messenger\", \"GRB\"\n",
    "                ]\n",
    "            },\n",
    "            \"应用领域\": {\n",
    "                \"天体物理学\": [\n",
    "                    # 基础概念\n",
    "                    \"天体物理\", \"天体物理学\", \"理论天体物理\", \"观测天体物理\",\n",
    "                    # 天体类型\n",
    "                    \"恒星\", \"恒星物理\", \"恒星演化\", \"恒星形成\", \"恒星死亡\",\n",
    "                    \"星系\", \"星系物理\", \"星系演化\", \"星系形成\", \"星系团\",\n",
    "                    \"宇宙\", \"宇宙学\", \"宇宙演化\", \"大爆炸\", \"暗物质\", \"暗能量\",\n",
    "                    # 致密天体\n",
    "                    \"白矮星\", \"中子星\", \"黑洞\", \"脉冲星\", \"磁星\",\n",
    "                    \"吸积盘\", \"喷流\", \"X射线双星\", \"伽马射线暴\", \"引力波\",\n",
    "                    # 星际介质\n",
    "                    \"星际介质\", \"星际尘埃\"],\n",
    "                    \"行星科学\": [\"行星\", \"火星\", \"木星\", \"土星\", \"金星\"],\n",
    "                    \"恒星物理学\": [\"恒星\", \"太阳\", \"恒星演化\", \"超新星\"],\n",
    "                    \"星系天文学\": [\"星系\", \"银河系\", \"星系团\", \"暗物质\"],\n",
    "                    \"空间技术\": [\"卫星\", \"航天\", \"遥感\", \"通信\", \"导航\"],\n",
    "                    \"射电天文学\": [\"射电\", \"射电望远镜\", \"脉冲星\", \"类星体\"],\n",
    "                    \"光学天文学\": [\"光学\", \"望远镜\", \"CCD\", \"光度\", \"测光\"],\n",
    "                    \"高能天体物理\": [\"高能\", \"X射线\", \"伽马射线\", \"中子星\"],\n",
    "                    \"太阳物理学\": [\"太阳\", \"日冕\", \"太阳风\", \"磁场\"],\n",
    "                    \"宇宙学\": [\"宇宙学\", \"大爆炸\", \"暗能量\", \"微波背景\"]\n",
    "                },\n",
    "                \"数据类型\": [\"光学\", \"射电\", \"光谱\", \"测距\", \"图像\", \"时序\", \"位置\"]\n",
    "            },\n",
    "            \"大气海洋\": {\n",
    "                \"功能导向\": {\n",
    "                    \"天气数值预报\": [\"天气\", \"预报\", \"数值\", \"模式\", \"预测\"],\n",
    "                    \"气候变化检测\": [\"气候\", \"变化\", \"检测\", \"趋势\", \"异常\"],\n",
    "                    \"极端事件预警\": [\"极端\", \"事件\", \"预警\", \"灾害\", \"台风\"],\n",
    "                    \"环境质量评估\": [\"环境\", \"质量\", \"评估\", \"污染\", \"监测\"],\n",
    "                    \"数据同化分析\": [\"同化\", \"分析\", \"融合\", \"观测\", \"模式\"],\n",
    "                    \"海洋动力学建模\": [\"海洋\", \"动力学\", \"建模\", \"环流\", \"波浪\"],\n",
    "                    \"大气成分监测\": [\"大气\", \"成分\", \"监测\", \"温室气体\", \"臭氧\"],\n",
    "                    \"趋势模式识别\": [\"趋势\", \"模式\", \"识别\", \"周期\", \"振荡\"],\n",
    "                    \"灾害风险评估\": [\"灾害\", \"风险\", \"评估\", \"洪水\", \"干旱\"],\n",
    "                    \"生态环境预测\": [\"生态\", \"环境\", \"预测\", \"生物\", \"栖息地\"]\n",
    "                },\n",
    "                \"应用领域\": {\n",
    "                    \"数值天气预报\": [\"数值预报\", \"天气模式\", \"预报\", \"气象\"],\n",
    "                    \"气候变化研究\": [\"气候变化\", \"全球变暖\", \"温室效应\"],\n",
    "                    \"海洋学\": [\"海洋\", \"海流\", \"海温\", \"盐度\", \"波浪\"],\n",
    "                    \"大气化学\": [\"大气化学\", \"臭氧\", \"气溶胶\", \"污染\"],\n",
    "                    \"环境科学\": [\"环境\", \"生态\", \"污染\", \"保护\"],\n",
    "                    \"气象学\": [\"气象\", \"天气\", \"降水\", \"温度\", \"风\"],\n",
    "                    \"水文学\": [\"水文\", \"径流\", \"蒸发\", \"降水\", \"水循环\"],\n",
    "                    \"极地科学\": [\"极地\", \"南极\", \"北极\", \"冰川\", \"海冰\"],\n",
    "                    \"遥感应用\": [\"遥感\", \"卫星\", \"观测\", \"监测\"],\n",
    "                    \"生态气象学\": [\"生态气象\", \"农业\", \"植被\", \"生长\"]\n",
    "                },\n",
    "                \"数据类型\": [\"观测\", \"再分析\", \"卫星\", \"模式\", \"代理\", \"时序\", \"网格\"]\n",
    "            },\n",
    "            \"其他\": {\n",
    "                \"功能导向\": {\n",
    "                    \"知识图谱构建\": [\"知识图谱\", \"本体\", \"关系\", \"实体\", \"语义\"],\n",
    "                    \"信息检索排序\": [\"检索\", \"排序\", \"搜索\", \"相关性\", \"索引\"],\n",
    "                    \"文本挖掘分析\": [\"文本挖掘\", \"自然语言\", \"语义\", \"主题\"],\n",
    "                    \"多语言处理\": [\"多语言\", \"翻译\", \"语言模型\", \"词汇\"],\n",
    "                    \"社会网络分析\": [\"社会网络\", \"网络分析\", \"图论\", \"关系\"],\n",
    "                    \"统计建模预测\": [\"统计\", \"建模\", \"回归\", \"预测\", \"概率\"],\n",
    "                    \"可视化呈现\": [\"可视化\", \"图表\", \"展示\", \"界面\", \"交互\"],\n",
    "                    \"情感态度分析\": [\"情感\", \"态度\", \"情绪\", \"观点\", \"倾向\"],\n",
    "                    \"档案数字化管理\": [\"档案\", \"数字化\", \"管理\", \"保存\", \"整理\"],\n",
    "                    \"跨学科数据融合\": [\"跨学科\", \"融合\", \"整合\", \"多源\", \"综合\"]\n",
    "                },\n",
    "                \"应用领域\": {\n",
    "                    \"数字人文\": [\"数字人文\", \"文化\", \"文学\", \"艺术\"],\n",
    "                    \"计算社会科学\": [\"计算社会科学\", \"社会\", \"行为\", \"群体\"],\n",
    "                    \"语言学\": [\"语言学\", \"语言\", \"语法\", \"语音\", \"方言\"],\n",
    "                    \"历史学\": [\"历史\", \"史学\", \"古代\", \"近代\", \"文献\"],\n",
    "                    \"文化研究\": [\"文化\", \"民俗\", \"传统\", \"习俗\"],\n",
    "                    \"政治学\": [\"政治\", \"政府\", \"政策\", \"治理\"],\n",
    "                    \"经济学\": [\"经济\", \"市场\", \"金融\", \"贸易\"],\n",
    "                    \"人口学\": [\"人口\", \"人口统计\", \"生育\", \"迁移\"],\n",
    "                    \"教育学\": [\"教育\", \"学习\", \"教学\", \"课程\"],\n",
    "                    \"科学计量学\": [\"科学计量\", \"引文\", \"期刊\", \"学术\"]\n",
    "                },\n",
    "                \"数据类型\": [\"文本\", \"调查\", \"档案\", \"多媒体\", \"网络\", \"语音\", \"图像\"]\n",
    "            }\n",
    "        }\n",
    "    \n",
    "    def extract_keywords(self, text: str) -> Set[str]:\n",
    "        \"\"\"从文本中提取关键词\"\"\"\n",
    "        # 转换为小写并移除标点符号\n",
    "        clean_text = re.sub(r'[^\\w\\s\\u4e00-\\u9fff]', ' ', text.lower())\n",
    "        words = clean_text.split()\n",
    "        \n",
    "        # 提取有意义的词汇（长度大于1）\n",
    "        keywords = set()\n",
    "        for word in words:\n",
    "            if len(word) > 1:\n",
    "                keywords.add(word)\n",
    "        \n",
    "        return keywords\n",
    "    \n",
    "    def calculate_discipline_scores(self, text: str) -> Dict[str, float]:\n",
    "        \"\"\"计算各学科的匹配分数\"\"\"\n",
    "        keywords = self.extract_keywords(text)\n",
    "        scores = {}\n",
    "        \n",
    "        for discipline, mapping in self.keyword_mapping.items():\n",
    "            score = 0\n",
    "            total_weight = 0\n",
    "            \n",
    "            # 功能导向匹配\n",
    "            for function, func_keywords in mapping[\"功能导向\"].items():\n",
    "                for keyword in func_keywords:\n",
    "                    total_weight += 3\n",
    "                    if any(kw in keyword.lower() or keyword.lower() in kw for kw in keywords):\n",
    "                        score += 3\n",
    "            \n",
    "            # 应用领域匹配\n",
    "            for domain, domain_keywords in mapping[\"应用领域\"].items():\n",
    "                for keyword in domain_keywords:\n",
    "                    total_weight += 2\n",
    "                    if any(kw in keyword.lower() or keyword.lower() in kw for kw in keywords):\n",
    "                        score += 2\n",
    "            \n",
    "            # 数据类型匹配\n",
    "            for data_type in mapping[\"数据类型\"]:\n",
    "                total_weight += 1\n",
    "                if any(kw in data_type.lower() or data_type.lower() in kw for kw in keywords):\n",
    "                    score += 1\n",
    "            \n",
    "            # 计算归一化分数\n",
    "            scores[discipline] = score / max(total_weight, 1) if total_weight > 0 else 0\n",
    "        \n",
    "        return scores\n",
    "    \n",
    "    def assign_primary_discipline(self, text: str) -> str:\n",
    "        \"\"\"分配主要学科标签\"\"\"\n",
    "        scores = self.calculate_discipline_scores(text)\n",
    "        if not scores:\n",
    "            return \"其他\"\n",
    "        \n",
    "        max_score = max(scores.values())\n",
    "        if max_score == 0:\n",
    "            return \"其他\"\n",
    "        \n",
    "        return max(scores, key=scores.get)\n",
    "    \n",
    "    def assign_detailed_tags(self, text: str, discipline: str) -> Tuple[str, str, str]:\n",
    "        \"\"\"为特定学科分配详细标签，返回(功能导向, 应用领域, 数据类型)\"\"\"\n",
    "        if discipline not in self.tag_hierarchy:\n",
    "            return (\"跨学科数据融合\", \"数字人文\", \"文本数据\")\n",
    "        \n",
    "        keywords = self.extract_keywords(text)\n",
    "        hierarchy = self.tag_hierarchy[discipline]\n",
    "        mapping = self.keyword_mapping[discipline]\n",
    "        \n",
    "        # 分配功能导向标签\n",
    "        function_scores = {}\n",
    "        for function in hierarchy[\"功能导向\"]:\n",
    "            score = 0\n",
    "            if function in mapping[\"功能导向\"]:\n",
    "                for keyword in mapping[\"功能导向\"][function]:\n",
    "                    if any(kw in keyword.lower() or keyword.lower() in kw for kw in keywords):\n",
    "                        score += 1\n",
    "            function_scores[function] = score\n",
    "        \n",
    "        best_function = max(function_scores, key=function_scores.get) if max(function_scores.values()) > 0 else hierarchy[\"功能导向\"][0]\n",
    "        \n",
    "        # 分配应用领域标签\n",
    "        domain_scores = {}\n",
    "        for domain in hierarchy[\"应用领域\"]:\n",
    "            score = 0\n",
    "            if domain in mapping[\"应用领域\"]:\n",
    "                for keyword in mapping[\"应用领域\"][domain]:\n",
    "                    if any(kw in keyword.lower() or keyword.lower() in kw for kw in keywords):\n",
    "                        score += 1\n",
    "            domain_scores[domain] = score\n",
    "        \n",
    "        best_domain = max(domain_scores, key=domain_scores.get) if max(domain_scores.values()) > 0 else hierarchy[\"应用领域\"][0]\n",
    "        \n",
    "        # 分配数据类型标签\n",
    "        data_scores = {}\n",
    "        for data_type in hierarchy[\"数据类型\"]:\n",
    "            score = 0\n",
    "            if any(kw in data_type.lower() or data_type.lower() in kw for kw in keywords):\n",
    "                score += 1\n",
    "            data_scores[data_type] = score\n",
    "        \n",
    "        best_data_type = max(data_scores, key=data_scores.get) if max(data_scores.values()) > 0 else hierarchy[\"数据类型\"][0]\n",
    "        \n",
    "        return (best_function, best_domain, best_data_type)\n",
    "    \n",
    "    def tag_corpus(self, text: str, title: str = \"\") -> Dict:\n",
    "        \"\"\"对语料进行完整标签分配\"\"\"\n",
    "        # 合并标题和内容进行分析\n",
    "        full_text = f\"{title} {text}\" if title else text\n",
    "        \n",
    "        # 分配主要学科\n",
    "        primary_discipline = self.assign_primary_discipline(full_text)\n",
    "        \n",
    "        # 分配详细标签\n",
    "        function, domain, data_type = self.assign_detailed_tags(full_text, primary_discipline)\n",
    "        \n",
    "        # 生成标签字符串\n",
    "        tag_string = f\"{primary_discipline}/{function}/{domain}/{data_type}\"\n",
    "        \n",
    "        # 计算置信度分数\n",
    "        scores = self.calculate_discipline_scores(full_text)\n",
    "        confidence = scores.get(primary_discipline, 0)\n",
    "        \n",
    "        return {\n",
    "            \"标签\": tag_string,\n",
    "            \"主要学科\": primary_discipline,\n",
    "            \"功能导向\": function,\n",
    "            \"应用领域\": domain,\n",
    "            \"数据类型\": data_type,\n",
    "            \"置信度\": round(confidence, 3),\n",
    "            \"学科分数\": {k: round(v, 3) for k, v in scores.items()},\n",
    "            \"文本长度\": len(full_text),\n",
    "            \"关键词数量\": len(self.extract_keywords(full_text))\n",
    "        }\n",
    "    \n",
    "    def batch_tag_corpus(self, corpus_list: List[Dict]) -> List[Dict]:\n",
    "        \"\"\"批量标签分配\"\"\"\n",
    "        results = []\n",
    "        for i, item in enumerate(corpus_list):\n",
    "            text = item.get(\"content\", \"\")\n",
    "            title = item.get(\"title\", \"\")\n",
    "            \n",
    "            tags = self.tag_corpus(text, title)\n",
    "            \n",
    "            result = {\n",
    "                \"id\": item.get(\"id\", i),\n",
    "                \"title\": title,\n",
    "                \"标签\": tags[\"标签\"],\n",
    "                \"置信度\": tags[\"置信度\"],\n",
    "                \"详细信息\": {\n",
    "                    \"主要学科\": tags[\"主要学科\"],\n",
    "                    \"功能导向\": tags[\"功能导向\"],\n",
    "                    \"应用领域\": tags[\"应用领域\"],\n",
    "                    \"数据类型\": tags[\"数据类型\"],\n",
    "                    \"学科分数\": tags[\"学科分数\"]\n",
    "                }\n",
    "            }\n",
    "            results.append(result)\n",
    "        \n",
    "        return results\n",
    "    \n",
    "    def export_tags_to_json(self, results: List[Dict], filename: str):\n",
    "        \"\"\"导出标签结果为JSON文件\"\"\"\n",
    "        with open(filename, 'w', encoding='utf-8') as f:\n",
    "            json.dump(results, f, ensure_ascii=False, indent=2)\n",
    "    \n",
    "    def export_simple_format(self, results: List[Dict], filename: str):\n",
    "        \"\"\"导出简化格式（ID, 标题, 标签）\"\"\"\n",
    "        simple_results = []\n",
    "        for result in results:\n",
    "            simple_results.append({\n",
    "                \"id\": result[\"id\"],\n",
    "                \"title\": result[\"title\"],\n",
    "                \"tag\": result[\"标签\"]\n",
    "            })\n",
    "        \n",
    "        with open(filename, 'w', encoding='utf-8') as f:\n",
    "            json.dump(simple_results, f, ensure_ascii=False, indent=2)\n",
    "    \n",
    "    def generate_tag_statistics(self, results: List[Dict]) -> Dict:\n",
    "        \"\"\"生成标签统计信息\"\"\"\n",
    "        stats = {\n",
    "            \"总数据量\": len(results),\n",
    "            \"学科分布\": defaultdict(int),\n",
    "            \"置信度分布\": {\"高(>0.3)\": 0, \"中(0.1-0.3)\": 0, \"低(<0.1)\": 0},\n",
    "            \"功能导向分布\": defaultdict(int),\n",
    "            \"应用领域分布\": defaultdict(int),\n",
    "            \"数据类型分布\": defaultdict(int)\n",
    "        }\n",
    "        \n",
    "        for result in results:\n",
    "            # 学科分布\n",
    "            discipline = result[\"详细信息\"][\"主要学科\"]\n",
    "            stats[\"学科分布\"][discipline] += 1\n",
    "            \n",
    "            # 置信度分布\n",
    "            confidence = result[\"置信度\"]\n",
    "            if confidence > 0.3:\n",
    "                stats[\"置信度分布\"][\"高(>0.3)\"] += 1\n",
    "            elif confidence > 0.1:\n",
    "                stats[\"置信度分布\"][\"中(0.1-0.3)\"] += 1\n",
    "            else:\n",
    "                stats[\"置信度分布\"][\"低(<0.1)\"] += 1\n",
    "            \n",
    "            # 各层级分布\n",
    "            stats[\"功能导向分布\"][result[\"详细信息\"][\"功能导向\"]] += 1\n",
    "            stats[\"应用领域分布\"][result[\"详细信息\"][\"应用领域\"]] += 1\n",
    "            stats[\"数据类型分布\"][result[\"详细信息\"][\"数据类型\"]] += 1\n",
    "        \n",
    "        return dict(stats)\n",
    "\n",
    "\n",
    "# 使用示例\n",
    "def main():\n",
    "    \"\"\"主函数示例\"\"\"\n",
    "    \n",
    "    # 初始化标签系统\n",
    "    tagger = ScienceCorpusTagger()\n",
    "    \n",
    "    # 示例数据\n",
    "    sample_corpus = [\n",
    "        {\n",
    "            \"id\": 1,\n",
    "            \"title\": \"MammalNet哺乳动物行为识别数据集\",\n",
    "            \"content\": \"MammalNet视频数据集围绕涵盖17个目、69个科和173个哺乳动物类别的生物哺乳动物分类学构建，并包括12种常见的高级哺乳动物行为（例如狩猎、梳理行为）。用于动物和行为识别研究。\"\n",
    "        },\n",
    "        {\n",
    "            \"id\": 2,\n",
    "            \"title\": \"湍流流动DNS数据库\",\n",
    "            \"content\": \"\"\n",
    "        },\n",
    "        {\n",
    "            \"id\": 3,\n",
    "            \"title\": \"分子机器学习基准MoleculeNet\",\n",
    "            \"content\": \"MoleculeNet是分子机器学习的大规模基准。管理多个公共数据集，建立评估指标，并提供多个先前提出的分子特征化和学习算法的高质量开源实现。用于分子性质预测和药物发现。\"\n",
    "        },\n",
    "        {\n",
    "            \"id\": 4,\n",
    "            \"title\": \"超高清月面地图\",\n",
    "            \"content\": \"超高清月面地图第三版，具有4.5亿像素，有1294个月面地形被标注，包括655个主环形坑，348个卫星坑，19个月海，17个月湖等15种不同类型地标。\"\n",
    "        },\n",
    "        {\n",
    "            \"id\": 5,\n",
    "            \"title\": \"全球海洋再分析数据GLORYS12V1\",\n",
    "            \"content\": \"GLORYS12V1产品是CMEMS全球海洋涡旋分辨率再分析产品，涵盖了高度计数据。该产品主要基于当前实时全球预报CMEMS系统，用于海洋动力学研究和环境监测。\"\n",
    "        }\n",
    "    ]\n",
    "    \n",
    "    # 批量标签分配\n",
    "    results = tagger.batch_tag_corpus(sample_corpus)\n",
    "    \n",
    "    # 打印结果\n",
    "    print(\"=== 标签分配结果 ===\\n\")\n",
    "    for result in results:\n",
    "        print(f\"ID: {result['id']}\")\n",
    "        print(f\"标题: {result['title']}\")\n",
    "        print(f\"标签: {result['标签']}\")\n",
    "        print(f\"置信度: {result['置信度']}\")\n",
    "        print(\"-\" * 50)\n",
    "    \n",
    "    # 生成统计信息\n",
    "    stats = tagger.generate_tag_statistics(results)\n",
    "    print(f\"\\n=== 标签统计信息 ===\")\n",
    "    print(json.dumps(stats, ensure_ascii=False, indent=2))\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "eee8d9dd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "开始处理数据...\n",
      "处理完成！\n"
     ]
    }
   ],
   "source": [
    "# 创建科学领域映射字典\n",
    "def create_domain_mapping():\n",
    "    \"\"\"创建科学领域到标签系统第一级的映射\"\"\"\n",
    "    domain_mapping = {\n",
    "        # 生命科学相关\n",
    "        \"生命科学\": \"生命科学\",\n",
    "        \"生物学\": \"生命科学\", \n",
    "        \"医学\": \"生命科学\",\n",
    "        \"生物医学\": \"生命科学\",\n",
    "        \"基因组学\": \"生命科学\",\n",
    "        \"蛋白质组学\": \"生命科学\",\n",
    "        \"神经科学\": \"生命科学\",\n",
    "        \"生物信息学\": \"生命科学\",\n",
    "        \n",
    "        # 工程技术相关\n",
    "        \"工程技术\": \"工程技术\",\n",
    "        \"机械工程\": \"工程技术\",\n",
    "        \"电气工程\": \"工程技术\", \n",
    "        \"自动化\": \"工程技术\",\n",
    "        \"控制工程\": \"工程技术\",\n",
    "        \"流体力学\": \"工程技术\",\n",
    "        \"热传递\": \"工程技术\",\n",
    "        \"结构工程\": \"工程技术\",\n",
    "        \n",
    "        # 物质科学相关\n",
    "        \"物质科学\": \"物质科学\",\n",
    "        \"化学\": \"物质科学\",\n",
    "        \"材料科学\": \"物质科学\",\n",
    "        \"物理化学\": \"物质科学\",\n",
    "        \"催化\": \"物质科学\",\n",
    "        \"药物化学\": \"物质科学\",\n",
    "        \n",
    "        # 空间信息相关\n",
    "        \"空间信息\": \"空间信息\",\n",
    "        \"天体物理\": \"空间信息\",\n",
    "        \"天文学\": \"空间信息\",\n",
    "        \"空间科学\": \"空间信息\",\n",
    "        \"遥感\": \"空间信息\",\n",
    "        \"地理信息\": \"空间信息\",\n",
    "        \n",
    "        # 大气海洋相关\n",
    "        \"大气海洋\": \"大气海洋\",\n",
    "        \"气象学\": \"大气海洋\",\n",
    "        \"海洋学\": \"大气海洋\",\n",
    "        \"气候学\": \"大气海洋\",\n",
    "        \"环境科学\": \"大气海洋\",\n",
    "        \"水文学\": \"大气海洋\",\n",
    "        \n",
    "        # 其他\n",
    "        \"计算机科学\": \"其他\",\n",
    "        \"数学\": \"其他\",\n",
    "        \"物理学\": \"其他\",\n",
    "        \"地球科学\": \"其他\",\n",
    "        \"社会科学\": \"其他\"\n",
    "    }\n",
    "    return domain_mapping\n",
    "\n",
    "def process_real_data(df, science_domain_col='学科', description_col='介绍'):\n",
    "    \"\"\"\n",
    "    处理真实数据，生成标签列\n",
    "    \n",
    "    Args:\n",
    "        df: pandas DataFrame\n",
    "        science_domain_col: 科学领域列名\n",
    "        description_col: 数据介绍列名\n",
    "    \n",
    "    Returns:\n",
    "        pandas DataFrame with new tag column\n",
    "    \"\"\"\n",
    "    # 初始化标签系统\n",
    "    tagger = ScienceCorpusTagger()\n",
    "    domain_mapping = create_domain_mapping()\n",
    "    \n",
    "    # 创建结果列表\n",
    "    tags = []\n",
    "    \n",
    "    for idx, row in df.iterrows():\n",
    "        # 获取科学领域\n",
    "        science_domain = str(row[science_domain_col]) if pd.notna(row[science_domain_col]) else \"\"\n",
    "        \n",
    "        # 获取数据介绍文本\n",
    "        description = str(row[description_col]) if pd.notna(row[description_col]) else \"\"\n",
    "        \n",
    "        # 映射科学领域到第一级标签\n",
    "        first_level = \"其他\"  # 默认值\n",
    "        for domain_key, mapped_value in domain_mapping.items():\n",
    "            if domain_key in science_domain:\n",
    "                first_level = mapped_value\n",
    "                break\n",
    "        \n",
    "        # 使用数据介绍文本分析后三级标签\n",
    "        if description.strip():\n",
    "            # 分析描述文本获取详细标签\n",
    "            function, domain, data_type = tagger.assign_detailed_tags(description, first_level)\n",
    "        else:\n",
    "            # 如果没有描述，使用默认值\n",
    "            function = \"跨学科数据融合\"\n",
    "            domain = \"数字人文\"  \n",
    "            data_type = \"文本数据\"\n",
    "        \n",
    "        # 组合完整标签\n",
    "        tag = f\"{first_level}/{function}/{domain}/{data_type}\"\n",
    "        tags.append(tag)\n",
    "    \n",
    "    # 添加标签列到DataFrame\n",
    "    result_df = df.copy()\n",
    "    result_df['标签'] = tags\n",
    "    \n",
    "    return result_df\n",
    "\n",
    "# 处理数据\n",
    "print(\"开始处理数据...\")\n",
    "df_with_tags = process_real_data(df_result)\n",
    "print(\"处理完成！\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "55b9da6a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "标签分割完成！\n",
      "处理了 101 条记录\n",
      "\n",
      "各列的唯一值数量:\n",
      "科学领域: 4 个\n",
      "功能标签-任务导向: 11 个\n",
      "二级标签-应用领域: 9 个\n",
      "\n",
      "分割结果示例:\n",
      "      item_name    学科 功能标签-任务导向 二级标签-应用领域                     标签\n",
      "0         dolma  None     多语言处理       语言学      其他/多语言处理/语言学/文本数据\n",
      "1  SDO_training  None    统计建模预测       教育学     其他/统计建模预测/教育学/文本数据\n",
      "2     4DNeX-10M  None    统计建模预测      数字人文    其他/统计建模预测/数字人文/文本数据\n",
      "3    MADLAD-400  None    文本挖掘分析       语言学     其他/文本挖掘分析/语言学/文本数据\n",
      "4         ASPED  None    知识图谱构建    计算社会科学  其他/知识图谱构建/计算社会科学/文本数据\n"
     ]
    }
   ],
   "source": [
    "# 对标签进行分割处理，按/分为四层\n",
    "def split_tags_to_columns(df):\n",
    "    \"\"\"\n",
    "    将标签列按/分割为四个独立的列\n",
    "    标签格式：科学领域/功能标签-任务导向/二级标签-应用领域/二级标签-数据类型\n",
    "    \"\"\"\n",
    "    df_result = df.copy()\n",
    "    \n",
    "    # 分割标签列\n",
    "    tag_splits = df_result['标签'].str.split('/', expand=True)\n",
    "    \n",
    "    # 确保有四列\n",
    "    if tag_splits.shape[1] >= 4:\n",
    "        # 将分割后的标签填入对应列\n",
    "        df_result['科学领域'] = tag_splits[0]\n",
    "        df_result['功能标签-任务导向'] = tag_splits[1] \n",
    "        df_result['二级标签-应用领域'] = tag_splits[2]\n",
    "        \n",
    "        print(\"标签分割完成！\")\n",
    "        print(f\"处理了 {len(df_result)} 条记录\")\n",
    "        \n",
    "        # 显示分割后的统计信息\n",
    "        print(\"\\n各列的唯一值数量:\")\n",
    "        print(f\"科学领域: {df_result['科学领域'].nunique()} 个\")\n",
    "        print(f\"功能标签-任务导向: {df_result['功能标签-任务导向'].nunique()} 个\") \n",
    "        print(f\"二级标签-应用领域: {df_result['二级标签-应用领域'].nunique()} 个\")\n",
    "        \n",
    "    else:\n",
    "        print(f\"警告：标签分割结果只有 {tag_splits.shape[1]} 列，预期为4列\")\n",
    "        \n",
    "    return df_result\n",
    "\n",
    "# 执行标签分割\n",
    "df_final = split_tags_to_columns(df_with_tags)\n",
    "\n",
    "# 查看分割结果示例\n",
    "print(\"\\n分割结果示例:\")\n",
    "display_cols = ['item_name', '学科', '功能标签-任务导向', '二级标签-应用领域', '标签']\n",
    "print(df_final[display_cols].head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "936b4f26",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== 开始执行数据清洗 ===\n",
      "模态列：已将 97 个空值替换为 '多模态'\n",
      "协议列：已将 23 个空值替换为 '公开'\n",
      "\n",
      "=== 数据清洗完成 ===\n"
     ]
    }
   ],
   "source": [
    "# 执行数据清洗操作\n",
    "print(\"\\n=== 开始执行数据清洗 ===\")\n",
    "\n",
    "# 1. 处理模态列\n",
    "# 将空值替换为\"多模态\"\n",
    "modal_null_count = df_final['modalities'].isnull().sum()\n",
    "df_final['modalities'] = df_final['modalities'].fillna('多模态')\n",
    "\n",
    "# 处理模态列中的异常值（如\"公开\"等明显不属于模态的值）\n",
    "# 这些值也替换为\"多模态\"\n",
    "invalid_modal_values = ['公开']\n",
    "for invalid_value in invalid_modal_values:\n",
    "    invalid_count = (df_final['modalities'] == invalid_value).sum()\n",
    "    if invalid_count > 0:\n",
    "        print(f\"将模态列中的 '{invalid_value}' ({invalid_count}条) 替换为 '多模态'\")\n",
    "        df_final['modalities'] = df_final['modalities'].replace(invalid_value, '多模态')\n",
    "\n",
    "print(f\"模态列：已将 {modal_null_count} 个空值替换为 '多模态'\")\n",
    "\n",
    "# 2. 处理协议列\n",
    "# 将空值替换为\"公开\"\n",
    "protocol_null_count = df_final['开源协议'].isnull().sum()\n",
    "df_final['开源协议'] = df_final['开源协议'].fillna('公开')\n",
    "print(f\"协议列：已将 {protocol_null_count} 个空值替换为 '公开'\")\n",
    "\n",
    "print(\"\\n=== 数据清洗完成 ===\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01ec00d5",
   "metadata": {},
   "source": [
    "# 表格格式整理"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a09326e2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.microsoft.datawrangler.viewer.v0+json": {
       "columns": [
        {
         "name": "index",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "数据原始名称",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "数据翻译名称",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "是否上传门户",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "科学领域",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "功能标签-任务导向",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "二级标签-应用领域",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "模态",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "是否开源",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "协议",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "国内/外",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "数据集内数量（条）",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "数据集大小",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "主要数据格式",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "原始数据发布/更新日期",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "数据介绍",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "来源合作机构",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "来源合作机构数据连接",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "数据原始发布机构",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "数据原始链接",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "备注-平台作者",
         "rawType": "object",
         "type": "string"
        }
       ],
       "ref": "3aa4a429-ca8e-458f-9c53-5f30e7d75de3",
       "rows": [
        [
         "0",
         "dolma",
         "N/A",
         "否",
         "其他",
         "多语言处理",
         "语言学",
         "多模态",
         "是",
         "odc-by",
         "国外",
         "N/A",
         "1.32 MB",
         ".md, .py, .txt",
         "2024-04-17",
         "Dolma是一个包含3万亿个token的大型数据集，涵盖网页内容、学术出版物、代码、书籍和百科全书材料等多种来源。该数据集用于训练和评估大规模语言模型，并提供工具包支持数据集的复制和使用。",
         "N/A",
         "N/A",
         "Allen Institute for AI",
         "https://huggingface.co/datasets/allenai/dolma",
         "allenai"
        ],
        [
         "1",
         "SDO_training",
         "N/A",
         "否",
         "其他",
         "统计建模预测",
         "教育学",
         "多模态",
         "是",
         "MIT",
         "国外",
         "N/A",
         "329.04 GB",
         ".csv, .md, .part_aa, .part_ab, .part_ac, .part_ad, .part_ae, .part_af, .part_ag, .part_ah, .part_ai, .part_aj, .part_ak, .part_al, .part_am, .part_an, .part_ao, .part_ap, .part_aq, .png",
         "2025-05-21",
         "该数据集提供来自NASA太阳动力学观测站（SDO）的机器学习就绪太阳数据，涵盖2010年5月13日至2024年7月31日的观测数据。数据集旨在促进日球物理学中的大规模机器学习应用，包括太阳活动预测、无监督表示学习和科学基础模型开发。",
         "N/A",
         "N/A",
         "Huggingface",
         "https://huggingface.co/datasets/nasa-ibm-ai4science/SDO_training",
         "nasa-ibm-ai4science"
        ],
        [
         "2",
         "4DNeX-10M",
         "N/A",
         "否",
         "其他",
         "统计建模预测",
         "数字人文",
         "多模态",
         "是",
         "Apache-2.0",
         "国外",
         "N/A",
         "6.63 TB",
         ".gz, .md, .zip",
         "2025-08-16",
         "4DNeX-10M是一个大规模混合数据集，汇集了来自不同来源的单目视频，包含静态和动态场景，并配有使用先进3D和4D重建方法生成的高质量伪4D标注。该数据集支持将RGB外观和XYZ几何序列联合建模为统一的6D视频表示，旨在促进高效且可泛化的4D场景生成。",
         "N/A",
         "N/A",
         "Huggingface",
         "https://huggingface.co/datasets/3DTopia/4DNeX-10M",
         "3DTopia"
        ],
        [
         "3",
         "MADLAD-400",
         "N/A",
         "否",
         "其他",
         "文本挖掘分析",
         "语言学",
         "多模态",
         "是",
         "odc-by",
         "国外",
         "N/A",
         "12.02 TB",
         ".gz, .md, .zip",
         "2024-09-10",
         "MADLAD-400是一个基于Common Crawl构建的多语言文档级数据集，覆盖419种语言。其主要优势在于多语言覆盖广、经过审核和高度过滤、保持文档级结构。数据集提供两个版本：带噪声版本（仅语言识别过滤）和清洁版本（应用多种过滤器但仍有噪声）。该数据集适用于多语言自然语言处理任务，但高度过滤可能影响某些应用的召回率。",
         "N/A",
         "N/A",
         "Allen Institute for AI",
         "https://huggingface.co/datasets/allenai/MADLAD-400",
         "allenai"
        ],
        [
         "4",
         "ASPED",
         "N/A",
         "否",
         "其他",
         "知识图谱构建",
         "计算社会科学",
         "多模态",
         "是",
         "CC-BY-4.0",
         "国外",
         "N/A",
         "1.24 TB",
         ".csv, .flac, .md, .mp4",
         "2024-01-24",
         "该数据集为ICASSP 2024会议上提出的ASPED数据集，包含在佐治亚理工学院及周边多个地点采集的行人活动音频和视频记录，用于行人行为研究。",
         "N/A",
         "N/A",
         "Huggingface",
         "https://huggingface.co/datasets/urbanaudiosensing/ASPED",
         "urbanaudiosensing"
        ]
       ],
       "shape": {
        "columns": 20,
        "rows": 5
       }
      },
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>数据原始名称</th>\n",
       "      <th>数据翻译名称</th>\n",
       "      <th>是否上传门户</th>\n",
       "      <th>科学领域</th>\n",
       "      <th>功能标签-任务导向</th>\n",
       "      <th>二级标签-应用领域</th>\n",
       "      <th>模态</th>\n",
       "      <th>是否开源</th>\n",
       "      <th>协议</th>\n",
       "      <th>国内/外</th>\n",
       "      <th>数据集内数量（条）</th>\n",
       "      <th>数据集大小</th>\n",
       "      <th>主要数据格式</th>\n",
       "      <th>原始数据发布/更新日期</th>\n",
       "      <th>数据介绍</th>\n",
       "      <th>来源合作机构</th>\n",
       "      <th>来源合作机构数据连接</th>\n",
       "      <th>数据原始发布机构</th>\n",
       "      <th>数据原始链接</th>\n",
       "      <th>备注-平台作者</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>dolma</td>\n",
       "      <td>N/A</td>\n",
       "      <td>否</td>\n",
       "      <td>其他</td>\n",
       "      <td>多语言处理</td>\n",
       "      <td>语言学</td>\n",
       "      <td>多模态</td>\n",
       "      <td>是</td>\n",
       "      <td>odc-by</td>\n",
       "      <td>国外</td>\n",
       "      <td>N/A</td>\n",
       "      <td>1.32 MB</td>\n",
       "      <td>.md, .py, .txt</td>\n",
       "      <td>2024-04-17</td>\n",
       "      <td>Dolma是一个包含3万亿个token的大型数据集，涵盖网页内容、学术出版物、代码、书籍和百...</td>\n",
       "      <td>N/A</td>\n",
       "      <td>N/A</td>\n",
       "      <td>Allen Institute for AI</td>\n",
       "      <td>https://huggingface.co/datasets/allenai/dolma</td>\n",
       "      <td>allenai</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>SDO_training</td>\n",
       "      <td>N/A</td>\n",
       "      <td>否</td>\n",
       "      <td>其他</td>\n",
       "      <td>统计建模预测</td>\n",
       "      <td>教育学</td>\n",
       "      <td>多模态</td>\n",
       "      <td>是</td>\n",
       "      <td>MIT</td>\n",
       "      <td>国外</td>\n",
       "      <td>N/A</td>\n",
       "      <td>329.04 GB</td>\n",
       "      <td>.csv, .md, .part_aa, .part_ab, .part_ac, .part...</td>\n",
       "      <td>2025-05-21</td>\n",
       "      <td>该数据集提供来自NASA太阳动力学观测站（SDO）的机器学习就绪太阳数据，涵盖2010年5月...</td>\n",
       "      <td>N/A</td>\n",
       "      <td>N/A</td>\n",
       "      <td>Huggingface</td>\n",
       "      <td>https://huggingface.co/datasets/nasa-ibm-ai4sc...</td>\n",
       "      <td>nasa-ibm-ai4science</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>4DNeX-10M</td>\n",
       "      <td>N/A</td>\n",
       "      <td>否</td>\n",
       "      <td>其他</td>\n",
       "      <td>统计建模预测</td>\n",
       "      <td>数字人文</td>\n",
       "      <td>多模态</td>\n",
       "      <td>是</td>\n",
       "      <td>Apache-2.0</td>\n",
       "      <td>国外</td>\n",
       "      <td>N/A</td>\n",
       "      <td>6.63 TB</td>\n",
       "      <td>.gz, .md, .zip</td>\n",
       "      <td>2025-08-16</td>\n",
       "      <td>4DNeX-10M是一个大规模混合数据集，汇集了来自不同来源的单目视频，包含静态和动态场景，...</td>\n",
       "      <td>N/A</td>\n",
       "      <td>N/A</td>\n",
       "      <td>Huggingface</td>\n",
       "      <td>https://huggingface.co/datasets/3DTopia/4DNeX-10M</td>\n",
       "      <td>3DTopia</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>MADLAD-400</td>\n",
       "      <td>N/A</td>\n",
       "      <td>否</td>\n",
       "      <td>其他</td>\n",
       "      <td>文本挖掘分析</td>\n",
       "      <td>语言学</td>\n",
       "      <td>多模态</td>\n",
       "      <td>是</td>\n",
       "      <td>odc-by</td>\n",
       "      <td>国外</td>\n",
       "      <td>N/A</td>\n",
       "      <td>12.02 TB</td>\n",
       "      <td>.gz, .md, .zip</td>\n",
       "      <td>2024-09-10</td>\n",
       "      <td>MADLAD-400是一个基于Common Crawl构建的多语言文档级数据集，覆盖419种...</td>\n",
       "      <td>N/A</td>\n",
       "      <td>N/A</td>\n",
       "      <td>Allen Institute for AI</td>\n",
       "      <td>https://huggingface.co/datasets/allenai/MADLAD...</td>\n",
       "      <td>allenai</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>ASPED</td>\n",
       "      <td>N/A</td>\n",
       "      <td>否</td>\n",
       "      <td>其他</td>\n",
       "      <td>知识图谱构建</td>\n",
       "      <td>计算社会科学</td>\n",
       "      <td>多模态</td>\n",
       "      <td>是</td>\n",
       "      <td>CC-BY-4.0</td>\n",
       "      <td>国外</td>\n",
       "      <td>N/A</td>\n",
       "      <td>1.24 TB</td>\n",
       "      <td>.csv, .flac, .md, .mp4</td>\n",
       "      <td>2024-01-24</td>\n",
       "      <td>该数据集为ICASSP 2024会议上提出的ASPED数据集，包含在佐治亚理工学院及周边多个...</td>\n",
       "      <td>N/A</td>\n",
       "      <td>N/A</td>\n",
       "      <td>Huggingface</td>\n",
       "      <td>https://huggingface.co/datasets/urbanaudiosens...</td>\n",
       "      <td>urbanaudiosensing</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         数据原始名称 数据翻译名称 是否上传门户 科学领域 功能标签-任务导向 二级标签-应用领域   模态 是否开源          协议  \\\n",
       "0         dolma    N/A      否   其他     多语言处理       语言学  多模态    是      odc-by   \n",
       "1  SDO_training    N/A      否   其他    统计建模预测       教育学  多模态    是         MIT   \n",
       "2     4DNeX-10M    N/A      否   其他    统计建模预测      数字人文  多模态    是  Apache-2.0   \n",
       "3    MADLAD-400    N/A      否   其他    文本挖掘分析       语言学  多模态    是      odc-by   \n",
       "4         ASPED    N/A      否   其他    知识图谱构建    计算社会科学  多模态    是   CC-BY-4.0   \n",
       "\n",
       "  国内/外 数据集内数量（条）      数据集大小  \\\n",
       "0   国外       N/A    1.32 MB   \n",
       "1   国外       N/A  329.04 GB   \n",
       "2   国外       N/A    6.63 TB   \n",
       "3   国外       N/A   12.02 TB   \n",
       "4   国外       N/A    1.24 TB   \n",
       "\n",
       "                                              主要数据格式 原始数据发布/更新日期  \\\n",
       "0                                     .md, .py, .txt  2024-04-17   \n",
       "1  .csv, .md, .part_aa, .part_ab, .part_ac, .part...  2025-05-21   \n",
       "2                                     .gz, .md, .zip  2025-08-16   \n",
       "3                                     .gz, .md, .zip  2024-09-10   \n",
       "4                             .csv, .flac, .md, .mp4  2024-01-24   \n",
       "\n",
       "                                                数据介绍 来源合作机构 来源合作机构数据连接  \\\n",
       "0  Dolma是一个包含3万亿个token的大型数据集，涵盖网页内容、学术出版物、代码、书籍和百...    N/A        N/A   \n",
       "1  该数据集提供来自NASA太阳动力学观测站（SDO）的机器学习就绪太阳数据，涵盖2010年5月...    N/A        N/A   \n",
       "2  4DNeX-10M是一个大规模混合数据集，汇集了来自不同来源的单目视频，包含静态和动态场景，...    N/A        N/A   \n",
       "3  MADLAD-400是一个基于Common Crawl构建的多语言文档级数据集，覆盖419种...    N/A        N/A   \n",
       "4  该数据集为ICASSP 2024会议上提出的ASPED数据集，包含在佐治亚理工学院及周边多个...    N/A        N/A   \n",
       "\n",
       "                 数据原始发布机构                                             数据原始链接  \\\n",
       "0  Allen Institute for AI      https://huggingface.co/datasets/allenai/dolma   \n",
       "1             Huggingface  https://huggingface.co/datasets/nasa-ibm-ai4sc...   \n",
       "2             Huggingface  https://huggingface.co/datasets/3DTopia/4DNeX-10M   \n",
       "3  Allen Institute for AI  https://huggingface.co/datasets/allenai/MADLAD...   \n",
       "4             Huggingface  https://huggingface.co/datasets/urbanaudiosens...   \n",
       "\n",
       "               备注-平台作者  \n",
       "0              allenai  \n",
       "1  nasa-ibm-ai4science  \n",
       "2              3DTopia  \n",
       "3              allenai  \n",
       "4    urbanaudiosensing  "
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 1. 删除不需要的列\n",
    "cols_to_drop = ['name', 'display_text', 'tags', 'libraries', 'license', 'description', 'extraction_status', 'tags_cleaned', '识别出的AI公司', '标签', '学科']\n",
    "df_final = df_final.drop(columns=[col for col in cols_to_drop if col in df_final.columns])\n",
    "\n",
    "# 2. 替换列名称\n",
    "df_final = df_final.rename(columns={\n",
    "    'item_name': '数据原始名称',\n",
    "    'url': '数据原始链接',\n",
    "    'author': '备注-平台作者',\n",
    "    'update_date': '原始数据发布/更新日期',\n",
    "    'modalities': '模态',\n",
    "    'size': '数据集大小',\n",
    "    '介绍': '数据介绍',\n",
    "    '开源协议': '协议',\n",
    "    'formats': '主要数据格式'\n",
    "})\n",
    "\n",
    "# 3. 按照指定顺序重排\n",
    "desired_order = [\n",
    "    '数据原始名称', '科学领域', '功能标签-任务导向', '二级标签-应用领域', '二级标签-数据类型',\n",
    "    '模态', '协议', '国内/外', '数据集大小', '主要数据格式',\n",
    "    '原始数据发布/更新日期', '数据介绍', '数据原始发布机构', '数据原始链接', '备注-平台作者'\n",
    " ]\n",
    "df_final = df_final[[col for col in desired_order if col in df_final.columns]]\n",
    "df_final.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "5c6e2cef",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_final.to_excel('科学智能语料.xlsx', index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
